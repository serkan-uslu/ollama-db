[
  {
    "id": "8ce6f2c7-5050-4d75-8b6b-0cc0d3ed195e",
    "model_identifier": "llama3.1",
    "model_name": "llama3.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.1",
    "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nMeta Llama 3.1\n\n\n\n\nLlama 3.1\n family of models available:\n\n\n\n\n8B\n\n\n70B\n\n\n405B\n\n\n\n\nLlama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation.\n\n\nThe upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta’s latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants.\n\n\nMeta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models.\n\n\nModel evaluations\n\n\nFor this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta’s experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta’s smaller models are competitive with closed and open models that have a similar number of parameters.\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nMeta AI Llama 3.1 launch blog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Meta Llama 3.1 \n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/4d0cab8e-952b-4c75-b110-1514d8db8fae)\n\n**Llama 3.1** family of models available: \n\n- **8B**\n-  **70B**\n-  **405B**\n\nLlama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. \n\nThe upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta's latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. \n\nMeta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models. \n\n### Model evaluations \n\nFor this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta's experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta's smaller models are competitive with closed and open models that have a similar number of parameters.\n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/73b11a5e-84e9-4397-9d47-f0299a6294b3)\n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/ad042a1c-bbc7-47de-bbbf-78a3cfc13485)\n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/2d582df5-ce45-4326-85c5-254c917554b2)\n\n### References \n- [Meta AI Llama 3.1 launch blog post](https://ai.meta.com/blog/meta-llama-3-1/) \n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "8b",
      "70b",
      "405b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.1:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "llama3.1:70b",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "llama3.1:405b",
        "size": "243GB",
        "size_gb": 243.0,
        "recommended_ram_gb": 303.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 4.9,
    "use_cases": [
      "Text Summarization",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for advanced AI applications.",
    "pulls": 110500000,
    "tags": 93,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ca2290f2-d38a-4ee5-95c1-cb8948d573c3",
    "model_identifier": "deepseek-r1",
    "model_name": "deepseek-r1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-r1",
    "description": "DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n\n\n\n\nModels\n\n\nDeepSeek-R1-0528-Qwen3-8B\n\n\nollama run deepseek-r1\n\n\n\nDeepSeek-R1\n\n\nollama run deepseek-r1:671b\n\n\n\n\n\nNote: to update the model from an older version, run \nollama pull deepseek-r1\n\n\n\n\nDistilled models\n\n\nDeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.\n\n\nBelow are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.\n\n\nDeepSeek-R1-0528-Qwen3-8B\n\n\nollama run deepseek-r1:8b\n\n\n\nDeepSeek-R1-Distill-Qwen-1.5B\n\n\nollama run deepseek-r1:1.5b\n\n\n\nDeepSeek-R1-Distill-Qwen-7B\n\n\nollama run deepseek-r1:7b\n\n\n\nDeepSeek-R1-Distill-Qwen-14B\n\n\nollama run deepseek-r1:14b\n\n\n\nDeepSeek-R1-Distill-Qwen-32B\n\n\nollama run deepseek-r1:32b\n\n\n\nDeepSeek-R1-Distill-Llama-70B\n\n\nollama run deepseek-r1:70b\n\n\n\nLicense\n\n\nThe model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n\n\nThe Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\n\n\nThe Llama 8B distilled model is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\n\n\nThe Llama 70B distilled model is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" />\n\nDeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n\n![image.png](/assets/library/deepseek-r1/ba9ac535-ac57-4d42-9f36-20067c8eaa50)\n\n## Models\n\n**DeepSeek-R1-0528-Qwen3-8B**\n\n```\nollama run deepseek-r1\n```\n\n**DeepSeek-R1**\n\n```\nollama run deepseek-r1:671b\n```\n\n> Note: to update the model from an older version, run `ollama pull deepseek-r1`\n\n### Distilled models \n\nDeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. \n\nBelow are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. \n\n**DeepSeek-R1-0528-Qwen3-8B** \n\n```\nollama run deepseek-r1:8b\n```\n\n**DeepSeek-R1-Distill-Qwen-1.5B**   \n\n```\nollama run deepseek-r1:1.5b\n```\n\n**DeepSeek-R1-Distill-Qwen-7B** \n\n```\nollama run deepseek-r1:7b\n```\n\n**DeepSeek-R1-Distill-Qwen-14B** \n\n```\nollama run deepseek-r1:14b\n```\n\n**DeepSeek-R1-Distill-Qwen-32B** \n\n```\nollama run deepseek-r1:32b\n```\n\n**DeepSeek-R1-Distill-Llama-70B**  \n\n```\nollama run deepseek-r1:70b\n```\n\n\n\n\n### License \n\nThe model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n\nThe Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\n\nThe Llama 8B distilled model is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license. \n\nThe Llama 70B distilled model is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "1.5b",
      "7b",
      "8b",
      "14b",
      "32b",
      "70b",
      "671b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-r1:1.5b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:7b",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:70b",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:671b",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      }
    ],
    "min_ram_gb": 1.1,
    "use_cases": [
      "Reasoning",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Users requiring high-performance reasoning and question answering in a variety of tasks.",
    "pulls": 78500000,
    "tags": 35,
    "last_updated": "2025-07-25",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4317cb06-b0fa-41e2-8b77-0a94c65492e4",
    "model_identifier": "llama3.2",
    "model_name": "llama3.2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.2",
    "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n\nSizes\n\n\n3B parameters (default)\n\n\nThe 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as:\n\n\n\n\nFollowing instructions\n\n\nSummarization\n\n\nPrompt rewriting\n\n\nTool use\n\n\n\n\nollama run llama3.2\n\n\n\n1B parameters\n\n\nThe 1B model is competitive with other 1-3B parameter models. It’s use cases include:\n\n\n\n\nPersonal information management\n\n\nMultilingual knowledge retrieval\n\n\nRewriting tasks running locally on edge\n\n\n\n\nollama run llama3.2:1b\n\n\n\nBenchmarks\n\n\n\n\nSupported Languages:\n English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/llama3.2/be01fadf-7fbd-404d-929b-50a77249b030\" width=\"280\" />\n\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n## Sizes\n\n### 3B parameters (default)\n\nThe 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as:\n\n* Following instructions\n* Summarization\n* Prompt rewriting\n* Tool use\n\n```\nollama run llama3.2\n```\n\n### 1B parameters\n\nThe 1B model is competitive with other 1-3B parameter models. It's use cases include:\n\n* Personal information management\n* Multilingual knowledge retrieval\n* Rewriting tasks running locally on edge\n\n```\nollama run llama3.2:1b\n```\n\n### Benchmarks\n\n![Llama 3.2 instruction-tuned benchmarks](https://ollama.com/assets/library/llama3.2/c1a51716-d8bb-4642-8044-48f5022b777d)\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "1b",
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.2:1b",
        "size": "1.3GB",
        "size_gb": 1.3,
        "recommended_ram_gb": 1.6,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "llama3.2:latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.3,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Function Calling",
      "Role Play"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for text summarization and role play scenarios.",
    "pulls": 57800000,
    "tags": 63,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "7670295a-f525-4d0e-a5cc-38574c9d6b33",
    "model_identifier": "nomic-embed-text",
    "model_name": "nomic-embed-text",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nomic-embed-text",
    "description": "A high-performing open embedding model with a large token context window.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.1.26 or later. \nDownload it here\n. It can only be used to generate embeddings.\n\n\n\n\nnomic-embed-text\n is a large context length text encoder that surpasses OpenAI \ntext-embedding-ada-002\n and \ntext-embedding-3-small\n performance on short and long context tasks.\n\n\nUsage\n\n\nThis model is an embedding model, meaning it can only be used to generate embeddings.\n\n\nREST API\n\n\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\n\n\n\nPython library\n\n\nollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering')\n\n\n\nJavascript library\n\n\nollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' })\n\n\n\nReferences\n\n\nHuggingFace\n\n\nBlog Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![nomic_logo](https://github.com/ollama/ollama/assets/251292/bf242e43-3c1a-4590-887d-abcab76cb304)\n\n\n> Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). It can only be used to generate embeddings.\n\n`nomic-embed-text` is a large context length text encoder that surpasses OpenAI `text-embedding-ada-002` and `text-embedding-3-small` performance on short and long context tasks.\n\n## Usage\n\nThis model is an embedding model, meaning it can only be used to generate embeddings.\n\n### REST API\n\n```\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\n```\n\n### Python library\n\n```\nollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering')\n```\n\n### Javascript library\n\n```\nollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' })\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)\n\n[Blog Post](https://blog.nomic.ai/posts/nomic-embed-text-v1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "embedding"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Generate text embeddings for further processing.",
    "pulls": 54700000,
    "tags": 3,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ce0f5f70-9ae4-4ba4-bd00-5d8e612268d5",
    "model_identifier": "gemma3",
    "model_name": "gemma3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma3",
    "description": "The current, most capable model that runs on a single GPU.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nThis model requires Ollama 0.6 or later. \nDownload Ollama\n\n\n\n\nGemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices.\n\n\nModels\n\n\nText\n\n\n270M parameter model\n (32k context window)\n\n\nollama run gemma3:270m\n\n\n\n1B parameter model\n (32k context window)\n\n\nollama run gemma3:1b \n\n\n\nMultimodal (Vision)\n\n\n4B parameter model\n (128k context window)\n\n\nollama run gemma3:4b\n\n\n\n12B parameter model\n (128k context window)\n\n\nollama run gemma3:12b\n\n\n\n27B parameter model\n (128k context window)\n\n\nollama run gemma3:27b\n\n\n\nQuantization aware trained models (QAT)\n\n\nThe quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models).\n\n\n1B parameter model\n\n\nollama run gemma3:1b-it-qat\n\n\n\n4B parameter model\n\n\nollama run gemma3:4b-it-qat\n\n\n\n12B parameter model\n\n\nollama run gemma3:12b-it-qat\n\n\n\n27B parameter model\n\n\nollama run gemma3:27b-it-qat\n\n\n\nEvaluation\n\n\n\n\nBenchmark Results\n\n\nGemma 3 270M\n\n\n\n\n\n\n\n\nBenchmark\n\n\nn-shot\n\n\nGemma 3 270m instruction tuned\n\n\n\n\n\n\n\n\n\n\nHellaSwag\n\n\n0-shot\n\n\n37.7\n\n\n\n\n\n\nPIQA\n\n\n0-shot\n\n\n66.2\n\n\n\n\n\n\nARC-c\n\n\n0-shot\n\n\n28.2\n\n\n\n\n\n\nWinoGrande\n\n\n0-shot\n\n\n52.3\n\n\n\n\n\n\nBIG-Bench Hard\n\n\nfew-shot\n\n\n26.7\n\n\n\n\n\n\nIF Eval\n\n\n0-shot\n\n\n51.2\n\n\n\n\n\n\n\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n\nReasoning, logic and code capabilities\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMetric\n\n\nGemma 3 PT 1B\n\n\nGemma 3 PT 4B\n\n\nGemma 3 PT 12B\n\n\nGemma 3 PT 27B\n\n\n\n\n\n\n\n\n\n\nHellaSwag\n\n\n10-shot\n\n\n62.3\n\n\n77.2\n\n\n84.2\n\n\n85.6\n\n\n\n\n\n\nBoolQ\n\n\n0-shot\n\n\n63.2\n\n\n72.3\n\n\n78.8\n\n\n82.4\n\n\n\n\n\n\nPIQA\n\n\n0-shot\n\n\n73.8\n\n\n79.6\n\n\n81.8\n\n\n83.3\n\n\n\n\n\n\nSocialIQA\n\n\n0-shot\n\n\n48.9\n\n\n51.9\n\n\n53.4\n\n\n54.9\n\n\n\n\n\n\nTriviaQA\n\n\n5-shot\n\n\n39.8\n\n\n65.8\n\n\n78.2\n\n\n85.5\n\n\n\n\n\n\nNatural Questions\n\n\n5-shot\n\n\n9.48\n\n\n20.0\n\n\n31.4\n\n\n36.1\n\n\n\n\n\n\nARC-c\n\n\n25-shot\n\n\n38.4\n\n\n56.2\n\n\n68.9\n\n\n70.6\n\n\n\n\n\n\nARC-e\n\n\n0-shot\n\n\n73.0\n\n\n82.4\n\n\n88.3\n\n\n89.0\n\n\n\n\n\n\nWinoGrande\n\n\n5-shot\n\n\n58.2\n\n\n64.7\n\n\n74.3\n\n\n78.8\n\n\n\n\n\n\nBIG-Bench Hard\n\n\n\n\n28.4\n\n\n50.9\n\n\n72.6\n\n\n77.7\n\n\n\n\n\n\nDROP\n\n\n3-shot, F1\n\n\n42.4\n\n\n60.1\n\n\n72.2\n\n\n77.2\n\n\n\n\n\n\nAGIEval\n\n\n3-5-shot\n\n\n22.2\n\n\n42.1\n\n\n57.4\n\n\n66.2\n\n\n\n\n\n\nMMLU\n\n\n5-shot, top-1\n\n\n26.5\n\n\n59.6\n\n\n74.5\n\n\n78.6\n\n\n\n\n\n\nMATH\n\n\n4-shot\n\n\n–\n\n\n24.2\n\n\n43.3\n\n\n50.0\n\n\n\n\n\n\nGSM8K\n\n\n5-shot, maj@1\n\n\n1.36\n\n\n38.4\n\n\n71.0\n\n\n82.6\n\n\n\n\n\n\nGPQA\n\n\n\n\n9.38\n\n\n15.0\n\n\n25.4\n\n\n24.3\n\n\n\n\n\n\nMMLU\n (Pro)\n\n\n5-shot\n\n\n11.2\n\n\n23.7\n\n\n40.8\n\n\n43.9\n\n\n\n\n\n\nMBPP\n\n\n3-shot\n\n\n9.80\n\n\n46.0\n\n\n60.4\n\n\n65.6\n\n\n\n\n\n\nHumanEval\n\n\npass@1\n\n\n6.10\n\n\n36.0\n\n\n45.7\n\n\n48.8\n\n\n\n\n\n\nMMLU\n (Pro COT)\n\n\n5-shot\n\n\n9.7\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n\n\n\nMultilingual capabilities\n\n\n\n\n\n\n\n\nBenchmark\n\n\nGemma 3 PT 1B\n\n\nGemma 3 PT 4B\n\n\nGemma 3 PT 12B\n\n\nGemma 3 PT 27B\n\n\n\n\n\n\n\n\n\n\nMGSM\n\n\n2.04\n\n\n34.7\n\n\n64.3\n\n\n74.3\n\n\n\n\n\n\nGlobal-MMLU-Lite\n\n\n24.9\n\n\n57.0\n\n\n69.4\n\n\n75.7\n\n\n\n\n\n\nBelebele\n\n\n26.6\n\n\n59.4\n\n\n78.0\n\n\n–\n\n\n\n\n\n\nWMT24++\n (ChrF)\n\n\n36.7\n\n\n48.4\n\n\n53.9\n\n\n55.7\n\n\n\n\n\n\nFloRes\n\n\n29.5\n\n\n39.2\n\n\n46.0\n\n\n48.8\n\n\n\n\n\n\nXL-Sum\n\n\n4.82\n\n\n8.55\n\n\n12.2\n\n\n14.9\n\n\n\n\n\n\nXQuAD\n (all)\n\n\n43.9\n\n\n68.0\n\n\n74.5\n\n\n76.8\n\n\n\n\n\n\n\n\nMultimodal capabilities\n\n\n\n\n\n\n\n\nBenchmark\n\n\nGemma 3 PT 4B\n\n\nGemma 3 PT 12B\n\n\nGemma 3 PT 27B\n\n\n\n\n\n\n\n\n\n\nCOCOcap\n\n\n102\n\n\n111\n\n\n116\n\n\n\n\n\n\nDocVQA\n (val)\n\n\n72.8\n\n\n82.3\n\n\n85.6\n\n\n\n\n\n\nInfoVQA\n (val)\n\n\n44.1\n\n\n54.8\n\n\n59.4\n\n\n\n\n\n\nMMMU\n (pt)\n\n\n39.2\n\n\n50.3\n\n\n56.1\n\n\n\n\n\n\nTextVQA\n (val)\n\n\n58.9\n\n\n66.5\n\n\n68.6\n\n\n\n\n\n\nRealWorldQA\n\n\n45.5\n\n\n52.2\n\n\n53.9\n\n\n\n\n\n\nReMI\n\n\n27.3\n\n\n38.5\n\n\n44.8\n\n\n\n\n\n\nAI2D\n\n\n63.2\n\n\n75.2\n\n\n79.0\n\n\n\n\n\n\nChartQA\n\n\n45.4\n\n\n60.9\n\n\n63.8\n\n\n\n\n\n\nChartQA\n (augmented)\n\n\n81.8\n\n\n88.5\n\n\n88.7\n\n\n\n\n\n\nVQAv2\n\n\n–\n\n\n–\n\n\n–\n\n\n\n\n\n\nBLINK\n\n\n38.0\n\n\n35.9\n\n\n39.6\n\n\n\n\n\n\nOKVQA\n\n\n51.0\n\n\n58.7\n\n\n60.2\n\n\n\n\n\n\nTallyQA\n\n\n42.5\n\n\n51.8\n\n\n54.3\n\n\n\n\n\n\nSpatialSense VQA\n\n\n50.9\n\n\n60.0\n\n\n59.4\n\n\n\n\n\n\nCountBenchQA\n\n\n26.1\n\n\n17.8\n\n\n68.0\n\n\n\n\n\n\n\n\nReference\n\n\nGemma Terms of Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Google Gemma 3 logo](/assets/library/gemma3/b54bf767-f9c5-4284-b551-a49aebe3a3c2)\n\n> This model requires Ollama 0.6 or later. [Download Ollama](https://ollama.com/download)\n\nGemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices.\n\n## Models \n\n### Text\n\n**270M parameter model** (32k context window) \n\n```\nollama run gemma3:270m\n```\n**1B parameter model** (32k context window)\n\n```\nollama run gemma3:1b \n```\n\n### Multimodal (Vision) \n\n**4B parameter model** (128k context window)  \n\n```\nollama run gemma3:4b\n```\n\n**12B parameter model** (128k context window)\n\n```\nollama run gemma3:12b\n```\n\n**27B parameter model** (128k context window)\n\n```\nollama run gemma3:27b\n```\n\n### Quantization aware trained models (QAT) \nThe quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models).   \n\n**1B parameter model**\n```\nollama run gemma3:1b-it-qat\n```\n\n**4B parameter model**\n```\nollama run gemma3:4b-it-qat\n```\n\n**12B parameter model**\n```\nollama run gemma3:12b-it-qat\n```\n\n**27B parameter model**\n```\nollama run gemma3:27b-it-qat\n```\n\n## Evaluation\n\n![Chatbot Arena ELO Score](/assets/library/gemma3/89dc5a19-179e-4dd3-8e5d-12ad54973148)\n\n### Benchmark Results\n\n**Gemma 3 270M**\n\n| **Benchmark**             |  **n-shot**   | **Gemma 3 270m instruction tuned** |\n| :------------------------ | :-----------: | ------------------: |\n| [HellaSwag][hellaswag]    |    0-shot     |                37.7 |\n| [PIQA][piqa]              |    0-shot     |                66.2 |\n| [ARC-c][arc]              |    0-shot     |                28.2 |\n| [WinoGrande][winogrande]  |    0-shot     |                52.3 |\n| [BIG-Bench Hard][bbh]     |   few-shot    |                26.7 |\n| [IF Eval][ifeval]         |    0-shot     |                51.2 |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[piqa]: https://arxiv.org/abs/1911.11641\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[bbh]: https://paperswithcode.com/dataset/bbh\n[ifeval]: https://arxiv.org/abs/2311.07911\n\n\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning, logic and code capabilities\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |     77.2      |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |     72.3      |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |     79.6      |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |     51.9      |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |     65.8      |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |     20.0      |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |     56.2      |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |     82.4      |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |     64.7      |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          |                |      28.4      |     50.9      |      72.6      |      77.7      |\n| [DROP][drop]                   | 3-shot, F1     |      42.4      |     60.1      |      72.2      |      77.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      22.2      |     42.1      |      57.4      |      66.2      |\n| [MMLU][mmlu]                   | 5-shot, top-1  |      26.5      |     59.6      |      74.5      |      78.6      |\n| [MATH][math]                   | 4-shot         |       --       |     24.2      |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 5-shot, maj@1  |      1.36      |     38.4      |      71.0      |      82.6      |\n| [GPQA][gpqa]                   |                |      9.38      |     15.0      |      25.4      |      24.3      |\n| [MMLU][mmlu] (Pro)             | 5-shot         |      11.2      |     23.7      |      40.8      |      43.9      |\n| [MBPP][mbpp]                   | 3-shot         |      9.80      |     46.0      |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | pass@1         |      6.10      |     36.0      |      45.7      |      48.8      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      9.7       |     NaN       |      NaN       |      NaN       |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bb",
    "capabilities": [
      "Vision",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "270m",
      "1b",
      "4b",
      "12b",
      "27b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma3:latest",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "gemma3:12b",
        "size": "8.1GB",
        "size_gb": 8.1,
        "recommended_ram_gb": 10.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "gemma3:27b",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 3.3,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers, Researchers, Data Scientists",
    "pulls": 32100000,
    "tags": 29,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "b6c76c14-46c9-4fa6-96db-65322951ee47",
    "model_identifier": "mistral",
    "model_name": "mistral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral",
    "description": "The 7B model released by Mistral AI, updated to version 0.3.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion.\n\n\nThe Mistral AI team has noted that Mistral 7B:\n\n\n\n\nOutperforms Llama 2 13B on all benchmarks\n\n\nOutperforms Llama 1 34B on many benchmarks\n\n\nApproaches CodeLlama 7B performance on code, while remaining good at English tasks\n\n\n\n\nVersions\n\n\n\n\n\n\n\n\nTag\n\n\nDate\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nv0.3\n \nlatest\n\n\n05/22/2024\n\n\nA new version of Mistral 7B that supports function calling.\n\n\n\n\n\n\nv0.2\n\n\n03/23/2024\n\n\nA minor release of Mistral 7B\n\n\n\n\n\n\nv0.1\n\n\n09/27/2023\n\n\nInitial release\n\n\n\n\n\n\n\n\nFunction calling\n\n\nMistral 0.3 supports function calling with Ollama’s \nraw mode\n.\n\n\nExample raw prompt\n\n\n[AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST]\n\n\n\nExample response\n\n\n[TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}]\n\n\n\nFor more information on raw mode, see the \nAPI documentation\n.\n\n\nVariations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstruct\n\n\nInstruct models follow instructions\n\n\n\n\n\n\ntext\n\n\nText models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion.\n\n\n\n\n\n\n\n\nUsage\n\n\nCLI\n\n\nInstruct:\n\n\nollama run mistral\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n\n\n\nReferences\n\n\nHuggingFace\n\n\nMistral AI News Release\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171\" width=\"200\"/>\n\nMistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion.\n\nThe Mistral AI team has noted that Mistral 7B:\n\n- Outperforms Llama 2 13B on all benchmarks\n- Outperforms Llama 1 34B on many benchmarks\n- Approaches CodeLlama 7B performance on code, while remaining good at English tasks\n\n### Versions\n\n| Tag             | Date       | Notes                                                       |\n| --------------- | ---------- | ----------------------------------------------------------- |\n| `v0.3` `latest` | 05/22/2024 | A new version of Mistral 7B that supports function calling. |\n| `v0.2`          | 03/23/2024 | A minor release of Mistral 7B                           |\n| `v0.1`          | 09/27/2023 | Initial release                                             |\n\n### Function calling\n\nMistral 0.3 supports function calling with Ollama's **raw mode**.\n\n<sub>Example raw prompt</sub>\n```\n[AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST]\n```\n\n<sub>Example response</sub>\n\n```\n[TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}]\n```\n\nFor more information on raw mode, see the [API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md#request-raw-mode).\n\n### Variations\n\n|            |                                                                                                                                    |\n| ---------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n| `instruct` | Instruct models follow instructions                                                                                                |\n| `text`     | Text models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion. |\n\n## Usage\n\n### CLI\n\nInstruct:\n\n```\nollama run mistral\n```\n\n### API\n\nExample:\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n\n[Mistral AI News Release](https://mistral.ai/news/announcing-mistral-7b/)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral:latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.4,
    "use_cases": [
      "Function Calling",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers who need a high-performance language model for complex tasks.",
    "pulls": 25500000,
    "tags": 84,
    "last_updated": "2025-07-25",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a448bf63-27fb-4b42-a01b-434161539e60",
    "model_identifier": "qwen2.5",
    "model_name": "qwen2.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2.5",
    "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2:\n\n\n\n\nIt possesses \nsignificantly more knowledge\n and has greatly enhanced capabilities in \ncoding\n and \nmathematics\n, due to specialized expert models in these domains.\n\n\nIt demonstrates significant advancements in \ninstruction following\n, \nlong-text generation\n (over 8K tokens), \nunderstanding structured data\n (e.g., tables), and \ngenerating structured outputs\n, especially in JSON format. It is also \nmore resilient to diverse system prompts\n, improving role-play and condition-setting for chatbots.\n\n\nIt supports \nlong contexts\n of up to 128K tokens and can generate up to 8K tokens.\n\n\nIt offers \nmultilingual support\n for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n\n\nPlease note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license.\n\n\nReferences\n\n\nGitHub\n\n\nBlog post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/qwen2.5/4b4f719f-c327-489e-8dc1-89a455c21e89\" width=\"320\" />\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2:\n\n- It possesses **significantly more knowledge** and has greatly enhanced capabilities in **coding** and **mathematics**, due to specialized expert models in these domains.\n- It demonstrates significant advancements in **instruction following**, **long-text generation** (over 8K tokens), **understanding structured data** (e.g., tables), and **generating structured outputs**, especially in JSON format. It is also **more resilient to diverse system prompts**, improving role-play and condition-setting for chatbots.\n- It supports **long contexts** of up to 128K tokens and can generate up to 8K tokens.\n- It offers **multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nPlease note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license.\n\n## References\n\n[GitHub](https://github.com/QwenLM/Qwen2.5)\n\n[Blog post](https://qwenlm.github.io/blog/qwen2.5/)\n\n[HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "0.5b",
      "1.5b",
      "3b",
      "7b",
      "14b",
      "32b",
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2.5:3b",
        "size": "1.9GB",
        "size_gb": 1.9,
        "recommended_ram_gb": 2.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:72b",
        "size": "47GB",
        "size_gb": 47.0,
        "recommended_ram_gb": 58.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.9,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Math",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced text generation and analysis tasks.",
    "pulls": 21900000,
    "tags": 133,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6e922edc-f5a2-49d2-9b84-2d7f1698c5c6",
    "model_identifier": "qwen3",
    "model_name": "qwen3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3",
    "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen 3\n is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n\n\nNew 30B model\n\n\nollama run qwen3:30b\n\n\n\n\n\nNew 235B model\n\n\nollama run qwen3:235b\n\n\n\n\n\nOverview\n\n\nThe Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, \nQwen3-235B-A22B\n, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, \nQwen3-30B-A3B\n, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n\n\n\n\nSignificantly enhancement in its reasoning capabilities\n, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n\n\nSuperior human preference alignment\n, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n\n\nExpertise in agent capabilities\n, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n\n\nSupport of 100+ languages and dialects\n with strong capabilities for \nmultilingual instruction following and translation\n.\n\n\n\n\nReference\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522)\n\n**Qwen 3** is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n\n### New 30B model\n\n```\nollama run qwen3:30b\n```\n\n![Qwen3-30B-A3B-Instruct-2507.jpg](/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0)\n\n\n### New 235B model\n\n```\nollama run qwen3:235b\n```\n\n![0d7zztq4GB7G2ZYowO-dQ.jpg](/assets/library/qwen3/8426a459-dd88-49cd-ae89-ece442e58ec5)\n\n### Overview\n\nThe Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, **Qwen3-235B-A22B**, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, **Qwen3-30B-A3B**, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following and translation**.\n\n### Reference\n- [Blog](https://qwenlm.github.io/blog/qwen3/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "0.6b",
      "1.7b",
      "4b",
      "8b",
      "14b",
      "30b",
      "32b",
      "235b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3:1.7b",
        "size": "1.4GB",
        "size_gb": 1.4,
        "recommended_ram_gb": 1.8,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3:4b",
        "size": "2.5GB",
        "size_gb": 2.5,
        "recommended_ram_gb": 3.1,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3:latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "40K",
        "context_window": 40000
      },
      {
        "tag": "qwen3:14b",
        "size": "9.3GB",
        "size_gb": 9.3,
        "recommended_ram_gb": 11.6,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3:30b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3:235b",
        "size": "142GB",
        "size_gb": 142.0,
        "recommended_ram_gb": 177.5,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 1.4,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 19700000,
    "tags": 58,
    "last_updated": "2025-10-25",
    "last_updated_str": "4 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "60fb1273-b771-4917-93ba-9b039801d867",
    "model_identifier": "gemma2",
    "model_name": "gemma2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma2",
    "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGoogle’s Gemma 2 model is available in three sizes, 2B, 9B and 27B, featuring a brand new architecture designed for class leading performance and efficiency.\n\n\nClass leading performance\n\n\nAt 27 billion parameters, Gemma 2 delivers performance surpassing models more than twice its size in benchmarks. This breakthrough efficiency sets a new standard in the open model landscape.\n\n\nThree sizes: 2B, 9B and 27B parameters\n\n\n\n\n2B Parameters \nollama run gemma2:2b\n\n\n9B Parameters \nollama run gemma2\n\n\n27B Parameters \nollama run gemma2:27b\n\n\n\n\nBenchmark\n\n\n\n\nIntended Usage\n\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n\n\n\nContent Creation and Communication\n\n\n\n\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\n\n\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\n\n\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\n\n\n\n\nResearch and Education\n\n\n\n\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\n\n\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\n\n\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\n\n\n\n\n\n\nUsing Gemma 2 with popular tooling\n\n\nLangChain\n\n\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.invoke(\"Why is the sky blue?\")\n\n\n\nLlamaIndex\n\n\nfrom llama_index.llms.ollama import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.complete(\"Why is the sky blue?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Ollama in a Noogler hat with the Gemma 2 logo](https://ollama.com/assets/library/gemma2/58a4be20-b402-4dfa-8f1d-05d820f1204f)\n\nGoogle's Gemma 2 model is available in three sizes, 2B, 9B and 27B, featuring a brand new architecture designed for class leading performance and efficiency.\n\n## Class leading performance \n\nAt 27 billion parameters, Gemma 2 delivers performance surpassing models more than twice its size in benchmarks. This breakthrough efficiency sets a new standard in the open model landscape. \n\n## Three sizes: 2B, 9B and 27B parameters\n\n* 2B Parameters `ollama run gemma2:2b`\n* 9B Parameters `ollama run gemma2`\n* 27B Parameters `ollama run gemma2:27b`\n\n## Benchmark\n\n![Benchmark](https://ollama.com/assets/library/gemma2/79663012-1c9c-4451-871b-4621f1a898d6)\n\n## Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n## Using Gemma 2 with popular tooling\n\n### LangChain\n\n```python\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.invoke(\"Why is the sky blue?\")\n```\n\n### LlamaIndex\n\n```python\nfrom llama_index.llms.ollama import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.complete(\"Why is the sky blue?\")\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "9b",
      "27b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma2:2b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "gemma2:latest",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "gemma2:27b",
        "size": "16GB",
        "size_gb": 16.0,
        "recommended_ram_gb": 20.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Translation",
      "Math"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers working with large language models.",
    "pulls": 16300000,
    "tags": 94,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "44c3fd81-5168-4865-bca5-3a816b8da6bf",
    "model_identifier": "phi3",
    "model_name": "phi3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi3",
    "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhi-3 is a family of open AI models developed by Microsoft.\n\n\nParameter sizes\n\n\n\n\nPhi-3 Mini\n – 3B parameters – \nollama run phi3:mini\n\n\nPhi-3 Medium\n – 14B parameters – \nollama run phi3:medium\n\n\n\n\nContext window sizes\n\n\n\n\nNote: the 128k version of this model requires \nOllama 0.1.39\n or later.\n\n\n\n\n\n\n4k \nollama run phi3:mini\n \nollama run phi3:medium\n\n\n128k \nollama run phi3:medium-128k\n\n\n\n\n\n\nPhi-3 Mini\n\n\nPhi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\n\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n\nPhi-3 Medium\n\n\nPhi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro.\n\n\n\n\nIntended Uses\n\n\nPrimary use cases\n\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require\n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\n\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n\nUse case considerations\n\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios.\nDevelopers  should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n\nResponsible AI Considerations\n\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n\n\n\nQuality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\n\n\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n\n\nInappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\n\n\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\n\n\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as “typing, math, random, collections, datetime, itertools”. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n\n\n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n\n\n\n\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n\n\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n\n\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n\n\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n\n\n\nTraining\n\n\nModel\n\n\n\n\nArchitecture: Phi-3 Mini has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n\nInputs: Text. It is best suited for prompts using chat format.\n\n\nContext length: 128K tokens\n\n\nGPUS: 512 H100-80G\n\n\nTraining time: 7 days\n\n\nTraining data: 3.3T tokens\n\n\nOutputs: Generated text in response to the input\n\n\nDates: Our models were trained between February and April 2024\n\n\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n\n\n\n\nDatasets\n\n\nOur training data includes a wide variety of sources, totaling 3.3 trillion tokens, and is a combination of\n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\n\nSoftware\n\n\n\n\nPyTorch\n\n\nDeepSpeed\n\n\nTransformers\n\n\nFlash-Attention\n\n\n\n\nLicense\n\n\nThe model is licensed under the \nMIT license\n.\n\n\nTrademarks\n\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow \nMicrosoft’s Trademark & Brand Guidelines\n. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n\n\nResources\n\n\n\n\nHuggingFace\n\n\nPhi-3 Microsoft Blog\n\n\nPhi-3 Technical Report\n\n\nPhi-3 on Azure AI Studio\n\n\nPhi-3 on Hugging Face\n\n\nPhi-3 ONNX: \n4K\n and \n128K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/e06e1a36-97b2-417a-b2b2-028c980359b1\" width=\"240\" />\n\nPhi-3 is a family of open AI models developed by Microsoft.\n\n## Parameter sizes\n\n- [Phi-3 Mini](https://ollama.com/library/phi3:mini) – 3B parameters – `ollama run phi3:mini`\n- [Phi-3 Medium](https://ollama.com/library/phi3:medium) – 14B parameters – `ollama run phi3:medium`\n\n## Context window sizes\n\n> Note: the 128k version of this model requires [Ollama 0.1.39](https://github.com/ollama/ollama/releases/tag/v0.1.39) or later.\n\n- 4k `ollama run phi3:mini` `ollama run phi3:medium`\n- 128k `ollama run phi3:medium-128k`\n\n![image.png](https://ollama.com/assets/library/phi3/83b3de66-82d8-4455-9117-256802c1b82e)\n\n## Phi-3 Mini\n\nPhi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n## Phi-3 Medium \n\nPhi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro. \n\n![image.png](https://ollama.com/assets/library/phi3/2868e29b-3bba-4c4a-a6ed-1a27fb102867)\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require\n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Use case considerations**\n\nOur models ",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3.8b",
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "phi3:latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "phi3:14b",
        "size": "7.9GB",
        "size_gb": 7.9,
        "recommended_ram_gb": 9.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 2.2,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Chat Assistant"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Advanced users and researchers for text generation and analysis tasks.",
    "pulls": 16000000,
    "tags": 72,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5b5f1dbd-2c45-4523-9658-92def52be216",
    "model_identifier": "llama3",
    "model_name": "llama3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3",
    "description": "Meta Llama 3: The most capable openly available LLM to date",
    "readme": "Readme\n\n\n\n\n\n\n\n\nLlama 3\n\n\nThe most capable openly available LLM to date.\n\n\n\n\nMeta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both \n8B\n and \n70B\n parameter sizes (pre-trained or instruction-tuned).\n\n\nLlama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.\n\n\n\n\n\n\nCLI\n\n\nOpen the terminal and run \nollama run llama3\n\n\nAPI\n\n\nExample using curl:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nAPI documentation\n\n\nModel variants\n\n\nInstruct\n is fine-tuned for chat/dialogue use cases.\n\n\nExample:\n\n\nollama run llama3\n\n\nollama run llama3:70b\n\n\nPre-trained\n is the base model.\n\n\nExample:\n\n\nollama run llama3:text\n\n\nollama run llama3:70b-text\n\n\nReferences\n\n\nIntroducing Meta Llama 3: The most capable openly available LLM to date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Llama 3\n\nThe most capable openly available LLM to date.\n\n<img src=\"https://github.com/ollama/ollama/assets/3325447/15750d75-668c-42bd-aaf2-d0d203136d55\" width=\"660\" />\n\nMeta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both **8B** and **70B** parameter sizes (pre-trained or instruction-tuned). \n\nLlama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.  \n\n<img src=\"https://github.com/ollama/ollama/assets/3325447/8910aebc-cd9e-4d2d-b9c2-258b5ac3eeac\" />\n\n\n<img src=\"https://github.com/ollama/ollama/assets/3325447/f6df22a6-fd54-4aa2-876b-2b9354821ec6\" />\n\n\n### CLI\n\nOpen the terminal and run `ollama run llama3`\n\n### API\n\nExample using curl:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n```\n\n[API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)\n\n## Model variants\n\n**Instruct** is fine-tuned for chat/dialogue use cases. \n\n*Example:*\n`ollama run llama3`\n`ollama run llama3:70b`\n\n**Pre-trained** is the base model. \n\n*Example:*\n`ollama run llama3:text`\n`ollama run llama3:70b-text`\n\n## References\n[Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "llama3:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Code Generation",
      "Translation",
      "Math"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 15900000,
    "tags": 68,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "89bfda17-2ede-432d-a5d5-d6ff2111dbaf",
    "model_identifier": "llava",
    "model_name": "llava",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llava",
    "description": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n🌋 LLaVA: Large Language and Vision Assistant\n\n\nLLaVA is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4.\n\n\nNew in LLaVA 1.6:\n\n\n\n\nIncreasing the input image resolution to up to 4x more pixels, supporting 672x672, 336x1344, 1344x336 resolutions.\n\n\nBetter visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\n\n\nBetter visual conversation for more scenarios, covering different applications.\n\n\nBetter world knowledge and logical reasoning.\n\n\n\n\nCLI Usage\n\n\nRun the model:\n\n\nollama run llava\n\n\n\nThen at the prompt, include the path to your image in the prompt:\n\n\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n\n\n\nAPI Usage\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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\"]\n}'\n\n\n\nReferences\n\n\nWebsite\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 🌋 LLaVA: Large Language and Vision Assistant\n\nLLaVA is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4.\n\n### New in LLaVA 1.6:\n* Increasing the input image resolution to up to 4x more pixels, supporting 672x672, 336x1344, 1344x336 resolutions.\n* Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\n* Better visual conversation for more scenarios, covering different applications.\n* Better world knowledge and logical reasoning.\n\n### CLI Usage\n\nRun the model:\n\n```\nollama run llava\n```\n\nThen at the prompt, include the path to your image in the prompt:\n\n```\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n```\n\n### API Usage\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "7b",
      "13b",
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "llava:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "llava:13b",
        "size": "8.0GB",
        "size_gb": 8.0,
        "recommended_ram_gb": 10.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llava:34b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Chat Assistant",
      "Image Understanding",
      "Reasoning"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Handling complex visual and language tasks requiring multimodal understanding.",
    "pulls": 12900000,
    "tags": 98,
    "last_updated": "2026-02-25",
    "last_updated_str": "to version 1.6.",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "fa21f98b-f4c8-4609-b7c3-fd34e62942ce",
    "model_identifier": "qwen2.5-coder",
    "model_name": "qwen2.5-coder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2.5-coder",
    "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen 2.5 Coder series of models are now updated in 6 sizes: \n0.5B, 1.5B, 3B, 7B, 14B and 32B\n.\n\n\nThere are significant improvements in \ncode generation\n, \ncode reasoning\n and \ncode fixing\n. The 32B model has competitive performance with OpenAI’s GPT-4o.\n\n\n32B:\n\n\nollama run qwen2.5-coder:32b\n\n\n14B:\n\n\nollama run qwen2.5-coder:14b\n\n\n7B:\n\n\nollama run qwen2.5-coder:7b\n\n\n3B:\n\n\nollama run qwen2.5-coder:3b\n\n\n1.5B:\n\n\nollama run qwen2.5-coder:1.5b\n\n\n0.5B:\n\n\nollama run qwen2.5-coder:0.5b\n\n\nCode capabilities reaching state of the art for open-source models\n\n\n\n\nCode Generation:\n Qwen2.5 Coder 32B Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.\n\n\nCode Repair:\n Code repair is an important programming skill. Qwen2.5 Coder 32B Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5 Coder 32B Instruct scored 73.7, performing comparably to GPT-4o on Aider.\n\n\nCode Reasoning:\n Code reasoning refers to the model’s ability to learn the process of code execution and accurately predict the model’s inputs and outputs. The recently released Qwen2.5 Coder 7B Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.\n\n\n\n\nMultiple programming languages\n\n\nAn intelligent programming assistant should be familiar with all programming languages. Qwen 2.5 Coder 32B performs excellent across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket. The Qwen team used their own unique data cleaning and balancing during the pre-training phase.\n\n\n\n\nAdditionally, the multi-language code repair capabilities of Qwen 2.5 Coder 32B Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen 2.5 Coder 32B Instruct scored 75.2, ranking first among all open-source models.\n\n\n\n\nHuman Preference\n\n\nTo evaluate the alignment performance of Qwen 2.5 Coder 32B Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an ‘A vs. B win’ evaluation method, which measures the percentage of instances in the test set where model A’s score exceeds model B’s. The results below demonstrate the advantages of Qwen 2.5 Coder 32B Instruct in preference alignment.\n\n\n\n\nComprehensive model sizes to fit your device\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/qwen2.5-coder/59b3c116-2653-4d50-9b5f-1fefd24a61bf\" width=\"450\" />\n\nQwen 2.5 Coder series of models are now updated in 6 sizes: **0.5B, 1.5B, 3B, 7B, 14B and 32B**. \n\nThere are significant improvements in **code generation**, **code reasoning** and **code fixing**. The 32B model has competitive performance with OpenAI's GPT-4o. \n\n**32B:** \n`ollama run qwen2.5-coder:32b`\n\n**14B:** \n`ollama run qwen2.5-coder:14b`\n\n**7B:** \n`ollama run qwen2.5-coder:7b`\n\n**3B:**\n`ollama run qwen2.5-coder:3b` \n\n**1.5B:**\n`ollama run qwen2.5-coder:1.5b`\n\n**0.5B:**\n`ollama run qwen2.5-coder:0.5b`\n\n### Code capabilities reaching state of the art for open-source models \n\n![Comparison benchmarks](/assets/library/qwen2.5-coder/05059413-3cc4-4b07-b546-001594d0ae26)\n\n**Code Generation:** Qwen2.5 Coder 32B Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.\n\n**Code Repair:** Code repair is an important programming skill. Qwen2.5 Coder 32B Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5 Coder 32B Instruct scored 73.7, performing comparably to GPT-4o on Aider.\n\n**Code Reasoning:** Code reasoning refers to the model’s ability to learn the process of code execution and accurately predict the model’s inputs and outputs. The recently released Qwen2.5 Coder 7B Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.\n\n![Benchmarks](/assets/library/qwen2.5-coder/0bd9e1aa-a87b-474b-84ba-264a85041605)\n\n### Multiple programming languages\nAn intelligent programming assistant should be familiar with all programming languages. Qwen 2.5 Coder 32B performs excellent across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket. The Qwen team used their own unique data cleaning and balancing during the pre-training phase. \n\n![McEval Performance](/assets/library/qwen2.5-coder/6436978b-1371-48a4-a21a-b6da729b74e1)\n\nAdditionally, the multi-language code repair capabilities of Qwen 2.5 Coder 32B Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen 2.5 Coder 32B Instruct scored 75.2, ranking first among all open-source models.\n\n![MdEval Performance](/assets/library/qwen2.5-coder/f2401bd6-f6d7-41ca-981d-98abc62f1493)\n\n### Human Preference \n\nTo evaluate the alignment performance of Qwen 2.5 Coder 32B Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an ‘A vs. B win’ evaluation method, which measures the percentage of instances in the test set where model A’s score exceeds model B’s. The results below demonstrate the advantages of Qwen 2.5 Coder 32B Instruct in preference alignment.\n\n![human preference](/assets/library/qwen2.5-coder/bbf378d8-c80e-4ae3-98ab-90111dfbf3e7)\n\n### Comprehensive model sizes to fit your device\n\n![Model sizes](/assets/library/qwen2.5-coder/752764ea-d510-4bc5-8658-dc5d8ba51019)\n\n## References\n\n[Blog Post](https://qwenlm.github.io/blog/qwen2.5-coder-family/)\n\n[HuggingFace](https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "0.5b",
      "1.5b",
      "3b",
      "7b",
      "14b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2.5-coder:3b",
        "size": "1.9GB",
        "size_gb": 1.9,
        "recommended_ram_gb": 2.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5-coder:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5-coder:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5-coder:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.9,
    "use_cases": [
      "Code Generation",
      "Code Review",
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working on code-related tasks.",
    "pulls": 11200000,
    "tags": 199,
    "last_updated": "2025-05-25",
    "last_updated_str": "9 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "e79a0bb9-38d0-462c-8897-dc4ad8e48495",
    "model_identifier": "mxbai-embed-large",
    "model_name": "mxbai-embed-large",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mxbai-embed-large",
    "description": "State-of-the-art large embedding model from mixedbread.ai",
    "readme": "Readme\n\n\n\n\n\n\n\n\nmxbai-embed-large\n\n\n\n\nAs of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs \ntext-embedding-3-large\n model and matches the performance of model 20x its size.\n\n\nmxbai-embed-large\n was trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length.\n\n\nUsage\n\n\nREST API\n\n\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\"\n}'\n\n\n\nPython library\n\n\nollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering')\n\n\n\nJavascript library\n\n\nollama.embeddings({ model: 'mxbai-embed-large', prompt: 'Represent this sentence for searching relevant passages:  The sky is blue because of Rayleigh scattering' })\n\n\n\nReferences\n\n\nBlog post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## mxbai-embed-large\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/215cfb6a-8efa-4e9b-824d-e5f466b58c49\" widht=\"400\">\n\nAs of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs `text-embedding-3-large` model and matches the performance of model 20x its size.\n\n`mxbai-embed-large` was trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length.\n\n## Usage\n\n\n### REST API\n\n```\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\"\n}'\n```\n\n### Python library\n\n```\nollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering')\n```\n\n### Javascript library\n\n```\nollama.embeddings({ model: 'mxbai-embed-large', prompt: 'Represent this sentence for searching relevant passages:  The sky is blue because of Rayleigh scattering' })\n```\n\n\n## References\n\n[Blog post](https://www.mixedbread.ai/blog/mxbai-embed-large-v1)\n\n[Hugging Face](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "335m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "For generating embeddings for text analysis and retrieval",
    "pulls": 7600000,
    "tags": 4,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8ae93762-56dc-4e15-8c39-afbea6f5ead0",
    "model_identifier": "phi4",
    "model_name": "phi4",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi4",
    "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhi-4\n is a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets.\n\n\n\n\nThe model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n\nContext length:\n 16k tokens\n\n\n\n\nPrimary use cases\n\n\nThe model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n\n\n\nMemory/compute constrained environments.\n\n\nLatency bound scenarios.\n\n\nReasoning and logic.\n\n\n\n\nOut-of-scope use cases\n\n\nThe models are not specifically designed or evaluated for all downstream purposes, thus:\n\n\n\n\nDevelopers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n\n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\n\n\nNothing contained in this readme should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" />\n\n**Phi-4** is a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. \n\n![Phi-4 benchmark](/assets/library/phi4/67391ae2-e565-4173-ac3c-5e49bc977ac4)\n\nThe model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. \n\n**Context length:** 16k tokens\n\n![Phi-4 performance eval by Microsoft](/assets/library/phi4/5da7d0b6-53df-42f9-9c1b-651bfc380cdd)\n\n\n### Primary use cases\n\nThe model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n1. Memory/compute constrained environments.\n2. Latency bound scenarios.\n3. Reasoning and logic.\n\n### Out-of-scope use cases \n\nThe models are not specifically designed or evaluated for all downstream purposes, thus:\n\n1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\n3. Nothing contained in this readme should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "phi4:latest",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 9.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Code Generation",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "AI systems and applications requiring precise instruction adherence and robust safety measures.",
    "pulls": 7200000,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "93487057-bedb-4c80-83fa-d6d20dd6f27f",
    "model_identifier": "gpt-oss",
    "model_name": "gpt-oss",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gpt-oss",
    "description": "OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nWelcome OpenAI’s gpt-oss!\n\n\nOllama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\n\nGet started\n\n\nYou can get started by \ndownloading the latest Ollama version\n.\n\n\nThe model can be downloaded directly in Ollama’s new app or via the terminal:\n\n\nollama run gpt-oss:20b\n\n\nollama run gpt-oss:120b\n\n\nFeature highlights\n\n\n\n\nAgentic capabilities:\n Use the models’ native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs.\n\n\nFull chain-of-thought:\n Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs.\n\n\nConfigurable reasoning effort:\n Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n\n\nFine-tunable:\n Fully customize models to your specific use case through parameter fine-tuning.\n\n\nPermissive Apache 2.0 license:\n Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n\n\n\n\n\n\nQuantization - MXFP4 format\n\n\nOpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU.\n\n\nOllama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama’s new engine to support the MXFP4 format.\n\n\nOllama collaborated with OpenAI to benchmark against their reference implementations to ensure Ollama’s implementations have the same quality.\n\n\n20B parameter model\n\n\n\n\ngpt-oss-20b model is designed for lower latency, local, or specialized use-cases.\n\n\n120B parameter model\n\n\n\n\nReference\n\n\n\n\nOpenAI launch blog\n\n\nOpenAI model card\n\n\nNVIDIA RTX blog\n\n\nNVIDIA cloud to edge\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![OpenAI gpt-oss banner](/assets/library/gpt-oss/e9da5025-e172-441d-9f06-8dfa797da9b0)\n\n### Welcome OpenAI's gpt-oss! \n\nOllama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases. \n\n### Get started \n\nYou can get started by [downloading the latest Ollama version](https://ollama.com/download). \n\nThe model can be downloaded directly in Ollama’s new app or via the terminal: \n\n**ollama run gpt-oss:20b** \n\n**ollama run gpt-oss:120b**\n\n### Feature highlights\n\n- **Agentic capabilities:** Use the models’ native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs.\n- **Full chain-of-thought:** Gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs.\n- **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n- **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n- **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n\n![benchmark](/assets/library/gpt-oss/d29a500d-303f-4d24-b557-a8247d17ed4c)\n\n### Quantization - MXFP4 format\n\nOpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU. \n\nOllama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama’s new engine to support the MXFP4 format. \n\nOllama collaborated with OpenAI to benchmark against their reference implementations to ensure Ollama’s implementations have the same quality. \n\n### 20B parameter model \n\n![gpt-oss 20B](/assets/library/gpt-oss/343d558d-a3bc-472e-9956-37fdc3cc4f6c)\n\ngpt-oss-20b model is designed for lower latency, local, or specialized use-cases. \n\n\n### 120B parameter model \n\n![gpt-oss 120B](/assets/library/gpt-oss/51b69d33-c747-4117-ba76-a6efa1b0a986)\n\n### Reference \n\n- [OpenAI launch blog](https://openai.com/index/introducing-gpt-oss)\n- [OpenAI model card](https://openai.com/index/gpt-oss-model-card/)\n- [NVIDIA RTX blog](https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss)\n- [NVIDIA cloud to edge](https://developer.nvidia.com/blog/delivering-1-5-m-tps-inference-on-nvidia-gb200-nvl72-nvidia-accelerates-openai-gpt-oss-models-from-cloud-to-edge/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "20b",
      "120b"
    ],
    "memory_requirements": [
      {
        "tag": "gpt-oss:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "gpt-oss:120b",
        "size": "65GB",
        "size_gb": 65.0,
        "recommended_ram_gb": 81.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 14.0,
    "use_cases": [
      "Reasoning",
      "Function Calling",
      "Role Play"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for advanced reasoning tasks and function calling.",
    "pulls": 7100000,
    "tags": 5,
    "last_updated": "2025-10-25",
    "last_updated_str": "4 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "b302fb9e-ae2d-48eb-8fff-2ed41bcfd251",
    "model_identifier": "gemma",
    "model_name": "gemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma",
    "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.1.26 or later. \nDownload it here\n.\n\n\n\n\nGemma is a new open model developed by Google and its DeepMind team. It’s inspired by Gemini models at Google.\n\n\nGemma is available in both \n2b\n and \n7b\n parameter sizes:\n\n\n\n\nollama run gemma:2b\n\n\nollama run gemma:7b\n (default)\n\n\n\n\nThe models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning.\n\n\nTo ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies.\n\n\nReference\n\n\nGoogle Gemma\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/01333db3-c27b-4044-88b3-9b2ffbe06415\"/>\n\n> Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download).\n\nGemma is a new open model developed by Google and its DeepMind team. It’s inspired by Gemini models at Google.\n\nGemma is available in both `2b` and `7b` parameter sizes:\n\n* `ollama run gemma:2b`\n* `ollama run gemma:7b` (default)\n\nThe models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning.\n\nTo ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies.\n\n# Reference\n\n[Google Gemma](https://ai.google.dev/gemma/docs/model_card)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma:2b",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "gemma:latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.7,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers for text generation and code tasks.",
    "pulls": 5900000,
    "tags": 102,
    "last_updated": "2026-02-25",
    "last_updated_str": "to version 1.1",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c4d320b8-0152-46b6-b9ac-b98a0bbe184b",
    "model_identifier": "qwen",
    "model_name": "qwen",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen",
    "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
    "readme": "Readme\n\n\n\n\n\n\n\n\nQwen 2 is now available \nhere\n.\n\n\n\n\nQwen is a series of transformer-based large language models by Alibaba Cloud, pre-trained on a large volume of data, including web texts, books, code, etc.\n\n\nNew in Qwen 1.5\n\n\n\n\n6 model sizes, including 0.5B, 1.8B, 4B (default), 7B, 14B, 32B (new) and 72B\n\n\n\n\nollama run qwen:0.5b\n\n\nollama run qwen:1.8b\n\n\nollama run qwen:4b\n\n\nollama run qwen:7b\n\n\nollama run qwen:14b\n\n\nollama run qwen:32b\n\n\nollama run qwen:72b\n\n\nollama run qwen:110b\n\n\n\n\nSignificant performance improvement in human preference for chat models\n\n\nMultilingual support of both base and chat models\n\n\nStable support of 32K context length for models of all sizes\n\n\n\n\nThe original Qwen model is offered in four different parameter sizes: 1.8B, 7B, 14B, and 72B.\n\n\nFeatures\n\n\n\n\nLow-cost deployment\n: the minimum memory requirement for inference is less than 2GB.\n\n\nLarge-scale high-quality training corpora\n: Models are pre-trained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.\n\n\nGood performance\n: Qwen supports long context lengths (8K on the \n1.8b\n, \n7b\n and \n14b\n parameter models, and 32K on the \n72b\n parameter model), and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including common-sense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks.\n\n\nMore comprehensive vocabulary coverage\n: Compared with other open-source models based on Chinese and English vocabularies, Qwen uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.\n\n\nSystem prompt\n: Qwen can realize role playing, language style transfer, task setting, and behavior-setting by using a system prompt.\n\n\n\n\nReference\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQwen 2 is now available [here](https://ollama.com/library/qwen2). \n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/160726a3-a986-427d-b1df-3b75357903a4\" width=\"280\" />\n\nQwen is a series of transformer-based large language models by Alibaba Cloud, pre-trained on a large volume of data, including web texts, books, code, etc. \n\n### New in Qwen 1.5 \n\n- 6 model sizes, including 0.5B, 1.8B, 4B (default), 7B, 14B, 32B (new) and 72B\n  * `ollama run qwen:0.5b`\n  * `ollama run qwen:1.8b`\n  * `ollama run qwen:4b` \n  * `ollama run qwen:7b` \n  * `ollama run qwen:14b`\n  * `ollama run qwen:32b`\n  * `ollama run qwen:72b`\n  * `ollama run qwen:110b`\n- Significant performance improvement in human preference for chat models\n- Multilingual support of both base and chat models\n- Stable support of 32K context length for models of all sizes\n\nThe original Qwen model is offered in four different parameter sizes: 1.8B, 7B, 14B, and 72B. \n\n## Features\n\n* **Low-cost deployment**: the minimum memory requirement for inference is less than 2GB.\n\n* **Large-scale high-quality training corpora**: Models are pre-trained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.\n\n* **Good performance**: Qwen supports long context lengths (8K on the `1.8b`, `7b` and `14b` parameter models, and 32K on the `72b` parameter model), and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including common-sense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks.\n\n* **More comprehensive vocabulary coverage**: Compared with other open-source models based on Chinese and English vocabularies, Qwen uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.\n\n* **System prompt**: Qwen can realize role playing, language style transfer, task setting, and behavior-setting by using a system prompt.\n\n\n## Reference\n\n[GitHub](https://github.com/QwenLM/Qwen)\n\n[Hugging Face](https://huggingface.co/Qwen)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "0.5b",
      "1.8b",
      "4b",
      "7b",
      "14b",
      "32b",
      "72b",
      "110b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen:1.8b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "than",
        "size": "2GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": ".",
        "context_window": null
      },
      {
        "tag": "qwen:latest",
        "size": "2.3GB",
        "size_gb": 2.3,
        "recommended_ram_gb": 2.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "2.3GB",
        "size_gb": 2.3,
        "recommended_ram_gb": 2.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen:7b",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:14b",
        "size": "8.2GB",
        "size_gb": 8.2,
        "recommended_ram_gb": 10.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:32b",
        "size": "18GB",
        "size_gb": 18.0,
        "recommended_ram_gb": 22.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:72b",
        "size": "41GB",
        "size_gb": 41.0,
        "recommended_ram_gb": 51.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:110b",
        "size": "63GB",
        "size_gb": 63.0,
        "recommended_ram_gb": 78.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for advanced language tasks.",
    "pulls": 5600000,
    "tags": 379,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9747750f-b222-4b7e-b2f7-f5bcd01e69eb",
    "model_identifier": "llama2",
    "model_name": "llama2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama2",
    "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLlama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat.\n\n\nCLI\n\n\nOpen the terminal and run \nollama run llama2\n\n\nAPI\n\n\nExample using curl:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nAPI documentation\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n70b models generally require at least 64GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nChat\n is fine-tuned for chat/dialogue use cases. These are the default in Ollama, and for models tagged with -chat in the tags tab.\n\n\nExample: \nollama run llama2\n\n\nPre-trained\n is without the chat fine-tuning. This is tagged as -text in the tags tab.\n\n\nExample: \nollama run llama2:text\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\nReferences\n\n\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n\n\nMeta’s Hugging Face repo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/286e4e13-0b2b-43a4-9b07-23a50d3a3d33\" width=\"360\" />\n\nLlama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat.\n\n### CLI\n\nOpen the terminal and run `ollama run llama2`\n\n### API\n\nExample using curl:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n```\n\n[API documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md)\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n- 70b models generally require at least 64GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n**Chat** is fine-tuned for chat/dialogue use cases. These are the default in Ollama, and for models tagged with -chat in the tags tab.\n\n*Example: `ollama run llama2`*\n\n**Pre-trained** is without the chat fine-tuning. This is tagged as -text in the tags tab.\n\n*Example: `ollama run llama2:text`*\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n## References\n[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n\n[Meta's Hugging Face repo](https://huggingface.co/meta-llama)\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama2:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama2:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama2:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Code Generation"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex language tasks.",
    "pulls": 5500000,
    "tags": 102,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c0662cda-8285-45ef-b542-54c125d61dfe",
    "model_identifier": "qwen2",
    "model_name": "qwen2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2",
    "description": "Qwen2 is a new series of large language models from Alibaba group",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen2\n is trained on data in \n29 languages\n, including \nEnglish and Chinese\n.\n\n\nIt is available in 4 parameter sizes: \n0.5B\n, \n1.5B\n, \n7B\n, \n72B\n.\n\n\nIn the 7B and 72B models, context length has been extended to \n128k tokens\n.\n\n\n\n\n\n\n\n\nModels\n\n\nQwen2-0.5B\n\n\nQwen2-1.5B\n\n\nQwen2-7B\n\n\nQwen2-72B\n\n\n\n\n\n\n\n\n\n\nParams\n\n\n0.49B\n\n\n1.54B\n\n\n7.07B\n\n\n72.71B\n\n\n\n\n\n\nNon-Emb Params\n\n\n0.35B\n\n\n1.31B\n\n\n5.98B\n\n\n70.21B\n\n\n\n\n\n\nGQA\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\n\n\nTie Embedding\n\n\nTrue\n\n\nTrue\n\n\nFalse\n\n\nFalse\n\n\n\n\n\n\nContext Length\n\n\n32K\n\n\n32K\n\n\n128K\n\n\n128K\n\n\n\n\n\n\n\n\nSupported languages\n\n\nThis is in addition to English and Chinese\n\n\n\n\n\n\n\n\nRegions\n\n\nLanguages\n\n\n\n\n\n\n\n\n\n\nWestern Europe\n\n\nGerman, French, Spanish, Portuguese, Italian, Dutch\n\n\n\n\n\n\nEastern & Central Europe\n\n\nRussian, Czech, Polish\n\n\n\n\n\n\nMiddle East\n\n\nArabic, Persian, Hebrew, Turkish\n\n\n\n\n\n\nEastern Asia\n\n\nJapanese, Korean\n\n\n\n\n\n\nSouth-Eastern Asia\n\n\nVietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog\n\n\n\n\n\n\nSouthern Asia\n\n\nHindi, Bengali, Urdu\n\n\n\n\n\n\n\n\nPerformance\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\nAll models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed.\n\n\nQwen2 72B model still uses the original Qianwen License.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/qwen2/c011288c-fb07-42eb-b802-7287a5f12ea6\" width=\"320\" />\n\n**Qwen2** is trained on data in **29 languages**, including **English and Chinese**. \n\nIt is available in 4 parameter sizes: **0.5B**, **1.5B**, **7B**, **72B**. \n\nIn the 7B and 72B models, context length has been extended to **128k tokens**. \n\nModels | Qwen2-0.5B | Qwen2-1.5B | Qwen2-7B | Qwen2-72B\n-- | -- | -- | -- | -- \nParams | 0.49B | 1.54B | 7.07B | 72.71B\nNon-Emb Params | 0.35B | 1.31B | 5.98B | 70.21B\nGQA | True | True | True | True\nTie Embedding | True | True | False | False\nContext Length | 32K | 32K | 128K | 128K\n\n### Supported languages\n\nThis is in addition to English and Chinese \n\nRegions | Languages\n-- | --\nWestern Europe | German, French, Spanish, Portuguese, Italian, Dutch\nEastern & Central Europe | Russian, Czech, Polish\nMiddle East | Arabic, Persian, Hebrew, Turkish\nEastern Asia | Japanese, Korean\nSouth-Eastern Asia | Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog\nSouthern Asia | Hindi, Bengali, Urdu\n\n### Performance \n\n![image.png](https://ollama.com/assets/library/qwen2/68b445e3-bf1b-4fff-9621-4e5bbf4a72a2)\n\n![image.png](https://ollama.com/assets/library/qwen2/72e9bf41-f8d6-4b7a-a7ef-9599ef533af6)\n\n![image.png](https://ollama.com/assets/library/qwen2/6c978d72-c37c-45a2-b7f4-c06178c0182c)\n\n![image.png](https://ollama.com/assets/library/qwen2/5247046f-3c32-4edc-a8e1-a10f831ef916)\n\n### License \n\nAll models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed. \n\nQwen2 72B model still uses the original Qianwen License.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "0.5b",
      "1.5b",
      "7b",
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2:latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen2:72b",
        "size": "41GB",
        "size_gb": 41.0,
        "recommended_ram_gb": 51.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.4,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese",
      "Arabic",
      "Japanese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 4900000,
    "tags": 97,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1a49a3f4-f2fa-484e-ae12-fe3530561384",
    "model_identifier": "minicpm-v",
    "model_name": "minicpm-v",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/minicpm-v",
    "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.3.10\n or later.\n\n\n\n\nMiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\n\n\n\n\n🔥 Leading Performance\n: MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding.\n\n\n🖼️ Multi Image Understanding and In-context Learning\n. MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\n\n\n💪 Strong OCR Capability\n: MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro. Based on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc.\n\n\n🚀 Superior Efficiency\n: In addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption.\n\n\n\n\nRefrences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/minicpm-v/9252c73d-2c9c-434c-8a34-21f4d5cdd25e\" width=\"320\" />\n\n> Note: this model requires [Ollama 0.3.10](https://github.com/ollama/ollama/releases/tag/v0.3.10) or later.\n\nMiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\n\n* **🔥 Leading Performance**: MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding.\n\n* **🖼️ Multi Image Understanding and In-context Learning**. MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\n\n* **💪 Strong OCR Capability**: MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro. Based on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc.\n\n* **🚀 Superior Efficiency**: In addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption.\n\n## Refrences\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-V)\n\n[Hugging Face](https://huggingface.co/openbmb/MiniCPM-V-2_6)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "minicpm-v:latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 5.5,
    "use_cases": [
      "Question Answering",
      "Image Understanding",
      "Text Summarization"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for image understanding tasks.",
    "pulls": 4600000,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0726f727-0658-49b5-b436-29955f9ac7e5",
    "model_identifier": "codellama",
    "model_name": "codellama",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codellama",
    "description": "A large language model that can use text prompts to generate and discuss code.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCode Llama is a model for generating and discussing code, built on top of \nLlama 2\n. It’s designed to make workflows faster and efficient for developers and make it easier for people to learn how to code. It can generate both code and natural language about code. Code Llama supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash and more.\n\n\nParameter counts\n\n\n\n\n\n\n\n\nParameter Count\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 billion\n\n\nView\n\n\nollama run codellama:7b\n\n\n\n\n\n\n13 billion\n\n\nView\n\n\nollama run codellama:13b\n\n\n\n\n\n\n34 billion\n\n\nView\n\n\nollama run codellama:34b\n\n\n\n\n\n\n70 billion\n\n\nView\n\n\nollama run codellama:70b\n\n\n\n\n\n\n\n\nUsage\n\n\nCLI\n\n\nollama run codellama \"Write me a function that outputs the fibonacci sequence\"\n\n\n\nAPI\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama\",\n  \"prompt\": \"Write me a function that outputs the fibonacci sequence\"\n}'\n\n\n\nVariations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstruct\n\n\nFine-tuned to generate helpful and safe answers in natural language\n\n\n\n\n\n\npython\n\n\nA specialized variation of Code Llama further fine-tuned on 100B tokens of Python code\n\n\n\n\n\n\ncode\n\n\nBase model for code completion\n\n\n\n\n\n\n\n\nExample prompts\n\n\nAsk questions\n\n\nollama run codellama:7b-instruct 'You are an expert programmer that writes simple, concise code and explanations. Write a python function to generate the nth fibonacci number.'\n\n\n\nFill-in-the-middle (FIM) or infill\n\n\nollama run codellama:7b-code '<PRE> def compute_gcd(x, y): <SUF>return result <MID>'\n\n\n\nFill-in-the-middle (FIM) is a special prompt format supported by the code completion model can complete code between two already written code blocks. Code Llama expects a specific format for infilling code:\n\n\n<PRE> {prefix} <SUF>{suffix} <MID>\n\n\n\n<PRE>\n, \n<SUF>\n and \n<MID>\n are special tokens that guide the model.\n\n\nCode review\n\n\nollama run codellama '\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n'\n\n\n\nWriting tests\n\n\nollama run codellama \"write a unit test for this function: $(cat example.py)\"\n\n\n\nCode completion\n\n\nollama run codellama:7b-code '# A simple python function to remove whitespace from a string:'\n\n\n\nMore information\n\n\n\n\nHow to prompt Code Llama\n\n\nWhitepaper\n\n\nCodeLlama GitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/286e4e13-0b2b-43a4-9b07-23a50d3a3d33\" width=\"360\" />\n\nCode Llama is a model for generating and discussing code, built on top of [Llama 2](https://ollama.ai/library/llama2). It's designed to make workflows faster and efficient for developers and make it easier for people to learn how to code. It can generate both code and natural language about code. Code Llama supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash and more.\n\n## Parameter counts\n\n| Parameter Count |                                |                            |\n| --------------- | ------------------------------ | -------------------------- |\n| 7 billion       | [View](/library/codellama:7b)  | `ollama run codellama:7b`  |\n| 13 billion      | [View](/library/codellama:13b) | `ollama run codellama:13b` |\n| 34 billion      | [View](/library/codellama:34b) | `ollama run codellama:34b` |\n| 70 billion      | [View](/library/codellama:70b) | `ollama run codellama:70b` |\n\n## Usage\n\n### CLI\n\n```\nollama run codellama \"Write me a function that outputs the fibonacci sequence\"\n```\n\n### API\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama\",\n  \"prompt\": \"Write me a function that outputs the fibonacci sequence\"\n}'\n```\n\n## Variations\n\n|            |                                                                                        |\n| ---------- | -------------------------------------------------------------------------------------- |\n| `instruct` | Fine-tuned to generate helpful and safe answers in natural language                    |\n| `python`   | A specialized variation of Code Llama further fine-tuned on 100B tokens of Python code |\n| `code`     | Base model for code completion                                                         |\n\n## Example prompts\n\n### Ask questions\n\n```\nollama run codellama:7b-instruct 'You are an expert programmer that writes simple, concise code and explanations. Write a python function to generate the nth fibonacci number.'\n```\n\n### Fill-in-the-middle (FIM) or infill\n\n```\nollama run codellama:7b-code '<PRE> def compute_gcd(x, y): <SUF>return result <MID>'\n```\n\nFill-in-the-middle (FIM) is a special prompt format supported by the code completion model can complete code between two already written code blocks. Code Llama expects a specific format for infilling code:\n\n```\n<PRE> {prefix} <SUF>{suffix} <MID>\n```\n\n`<PRE>`, `<SUF>` and `<MID>` are special tokens that guide the model.\n\n### Code review\n\n```\nollama run codellama '\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n'\n```\n\n### Writing tests\n\n```\nollama run codellama \"write a unit test for this function: $(cat example.py)\"\n```\n\n### Code completion\n\n```\nollama run codellama:7b-code '# A simple python function to remove whitespace from a string:'\n```\n\n## More information\n\n- [How to prompt Code Llama](https://ollama.ai/blog/how-to-prompt-code-llama)\n- [Whitepaper](https://arxiv.org/abs/2308.12950)\n- [CodeLlama GitHub](https://github.com/facebookresearch/codellama)\n- [Hugging Face](https://huggingface.co/codellama)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "34b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "codellama:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      },
      {
        "tag": "codellama:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "codellama:34b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "codellama:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers who need a code generation model.",
    "pulls": 4300000,
    "tags": 199,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5da0ca1e-19f2-4245-832f-c508140d13d6",
    "model_identifier": "llama3.2-vision",
    "model_name": "llama3.2-vision",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.2-vision",
    "description": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\n\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\n\n\nUsage\n\n\nFirst, pull the model:\n\n\nollama pull llama3.2-vision\n\n\n\nPython Library\n\n\nTo use Llama 3.2 Vision with the Ollama \nPython library\n:\n\n\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2-vision',\n    messages=[{\n        'role': 'user',\n        'content': 'What is in this image?',\n        'images': ['image.jpg']\n    }]\n)\n\nprint(response)\n\n\n\nJavaScript Library\n\n\nTo use Llama 3.2 Vision with the Ollama \nJavaScript library\n:\n\n\nimport ollama from 'ollama'\n\nconst response = await ollama.chat({\n  model: 'llama3.2-vision',\n  messages: [{\n    role: 'user',\n    content: 'What is in this image?',\n    images: ['image.jpg']\n  }]\n})\n\nconsole.log(response)\n\n\n\ncURL\n\n\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2-vision\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"<base64-encoded image data>\"]\n    }\n  ]\n}'\n\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/llama3.2-vision/ea1fa75c-0d15-453d-a291-ce2d97d8646a\" width=\"280\" />\n\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\n\n## Usage\n\nFirst, pull the model:\n\n```bash\nollama pull llama3.2-vision\n```\n\n### Python Library\n\nTo use Llama 3.2 Vision with the Ollama [Python library](https://github.com/ollama/ollama-python):\n\n```python\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2-vision',\n    messages=[{\n        'role': 'user',\n        'content': 'What is in this image?',\n        'images': ['image.jpg']\n    }]\n)\n\nprint(response)\n```\n\n### JavaScript Library\n\nTo use Llama 3.2 Vision with the Ollama [JavaScript library](https://github.com/ollama/ollama-js):\n\n```javascript\nimport ollama from 'ollama'\n\nconst response = await ollama.chat({\n  model: 'llama3.2-vision',\n  messages: [{\n    role: 'user',\n    content: 'What is in this image?',\n    images: ['image.jpg']\n  }]\n})\n\nconsole.log(response)\n```\n\n### cURL\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2-vision\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"<base64-encoded image data>\"]\n    }\n  ]\n}'\n```\n\n## References\n\n[GitHub](https://github.com/meta-llama/llama-models)\n\n[HuggingFace](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "11b",
      "90b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.2-vision:latest",
        "size": "7.8GB",
        "size_gb": 7.8,
        "recommended_ram_gb": 9.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "7.8GB",
        "size_gb": 7.8,
        "recommended_ram_gb": 9.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "llama3.2-vision:90b",
        "size": "55GB",
        "size_gb": 55.0,
        "recommended_ram_gb": 68.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 7.8,
    "use_cases": [
      "Image Understanding",
      "Question Answering",
      "Text Summarization",
      "Reasoning",
      "Translation"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers working with multimodal data.",
    "pulls": 3800000,
    "tags": 9,
    "last_updated": "2025-05-25",
    "last_updated_str": "9 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ff8d7f97-c400-4486-9434-db4f4976aae4",
    "model_identifier": "tinyllama",
    "model_name": "tinyllama",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/tinyllama",
    "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nTinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n\nReferences\n\n\nHugging Face\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/6f17d445-d019-4e54-bac8-9cb6b3b01a26\" width=\"320\" />\n\nTinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n## References\n\n[Hugging Face](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6)\n\n[GitHub](https://github.com/jzhang38/TinyLlama)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.1b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Chat Assistant"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers who need a compact model for various language tasks.",
    "pulls": 3700000,
    "tags": 36,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "30ab6156-7678-4b8e-914c-9c9f769d06ef",
    "model_identifier": "dolphin3",
    "model_name": "dolphin3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphin3",
    "description": "Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\n\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products.\n\n\n\n\nThey maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\n\n\nThey maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\n\n\nThey maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\n\n\nThey can see all your queries and they can potentially use that data in ways you wouldn’t want. Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt. You decide the alignment. You have control of your data. Dolphin does not impose its ethics or guidelines on you. You are the one who decides the guidelines.\n\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/dolphin3/ce75cebe-b012-4195-9a6b-aa6c4b68f93f\" width=\"360\" />\n\nDolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products.\n\n1. They maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\n2. They maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\n3. They maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\n4. They can see all your queries and they can potentially use that data in ways you wouldn't want. Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt. You decide the alignment. You have control of your data. Dolphin does not impose its ethics or guidelines on you. You are the one who decides the guidelines.\n\n## References\n\n[Hugging Face](https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.1-8B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphin3:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 4.9,
    "use_cases": [
      "Code Generation",
      "Function Calling",
      "Role Play"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Developers and researchers seeking a general-purpose AI model with advanced capabilities.",
    "pulls": 3600000,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9b058d72-28ae-4974-9d6a-63486800178e",
    "model_identifier": "deepseek-v3",
    "model_name": "deepseek-v3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-v3",
    "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.5.5\n or later.\n\n\n\n\n\n\nDeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally.\n\n\nReferences\n\n\nGitHub\n\n\nPaper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires [Ollama 0.5.5](https://github.com/ollama/ollama/releases/tag/v0.5.5) or later.\n\n<img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" />\n\nDeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally.\n\n## References\n\n[GitHub](https://github.com/deepseek-ai/DeepSeek-V3)\n\n[Paper](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "671b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-v3:latest",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      },
      {
        "tag": "latest",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K",
        "context_window": 160000
      }
    ],
    "min_ram_gb": 404.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for advanced language tasks.",
    "pulls": 3500000,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "3766ff1c-92ec-42c2-b630-91b4ca49b4c7",
    "model_identifier": "olmo2",
    "model_name": "olmo2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/olmo2",
    "description": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.5.5\n\n\n\n\n\n\nOLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.\n\n\nReferences\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires [Ollama 0.5.5](https://github.com/ollama/ollama/releases/tag/v0.5.5)\n\n![1732650119-wide-4x.webp](/assets/library/olmo2/71e694b3-a4fe-4bd1-8338-684506f85e8d)\n\nOLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.\n\n## References\n\n[Blog post](https://allenai.org/blog/olmo2)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "olmo2:latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "olmo2:13b",
        "size": "8.4GB",
        "size_gb": 8.4,
        "recommended_ram_gb": 10.5,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 4.5,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Data Scientists for academic tasks.",
    "pulls": 3500000,
    "tags": 9,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "99f12f95-3139-4ae5-a4d8-064e8657afba",
    "model_identifier": "mistral-nemo",
    "model_name": "mistral-nemo",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-nemo",
    "description": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.\n\n\n\n\nReference\n\n\nBlog\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/mistral-nemo/72045292-694a-4867-88c8-8635c9d97030\" width=\"280\" />\n\nMistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.\n\n![nemo-base-performance.png](https://ollama.com/assets/library/mistral-nemo/1adf7b56-30e3-49a0-8a52-bb74c19d8a78)\n\n## Reference\n\n[Blog](https://mistral.ai/news/mistral-nemo/)\n\n[Hugging Face](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "12b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-nemo:latest",
        "size": "7.1GB",
        "size_gb": 7.1,
        "recommended_ram_gb": 8.9,
        "quantization": "q4_k_m",
        "context": "1000K context",
        "context_window": 1000000
      },
      {
        "tag": "latest",
        "size": "7.1GB",
        "size_gb": 7.1,
        "recommended_ram_gb": 8.9,
        "quantization": "q4_k_m",
        "context": "1000K",
        "context_window": 1000000
      }
    ],
    "min_ram_gb": 7.1,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex text processing tasks.",
    "pulls": 3400000,
    "tags": 17,
    "last_updated": "2025-07-25",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c794a494-4192-49fa-8adb-2da3eaf903c9",
    "model_identifier": "bge-m3",
    "model_name": "bge-m3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/bge-m3",
    "description": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nBGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity:\n\n\n\n\nMulti-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n\n\nMulti-Linguality: It can support more than 100 working languages.\n\n\nMulti-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\n\n\n\n\nBenchmarks from the open-source community\n\n\n\n\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity:\n\n- Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n- Multi-Linguality: It can support more than 100 working languages.\n- Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\n\n**Benchmarks from the open-source community**\n![image.png](https://ollama.com/assets/library/bge-m3/17a9804b-f3da-4d09-bd18-90d4fe8900d3)\n\n```\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "567m"
    ],
    "memory_requirements": [
      {
        "tag": "bge-m3:latest",
        "size": "1.2GB",
        "size_gb": 1.2,
        "recommended_ram_gb": 1.5,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "1.2GB",
        "size_gb": 1.2,
        "recommended_ram_gb": 1.5,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.2,
    "use_cases": [
      "Text Embedding",
      "RAG / Retrieval"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": null,
    "best_for": "For text embedding tasks in multilingual environments.",
    "pulls": 3300000,
    "tags": 3,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "bdeaf3e4-d078-4436-b51e-860bd6437455",
    "model_identifier": "llama3.3",
    "model_name": "llama3.3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.3",
    "description": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNew state-of-the-art 70B\n model from Meta that offers similar performance compared to Llama 3.1 405B model.\n\n\nThe Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n\n\n\nSupported languages\n: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n\nNew capabilities\n\n\nThis release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n\nTool-use:\n Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n\n\nMultilinguality:\n Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n\n\nIntended Use\n\n\nIntended Use Cases\n Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\n\n\nOut-of-scope\n Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n\nNote:\n Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/llama3.2/be01fadf-7fbd-404d-929b-50a77249b030\" width=\"280\" />\n\n**New state-of-the-art 70B** model from Meta that offers similar performance compared to Llama 3.1 405B model. \n\nThe Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. \n\n![llama 3.3 benchmark](/assets/library/llama3.3/70cc3470-fa00-4179-b32e-d40d6494ff4e)\n\n**Supported languages**: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n### New capabilities\nThis release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use:** Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n\n**Multilinguality:** Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n\n### Intended Use\n\n**Intended Use Cases** Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**Note:** Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.3:latest",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 43.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Developers working on multilingual text processing tasks.",
    "pulls": 3300000,
    "tags": 14,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "fb3e6589-dfcc-4de3-9d32-ef6cb055bf8a",
    "model_identifier": "qwen3-coder",
    "model_name": "qwen3-coder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3-coder",
    "description": "Alibaba's performant long context models for agentic and coding tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen3-Coder\n is the most agentic code model to date in the Qwen series.\n\n\nGet started\n\n\n480B\n\n\nCloud\n\n\nollama run qwen3-coder:480b-cloud\n\n\n\nLocal\n\n\nollama run qwen3-coder:480b\n\n\n\nRunning locally requires a minimum of 250GB of memory or unified memory.\n\n\n30B\n\n\nollama run qwen3-coder:30b\n\n\n\nOverview\n\n\nqwen3-coder:30b\n offers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency.\n\n\n\n\nExceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks.\n\n\nLong context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding.\n\n\nScaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities.\n\n\nExecution-driven reinforcement learning that significantly boosts code execution success rates across diverse real-world coding tasks.\n\n\n\n\n\n\nReference\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522)\n**Qwen3-Coder** is the most agentic code model to date in the Qwen series.\n\n### Get started \n\n**480B** \n\nCloud\n```\nollama run qwen3-coder:480b-cloud\n```\n\nLocal\n```\nollama run qwen3-coder:480b\n```\nRunning locally requires a minimum of 250GB of memory or unified memory. \n\n\n**30B**\n\n```\nollama run qwen3-coder:30b\n```\n\n### Overview\n`qwen3-coder:30b` offers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency.\n\n- Exceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks.\n- Long context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding.\n- Scaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities.\n- Execution-driven reinforcement learning that significantly boosts code execution success rates across diverse real-world coding tasks.\n\n![image.png](/assets/library/qwen3-coder/52070971-5a66-4947-90a0-5e983a5809e7)\n\n### Reference\n- [Blog](https://qwenlm.github.io/blog/qwen3-coder/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "30b",
      "480b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3-coder:latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "256K",
        "context_window": 256000
      },
      {
        "tag": "qwen3-coder:480b",
        "size": "290GB",
        "size_gb": 290.0,
        "recommended_ram_gb": 362.5,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 19.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Real-world software engineering tasks requiring strong performance and efficiency.",
    "pulls": 3100000,
    "tags": 10,
    "last_updated": "2025-09-25",
    "last_updated_str": "5 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "12638308-ef47-484d-ab5c-0dd205041df7",
    "model_identifier": "deepseek-coder",
    "model_name": "deepseek-coder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-coder",
    "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek Coder is trained from scratch on both 87% code and 13% natural language in English and Chinese. Each of the models are pre-trained on 2 trillion tokens.\n\n\nModels available\n\n\n1.3 billion parameter model\n\n\nollama run deepseek-coder\n\n\n\n6.7 billion parameter model\n\n\nollama run deepseek-coder:6.7b\n\n\n\n33 billion parameter model\n\n\nollama run deepseek-coder:33b\n\n\n\nCLI\n\n\nOpen the terminal and run \nollama run deepseek-coder\n\n\nAPI\n\n\nExample using curl:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"deepseek-coder\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nAPI documentation\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/0e18bfb4-2964-41e4-b01c-043670228690\" width=\"460\">\n\nDeepSeek Coder is trained from scratch on both 87% code and 13% natural language in English and Chinese. Each of the models are pre-trained on 2 trillion tokens. \n\n### Models available\n\n1.3 billion parameter model\n\n```\nollama run deepseek-coder\n```\n\n6.7 billion parameter model \n\n```\nollama run deepseek-coder:6.7b\n```\n\n33 billion parameter model\n\n```\nollama run deepseek-coder:33b\n```\n\n### CLI\n\nOpen the terminal and run `ollama run deepseek-coder`\n\n### API\n\nExample using curl:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"deepseek-coder\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n```\n\n[API documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md)\n\n## References\n\n[HuggingFace](https://huggingface.co/deepseek-ai)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.3b",
      "6.7b",
      "33b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-coder:6.7b",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "deepseek-coder:33b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Code Generation",
      "Question Answering",
      "Text Summarization"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for code generation and answering programming-related questions.",
    "pulls": 3100000,
    "tags": 102,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a8d6a2af-7024-49c3-8ce4-3747b9735044",
    "model_identifier": "smollm2",
    "model_name": "smollm2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/smollm2",
    "description": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.\n\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![y45hIMNREW7w_XpHYB_0q.png](/assets/library/smollm2/2616a6ce-5645-48c3-bd5d-9a00f1dd0c9e)\n\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. \n\n![benchmark](/assets/library/smollm2/1a637eca-6f07-4572-8e01-011339c072d8)\n\n\n## References\n\n[HuggingFace](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "135m",
      "360m",
      "1.7b"
    ],
    "memory_requirements": [
      {
        "tag": "smollm2:latest",
        "size": "1.8GB",
        "size_gb": 1.8,
        "recommended_ram_gb": 2.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "1.8GB",
        "size_gb": 1.8,
        "recommended_ram_gb": 2.2,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.8,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers for text generation and summarization tasks.",
    "pulls": 2500000,
    "tags": 49,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c90464da-71ed-4435-8071-7b92166a93ca",
    "model_identifier": "all-minilm",
    "model_name": "all-minilm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/all-minilm",
    "description": "Embedding models on very large sentence level datasets.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.1.26 or later. \nDownload it here\n. It can only be used to generate embeddings.\n\n\n\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective.\n\n\nUsage\n\n\nREST API\n\n\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\n\n\n\nPython library\n\n\nollama.embeddings(model='all-minilm', prompt='The sky is blue because of Rayleigh scattering')\n\n\n\nJavascript library\n\n\nollama.embeddings({ model: 'all-minilm', prompt: 'The sky is blue because of Rayleigh scattering' })\n\n\n\nReferences\n\n\nHuggingFace\n\n\nWebsite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/4a6d8231-b7b5-411b-80b9-db37e5323cf9\" width=\"320\" />\n\n> Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). It can only be used to generate embeddings.\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective.\n\n## Usage\n\n### REST API\n\n```\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\n```\n\n### Python library\n\n```\nollama.embeddings(model='all-minilm', prompt='The sky is blue because of Rayleigh scattering')\n```\n\n### Javascript library\n\n```\nollama.embeddings({ model: 'all-minilm', prompt: 'The sky is blue because of Rayleigh scattering' })\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/sentence-transformers)\n\n[Website](https://www.sbert.net/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "22m",
      "33m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Generate sentence embeddings for text analysis and processing.",
    "pulls": 2400000,
    "tags": 10,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f13bb156-059b-422b-b8ed-532f2da8b627",
    "model_identifier": "mistral-small",
    "model_name": "mistral-small",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-small",
    "description": "Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models.\n\n\nMistral Small can be deployed locally and is exceptionally “knowledge-dense”, fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.\nPerfect for:\n\n\n\n\nFast response conversational agents.\n\n\nLow latency function calling.\n\n\nSubject matter experts via fine-tuning.\n\n\nLocal inference for hobbyists and organizations handling sensitive data.\n\n\n\n\nKey Features\n\n\n\n\nMultilingual:\n Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n\n\nAgent-Centric:\n Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n\n\nAdvanced Reasoning:\n State-of-the-art conversational and reasoning capabilities.\n\n\nApache 2.0 License:\n Open license allowing usage and modification for both commercial and non-commercial purposes.\n\n\nContext Window:\n A 32k context window.\n\n\nSystem Prompt:\n Maintains strong adherence and support for system prompts.\n\n\nTokenizer:\n Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n\n\n\nHuman Evaluations\n\n\n\n\nWe conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n\nInstruct performance\n\n\nOur instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks.\n\n\n\n\n\n\n\n\nPerformance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (Qwen2.5-32B-Instruct, Llama-3.3-70B-Instruct, Gemma-2-27B-IT). Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n\n\nCustomers are evaluating Mistral Small 3 across multiple industries, including:\n\n\n\n\nFinancial services customers for fraud detection\n\n\nHealthcare providers for customer triaging\n\n\nRobotics, automotive, and manufacturing companies for on-device command and control\n\n\nHorizontal use cases across customers include virtual customer service, and sentiment and feedback analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/mistral-small/87657512-b656-4c53-a8bd-64f0591d6fa9\" width=\"200\"/>\n\nMistral Small 3 sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models. \n\nMistral Small can be deployed locally and is exceptionally \"knowledge-dense\", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.\nPerfect for:\n\n- Fast response conversational agents.\n- Low latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n\n### Key Features\n- **Multilingual:** Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 32k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n### Human Evaluations \n![Human ratings](/assets/library/mistral-small/90f227bd-9751-4fe9-aa23-4d5d89b9d0c6)\n\nWe conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n### Instruct performance \nOur instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks.\n\n![instruct performance](/assets/library/mistral-small/d27f75e4-0dae-4721-bade-2999a1dd4a7b)\n![instruct performance](/assets/library/mistral-small/e677ae9e-edfa-47f9-a35c-b1eb9dfb51c8)\n\n![instruct performance](/assets/library/mistral-small/4545bb49-d87f-4731-bfdb-e191dc2c2a9a)\n\nPerformance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (Qwen2.5-32B-Instruct, Llama-3.3-70B-Instruct, Gemma-2-27B-IT). Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n \nCustomers are evaluating Mistral Small 3 across multiple industries, including: \n\n- Financial services customers for fraud detection\n- Healthcare providers for customer triaging \n- Robotics, automotive, and manufacturing companies for on-device command and control\n- Horizontal use cases across customers include virtual customer service, and sentiment and feedback analysis.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "22b",
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-small:22b",
        "size": "13GB",
        "size_gb": 13.0,
        "recommended_ram_gb": 16.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "mistral-small:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 13.0,
    "use_cases": [
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Local inference for hobbyists and organizations handling sensitive data.",
    "pulls": 2400000,
    "tags": 21,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9fbf201f-35ef-4a3d-b7bf-46b9bf696aea",
    "model_identifier": "codegemma",
    "model_name": "codegemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codegemma",
    "description": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.\n\n\nVariants:\n\n\n\n\ninstruct\n a 7b instruction-tuned variant for natural language-to-code chat and instruction following\n\n\ncode\n a 7b pretrained variant that specializes in code completion and generation from code prefixes and/or suffixes\n\n\n2b\n a state of the art 2B pretrained variant that provides up to 2x faster code completion\n\n\n\n\nAdvantages:\n\n\n\n\nIntelligent code completion and generation\n: Complete lines, functions, and even generate entire blocks of code, whether you’re working locally or using Google Cloud resources.\n\n\nEnhanced accuracy\n: Trained on 500 billion tokens of primarily English language data from web documents, mathematics, and code, CodeGemma models generate code that’s not only more syntactically correct but also semantically meaningful, reducing errors and debugging time.\n\n\nMulti-language proficiency\n: Supports Python, JavaScript, Java, Kotlin, C++, C#, Rust, Go, and other languages.\n\n\nStreamlined workflows\n: Integrate a CodeGemma model into your development environment to write less boilerplate and focus on interesting and differentiated code that matters, faster.\n\n\n\n\n\n\nFill-in-the-middle\n\n\nCodeGemma models support fill-in-the-middle (FIM), for use in autocomplete or coding assistant tooling. Below is an example using the Ollama \nPython\n library:\n\n\nresponse = generate(\n  model='codegemma:2b-code',\n  prompt=f'<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|>',\n  options={\n    'num_predict': 128,\n    'temperature': 0,\n    'top_p': 0.9,\n    'stop': ['<|file_separator|>'],\n  },\n)\n\n\n\nReferences\n\n\nHugging Face\n\n\nReport\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/56ffc5fc-0c30-4ab5-a0e3-65fc66de17bc\" width=\"320\" />\n\nCodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.\n\n### Variants:\n\n* `instruct` a 7b instruction-tuned variant for natural language-to-code chat and instruction following\n* `code` a 7b pretrained variant that specializes in code completion and generation from code prefixes and/or suffixes\n* `2b` a state of the art 2B pretrained variant that provides up to 2x faster code completion\n\n### Advantages:\n\n* **Intelligent code completion and generation**: Complete lines, functions, and even generate entire blocks of code, whether you're working locally or using Google Cloud resources.\n\n* **Enhanced accuracy**: Trained on 500 billion tokens of primarily English language data from web documents, mathematics, and code, CodeGemma models generate code that's not only more syntactically correct but also semantically meaningful, reducing errors and debugging time.\n\n* **Multi-language proficiency**: Supports Python, JavaScript, Java, Kotlin, C++, C#, Rust, Go, and other languages.\n\n* **Streamlined workflows**: Integrate a CodeGemma model into your development environment to write less boilerplate and focus on interesting and differentiated code that matters, faster.\n\n![benchmarks](https://github.com/ollama/ollama/assets/251292/0d8473cb-bcee-4bd0-9214-c527ce367d88)\n\n### Fill-in-the-middle\n\nCodeGemma models support fill-in-the-middle (FIM), for use in autocomplete or coding assistant tooling. Below is an example using the Ollama [Python](https://github.com/ollama/ollama-python) library:\n\n```python\nresponse = generate(\n  model='codegemma:2b-code',\n  prompt=f'<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|>',\n  options={\n    'num_predict': 128,\n    'temperature': 0,\n    'top_p': 0.9,\n    'stop': ['<|file_separator|>'],\n  },\n)\n```\n\n### References\n\n[Hugging Face](https://huggingface.co/collections/google/codegemma-release-66152ac7b683e2667abdee11)\n\n[Report](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "codegemma:2b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "codegemma:latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Code Generation"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for coding tasks.",
    "pulls": 2200000,
    "tags": 85,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0e3eb025-dc14-47d5-8c04-0cfec90bc6b4",
    "model_identifier": "granite3.1-moe",
    "model_name": "granite3.1-moe",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3.1-moe",
    "description": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nGranite mixture of experts models\n\n\nThe IBM Granite \n1B and 3B models\n are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.\n\n\nThe models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference.\n\n\nParameter Sizes\n\n\n1B:\n\n\nollama run granite3.1-moe:1b\n\n\n3B:\n\n\nollama run granite3.1-moe:3b\n\n\nSupported Languages\n\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\n\n\nCapabilities\n\n\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related tasks\n\n\nFunction-calling tasks\n\n\nMultilingual dialog use cases\n\n\nLong-context tasks including long document/meeting summarization, long document QA, etc.\n\n\n\n\nGranite dense models\n\n\nThe Granite dense models are available in \n2B and 8B\n parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n\nSee model page\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nGitHub Repository:\n \nibm-granite/granite-language-models\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: December 18th, 2024\n\n\nLicense:\n \nApache 2.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Granite mixture of experts models\n\nThe IBM Granite **1B and 3B models** are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.\n\nThe models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference.\n\n### Parameter Sizes\n\n**1B:**\n  \n`ollama run granite3.1-moe:1b`\n\n**3B:**\n\n`ollama run granite3.1-moe:3b`\n\n### Supported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) \n\n### Capabilities\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related tasks\n* Function-calling tasks\n* Multilingual dialog use cases\n* Long-context tasks including long document/meeting summarization, long document QA, etc.\n\n\n## Granite dense models\n\nThe Granite dense models are available in **2B and 8B** parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n[See model page](https://ollama.com/library/granite3-dense) \n\n## Learn more\n\n- **Developers:** IBM Research\n- **GitHub Repository:** [ibm-granite/granite-language-models](https://github.com/ibm-granite/granite-3.1-language-models)\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: December 18th, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "1b",
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3.1-moe:1b",
        "size": "1.4GB",
        "size_gb": 1.4,
        "recommended_ram_gb": 1.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "granite3.1-moe:latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.4,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "Spanish",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for text processing tasks.",
    "pulls": 2200000,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "96e1ffd5-7f46-4f19-8df3-c583f402a563",
    "model_identifier": "falcon3",
    "model_name": "falcon3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/falcon3",
    "description": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nFalcon3 represents TII’s latest advancement in efficient language models under 10B parameters, focused on enhancing science, math, and code capabilities while maintaining training efficiency.\n\n\nKey Features\n\n\n\n\nFour sizes: 1B, 3B, 7B, 10B\n\n\nDepth up-scaling technique used to create 10B model from 7B\n\n\nKnowledge distillation for smaller models (1B, 3B)\n\n\n\n\nPerformance Highlights\n\n\n\n\nfalcon3:1b\n outperforms \nsmollm2:1.7b\n, matches \ngemma2:2b\n\n\nfalcon3:10b\n achieves SOTA in under-13B category\n\n\nExtended context length up to 32K tokens (8K for 1B model)\n\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/falcon3/7ead93c7-c315-42ad-b49f-3aa370c3e7db\" width=\"425\" />\n\nFalcon3 represents TII's latest advancement in efficient language models under 10B parameters, focused on enhancing science, math, and code capabilities while maintaining training efficiency.\n\n# Key Features\n- Four sizes: 1B, 3B, 7B, 10B\n- Depth up-scaling technique used to create 10B model from 7B\n- Knowledge distillation for smaller models (1B, 3B)\n\n# Performance Highlights\n- `falcon3:1b` outperforms `smollm2:1.7b`, matches `gemma2:2b`\n- `falcon3:10b` achieves SOTA in under-13B category\n- Extended context length up to 32K tokens (8K for 1B model)\n\n# References\n[Hugging Face](https://huggingface.co/blog/falcon3)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1b",
      "3b",
      "7b",
      "10b"
    ],
    "memory_requirements": [
      {
        "tag": "falcon3:1b",
        "size": "1.8GB",
        "size_gb": 1.8,
        "recommended_ram_gb": 2.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "falcon3:3b",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "falcon3:latest",
        "size": "4.6GB",
        "size_gb": 4.6,
        "recommended_ram_gb": 5.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.6GB",
        "size_gb": 4.6,
        "recommended_ram_gb": 5.8,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "falcon3:10b",
        "size": "6.3GB",
        "size_gb": 6.3,
        "recommended_ram_gb": 7.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.8,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Math"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for general AI tasks.",
    "pulls": 2200000,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "85b04437-f02f-4b1a-93cd-6d1677280d1b",
    "model_identifier": "llava-llama3",
    "model_name": "llava-llama3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llava-llama3",
    "description": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nllava-llama3\n is a LLaVA model fine-tuned from Llama 3 Instruct and CLIP-ViT-Large-patch14-336 with ShareGPT4V-PT and InternVL-SFT by XTuner.\n\n\n\n\nReferences\n\n\nHugging Face\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/llava-llama3/dc3b65cd-62de-45cd-93f9-5c6da62214fa\" width=\"320\" />\n\n`llava-llama3` is a LLaVA model fine-tuned from Llama 3 Instruct and CLIP-ViT-Large-patch14-336 with ShareGPT4V-PT and InternVL-SFT by XTuner.\n\n<img src=\"https://ollama.com/assets/library/llava-llama3/a4be49e3-b088-4f59-8536-4e55a98ccf4c\" width=\"480\" />\n\n\n## References\n\n[Hugging Face](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)\n\n[GitHub](https://github.com/InternLM/xtuner/tree/main)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "llava-llama3:latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 5.5,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Data Scientists for advanced language tasks.",
    "pulls": 2100000,
    "tags": 4,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "fa608ebe-7a27-4429-9202-407fe0977cb8",
    "model_identifier": "starcoder2",
    "model_name": "starcoder2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/starcoder2",
    "description": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSupporting a context window of up to 16,384 tokens, StarCoder2 is the next generation of transparently trained open code LLMs.\n\n\n\n\nstarcoder2:instruct\n: a 15B model that follows natural and human-written instructions\n\n\nstarcoder2:15b\n was trained on 600+ programming languages and 4+ trillion tokens.\n\n\nstarcoder2:7b\n was trained on 17 programming languages and 3.5+ trillion tokens.\n\n\nstarcoder2:3b\n was trained on 17 programming languages and 3+ trillion tokens.\n\n\n\n\nStarCoder2-15B is the best in its size class and matches 33B+ models on many evaluations. StarCoder2-3B matches the performance of StarCoder1-15B.\n\n\nReferences\n\n\n\n\nGitHub\n\n\nHuggingFace\n\n\nPaper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![StarCode 2](https://github.com/ollama/ollama/assets/251292/5456bb29-de2a-4463-930c-3aaad585f430)\n\nSupporting a context window of up to 16,384 tokens, StarCoder2 is the next generation of transparently trained open code LLMs.\n\n* `starcoder2:instruct`: a 15B model that follows natural and human-written instructions\n* `starcoder2:15b` was trained on 600+ programming languages and 4+ trillion tokens.\n* `starcoder2:7b` was trained on 17 programming languages and 3.5+ trillion tokens.\n* `starcoder2:3b` was trained on 17 programming languages and 3+ trillion tokens.\n\n\nStarCoder2-15B is the best in its size class and matches 33B+ models on many evaluations. StarCoder2-3B matches the performance of StarCoder1-15B.\n\n## References\n- [GitHub](https://github.com/bigcode-project/starcoder2)\n- [HuggingFace](https://huggingface.co/bigcode)\n- [Paper](https://arxiv.org/abs/2402.19173)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3b",
      "7b",
      "15b"
    ],
    "memory_requirements": [
      {
        "tag": "starcoder2:latest",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      },
      {
        "tag": "starcoder2:7b",
        "size": "4.0GB",
        "size_gb": 4.0,
        "recommended_ram_gb": 5.0,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "starcoder2:15b",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 1.7,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working with code generation tasks.",
    "pulls": 2100000,
    "tags": 67,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f3fd809c-021a-4247-b4ed-8f69a7f7124f",
    "model_identifier": "snowflake-arctic-embed",
    "model_name": "snowflake-arctic-embed",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/snowflake-arctic-embed",
    "description": "A suite of text embedding models by Snowflake, optimized for performance.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nsnowflake-arctic-embed\n is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.\n\n\nThe models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance.\n\n\nThis model is available in 5 parameter sizes:\n\n\n\n\nsnowflake-arctic-embed:335m\n (default)\n\n\nsnowflake-arctic-embed:137m\n\n\nsnowflake-arctic-embed:110m\n\n\nsnowflake-arctic-embed:33m\n\n\nsnowflake-arctic-embed:22m\n\n\n\n\nReference\n\n\nBlog Post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/115f484a-1911-46e9-902f-65a70f84e1d0\" width=\"320\" />\n\n`snowflake-arctic-embed` is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.\n\nThe models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance.\n\nThis model is available in 5 parameter sizes:\n\n* `snowflake-arctic-embed:335m` (default)\n* `snowflake-arctic-embed:137m`\n* `snowflake-arctic-embed:110m`\n* `snowflake-arctic-embed:33m`\n* `snowflake-arctic-embed:22m`\n\n## Reference\n\n[Blog Post](https://www.snowflake.com/blog/introducing-snowflake-arctic-embed-snowflakes-state-of-the-art-text-embedding-family-of-models/)\n\n[HuggingFace](https://huggingface.co/Snowflake)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "22m",
      "33m",
      "110m",
      "137m",
      "335m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": null,
    "best_for": "For text embedding tasks requiring high performance and various parameter sizes.",
    "pulls": 2100000,
    "tags": 16,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "457c2ba2-bb99-46a0-a0dc-a6dfcfb8c3d3",
    "model_identifier": "qwq",
    "model_name": "qwq",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwq",
    "description": "QwQ is the reasoning model of the Qwen series.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\n\n\n\n\nFuture Work\n\n\nThis marks Qwen’s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling.\n\n\nReference\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\n\n![](/assets/library/qwq/e3d71b1c-9c62-413a-a63a-1ca604189a17)\n\n### Future Work \n\nThis marks Qwen’s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling.\n\n### Reference\n- [Blog](https://qwenlm.github.io/blog/qwq-32b/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "qwq:latest",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "latest",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "40K",
        "context_window": 40000
      }
    ],
    "min_ram_gb": 20.0,
    "use_cases": [
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "Chinese",
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for complex language tasks.",
    "pulls": 2000000,
    "tags": 8,
    "last_updated": "2025-03-25",
    "last_updated_str": "11 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "685f43a0-b8aa-4d04-9e1b-d58ccfbed37a",
    "model_identifier": "orca-mini",
    "model_name": "orca-mini",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/orca-mini",
    "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nOrca Mini is a Llama and Llama 2 model trained on Orca Style datasets created using the approaches defined in the paper, Orca: Progressive Learning from Complex Explanation Traces of GPT-4. There are two variations available. The original Orca Mini based on Llama in 3, 7, and 13 billion parameter sizes, and v3 based on Llama 2 in 7, 13, and 70 billion parameter sizes.\n\n\nUsage\n\n\nCLI\n\n\nOpen the terminal and run \nollama run orca-mini\n\n\nAPI\n\n\nExample:\n\n\n  curl -X POST http://localhost:11434/api/generate -d '{\n    \"model\": \"orca-mini\",\n    \"prompt\":\"Why is the sky blue?\"\n   }'\n\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n70b models generally require at least 64GB of RAM\n\n\n\n\nReference\n\n\n3b parameters original source:\n \nPankaj Mathur\n\n\n7b parameters original source:\n \nPankaj Mathur\n\n\n13b parameters original source:\n \nPankaj Mathur\n\n\nOrca Mini v3 source on Ollama\n\n\n13b parameters original source:\n \nPankaj Mathur\n\n\n70b parameters source:\n \nPankaj Mathur\n\n\nOrca: Progressive Learning from Complex Explanation Traces of GPT-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrca Mini is a Llama and Llama 2 model trained on Orca Style datasets created using the approaches defined in the paper, Orca: Progressive Learning from Complex Explanation Traces of GPT-4. There are two variations available. The original Orca Mini based on Llama in 3, 7, and 13 billion parameter sizes, and v3 based on Llama 2 in 7, 13, and 70 billion parameter sizes.\n\n## Usage\n\n### CLI\n\nOpen the terminal and run `ollama run orca-mini`\n\n### API\n\nExample: \n\n  ```bash\n  curl -X POST http://localhost:11434/api/generate -d '{\n    \"model\": \"orca-mini\",\n    \"prompt\":\"Why is the sky blue?\"\n   }'\n  ```\n\n\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n- 70b models generally require at least 64GB of RAM\n\n\n## Reference\n\n\n3b parameters original source:\n [Pankaj Mathur](https://huggingface.co/psmathur/orca_mini_3b)\n\n7b parameters original source:\n [Pankaj Mathur](https://huggingface.co/psmathur/orca_mini_7b)\n\n13b parameters original source:\n [Pankaj Mathur](https://huggingface.co/psmathur/orca_mini_13b)\n\n\n**Orca Mini v3 source on Ollama**\n\n13b parameters original source:\n [Pankaj Mathur](https://huggingface.co/psmathur/orca_mini_v3_13b)\n\n70b parameters source:\n [Pankaj Mathur](https://huggingface.co/psmathur/orca_mini_v3_70b)\n\n[Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707 )\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3b",
      "7b",
      "13b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "orca-mini:latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "orca-mini:7b",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "orca-mini:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "orca-mini:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.0,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "intermediate",
    "best_for": "Beginners and researchers for language tasks.",
    "pulls": 2000000,
    "tags": 119,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "02339749-a37f-40a2-aaed-e910cd3ecf1a",
    "model_identifier": "mixtral",
    "model_name": "mixtral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mixtral",
    "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Mixtral large Language Models (LLM) are a set of pretrained generative Sparse Mixture of Experts.\n\n\nSizes\n\n\n\n\nmixtral:8x22b\n\n\nmixtral:8x7b\n\n\n\n\nMixtral 8x22b\n\n\nollama run mixtral:8x22b\n\n\n\nMixtral 8x22B sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.\n\n\nMixtral 8x22B comes with the following strengths:\n\n\n\n\nIt is fluent in English, French, Italian, German, and Spanish\n\n\nIt has strong maths and coding capabilities\n\n\nIt is natively capable of function calling\n\n\n64K tokens context window allows precise information recall from large documents\n\n\n\n\nReferences\n\n\nAnnouncement\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/59bf76bc-1f6f-4a45-be4f-d6a13d7f0645\" width=\"320\">\n\nThe Mixtral large Language Models (LLM) are a set of pretrained generative Sparse Mixture of Experts.\n\n## Sizes\n\n* `mixtral:8x22b`\n* `mixtral:8x7b`\n\n## Mixtral 8x22b\n\n```\nollama run mixtral:8x22b\n```\n\nMixtral 8x22B sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.\n\nMixtral 8x22B comes with the following strengths:\n\n* It is fluent in English, French, Italian, German, and Spanish\n* It has strong maths and coding capabilities\n* It is natively capable of function calling\n* 64K tokens context window allows precise information recall from large documents\n\n## References\n\n[Announcement](https://mistral.ai/news/mixtral-of-experts/)\n\n[HuggingFace](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "8x7b",
      "8x22b"
    ],
    "memory_requirements": [
      {
        "tag": "mixtral:latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "mixtral:8x22b",
        "size": "80GB",
        "size_gb": 80.0,
        "recommended_ram_gb": 100.0,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 26.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers who need a high-performance, low-parameter language model.",
    "pulls": 1900000,
    "tags": 70,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "67ec3d4a-aec0-4819-8191-2ae0e5719385",
    "model_identifier": "llama2-uncensored",
    "model_name": "llama2-uncensored",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama2-uncensored",
    "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nLlama 2 Uncensored is based on Meta’s Llama 2 model, and was created by George Sung and Jarrad Hope using the process defined by Eric Hartford in his \nblog post\n.\n\n\nCLI\n\n\nOpen the terminal and run \nollama run llama2-uncensored\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2-uncensored\",\n  \"prompt\":\"Write a recipe for dangerously spicy mayo.\"\n }'\n\n\n\nAPI documentation\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n70b models generally require at least 64GB of RAM\n\n\n\n\nReferences\n\n\nhttps://erichartford.com/uncensored-models\n\n\n7b parameters original source:\n \nGeorge Sung\n\n\n70b parameters original source:\n \nGeorge Sung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLlama 2 Uncensored is based on Meta's Llama 2 model, and was created by George Sung and Jarrad Hope using the process defined by Eric Hartford in his [blog post](https://erichartford.com/uncensored-models).\n\n### CLI\n\nOpen the terminal and run `ollama run llama2-uncensored`\n\n### API\n\nExample:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2-uncensored\",\n  \"prompt\":\"Write a recipe for dangerously spicy mayo.\"\n }'\n```\n\n[API documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md)\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 70b models generally require at least 64GB of RAM\n\n## References\n\n[https://erichartford.com/uncensored-models](https://erichartford.com/uncensored-models )\n\n7b parameters original source:\n [George Sung](https://huggingface.co/georgesung/llama2_7b_chat_uncensored)\n\n70b parameters original source:\n [George Sung](https://huggingface.co/jarradh/llama2_70b_chat_uncensored)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama2-uncensored:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "llama2-uncensored:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Question Answering",
      "Creative Writing"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 1800000,
    "tags": 34,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "801453a2-f0ba-4c97-970a-2d4eb995fbc3",
    "model_identifier": "deepseek-coder-v2",
    "model_name": "deepseek-coder-v2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-coder-v2",
    "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/deepseek-coder-v2/cce4988a-b798-4c63-82ab-14c5c09462fe\" width=\"280\" />\n\nDeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus.\n\n## References\n\n[Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "16b",
      "236b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-coder-v2:latest",
        "size": "8.9GB",
        "size_gb": 8.9,
        "recommended_ram_gb": 11.1,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      },
      {
        "tag": "latest",
        "size": "8.9GB",
        "size_gb": 8.9,
        "recommended_ram_gb": 11.1,
        "quantization": "q4_k_m",
        "context": "160K",
        "context_window": 160000
      },
      {
        "tag": "deepseek-coder-v2:236b",
        "size": "133GB",
        "size_gb": 133.0,
        "recommended_ram_gb": 166.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 8.9,
    "use_cases": [
      "Code Generation",
      "Text Summarization"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on code-related tasks.",
    "pulls": 1600000,
    "tags": 64,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d0ecd792-bee3-42c7-b851-8345e9590ed4",
    "model_identifier": "qwen3-vl",
    "model_name": "qwen3-vl",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3-vl",
    "description": "The most powerful vision-language model in the Qwen model family to date.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nQwen3-VL models require \nOllama 0.12.7\n\n\n\n\nQwen3-VL is the most powerful vision-language model in the Qwen family to date.\n\n\nIn this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents — Qwen3-VL shows clear and significant progress in every area.\n\n\nModels\n\n\n2B\n\n\nollama run qwen3-vl:2b\n\n\n\n4B\n\n\nollama run qwen3-vl:4b\n\n\n\n8B\n\n\nollama run qwen3-vl:8b\n\n\n\n30B\n\n\nollama run qwen3-vl:30b\n\n\n\n32B\n\n\nollama run qwen3-vl:32b\n\n\n\n235B\n\n\nollama run qwen3-vl:235b\n\n\n\nollama run qwen3-vl:235b-cloud\n\n\n\nKey features\n\n\nVisual Agent Capabilities\n: Qwen3-VL can operate computer and mobile interfaces — recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks.\n\n\nSuperior Text-Centric Performance\n: Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously strengthening its language capabilities. Its performance on text-based tasks matches that of Qwen3-235B-A22B-2507 — the flagship language model — making it a truly “text-grounded, multimodal powerhouse” for the next generation of vision-language models.\n\n\nGreatly Improved Visual Coding\n: It can now generate code from images or videos — for example, turning a design mockup into Draw.io, HTML, CSS, or JavaScript code — making “what you see is what you get” visual programming a reality.\n\n\nMuch Better Spatial Understanding\n: 2D grounding from absolute coordinates to relative coordinates. It can judge object positions, viewpoint changes, and occlusion relationships. It supports 3D grounding, laying the foundation for complex spatial reasoning and embodied AI applications.\n\n\nLong Context & Long Video Understanding\n: All models natively support 256K tokens of context, expandable up to 1 million tokens. This means you can input hundreds of pages of technical documents, entire textbooks, or even two-hour videos — and the model will remember everything and retrieve details accurately, down to the exact second in videos.\n\n\nStronger Multimodal Reasoning (Thinking Version)\n: The Thinking model is specially optimized for STEM and math reasoning. When facing complex subject questions, it can notice fine details, break down problems step by step, analyze cause and effect, and give logical, evidence-based answers. It achieves strong performance on reasoning benchmarks like MathVision, MMMU, and MathVista.\n\n\nUpgraded Visual Perception & Recognition\n: By improving the quality and diversity of pre-training data, the model can now recognize a much wider range of objects — from celebrities, anime characters, products, and landmarks, to animals and plants — covering both everyday life and professional “recognize anything” needs.\n\n\nBetter OCR Across More Languages & Complex Scenes\n: OCR now supports 32 languages (up from 10), covering more countries and regions. It performs more reliably under challenging real-world conditions like poor lighting, blur, or tilted text. Recognition accuracy for rare characters, ancient scripts, and technical terms has also improved significantly. Its ability to understand long documents and reconstruct fine structures is further enhanced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Qwen 3 banner](/assets/library/qwen3-vl/d784b840-5f70-4a60-86bc-f10ad6c0627f)\n\n> Qwen3-VL models require [Ollama 0.12.7](https://github.com/ollama/ollama/releases)\n\nQwen3-VL is the most powerful vision-language model in the Qwen family to date. \n\nIn this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents — Qwen3-VL shows clear and significant progress in every area. \n\n### Models\n\n**2B** \n```\nollama run qwen3-vl:2b\n```\n\n**4B** \n```\nollama run qwen3-vl:4b\n```\n\n**8B** \n```\nollama run qwen3-vl:8b\n```\n\n**30B** \n```\nollama run qwen3-vl:30b\n```\n\n**32B** \n```\nollama run qwen3-vl:32b\n```\n\n**235B** \n```\nollama run qwen3-vl:235b\n```\n\n```\nollama run qwen3-vl:235b-cloud\n```\n\n\n### Key features\n\n**Visual Agent Capabilities**: Qwen3-VL can operate computer and mobile interfaces — recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks.\n\n**Superior Text-Centric Performance**: Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously strengthening its language capabilities. Its performance on text-based tasks matches that of Qwen3-235B-A22B-2507 — the flagship language model — making it a truly “text-grounded, multimodal powerhouse” for the next generation of vision-language models.\n\n**Greatly Improved Visual Coding**: It can now generate code from images or videos — for example, turning a design mockup into Draw.io, HTML, CSS, or JavaScript code — making “what you see is what you get” visual programming a reality.\n\n**Much Better Spatial Understanding**: 2D grounding from absolute coordinates to relative coordinates. It can judge object positions, viewpoint changes, and occlusion relationships. It supports 3D grounding, laying the foundation for complex spatial reasoning and embodied AI applications.\n\n**Long Context & Long Video Understanding**: All models natively support 256K tokens of context, expandable up to 1 million tokens. This means you can input hundreds of pages of technical documents, entire textbooks, or even two-hour videos — and the model will remember everything and retrieve details accurately, down to the exact second in videos.\n\n**Stronger Multimodal Reasoning (Thinking Version)**: The Thinking model is specially optimized for STEM and math reasoning. When facing complex subject questions, it can notice fine details, break down problems step by step, analyze cause and effect, and give logical, evidence-based answers. It achieves strong performance on reasoning benchmarks like MathVision, MMMU, and MathVista.\n\n**Upgraded Visual Perception & Recognition**: By improving the quality and diversity of pre-training data, the model can now recognize a much wider range of objects — from celebrities, anime characters, products, and landmarks, to animals and plants — covering both everyday life and professional “recognize anything” needs.\n\n**Better OCR Across More Languages & Complex Scenes**: OCR now supports 32 languages (up from 10), covering more countries and regions. It performs more reliably under challenging real-world conditions like poor lighting, blur, or tilted text. Recognition accuracy for rare characters, ancient scripts, and technical terms has also improved significantly. Its ability to understand long documents and reconstruct fine structures is further enhanced.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "2b",
      "4b",
      "8b",
      "30b",
      "32b",
      "235b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3-vl:2b",
        "size": "1.9GB",
        "size_gb": 1.9,
        "recommended_ram_gb": 2.4,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3-vl:4b",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3-vl:latest",
        "size": "6.1GB",
        "size_gb": 6.1,
        "recommended_ram_gb": 7.6,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "latest",
        "size": "6.1GB",
        "size_gb": 6.1,
        "recommended_ram_gb": 7.6,
        "quantization": "q4_k_m",
        "context": "256K",
        "context_window": 256000
      },
      {
        "tag": "qwen3-vl:30b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3-vl:32b",
        "size": "21GB",
        "size_gb": 21.0,
        "recommended_ram_gb": 26.2,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3-vl:235b",
        "size": "143GB",
        "size_gb": 143.0,
        "recommended_ram_gb": 178.8,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 1.9,
    "use_cases": [
      "Image Understanding",
      "Question Answering",
      "Text Summarization",
      "Reasoning",
      "Translation"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Developers working with vision-language models.",
    "pulls": 1600000,
    "tags": 59,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8fe7e9b0-3294-45e1-b942-d83ed7de79f8",
    "model_identifier": "cogito",
    "model_name": "cogito",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/cogito",
    "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Cogito v1 Preview LLMs are instruction tuned generative models (text in/text out). All models are released under an open license for commercial use.\n\n\n\n\nCogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).\n\n\nThe LLMs are trained using \nIterated Distillation and Amplification (IDA)\n - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.\n\n\nThe models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\n\n\n\n\nIn both standard and reasoning modes, Cogito v1-preview models outperform their size equivalent counterparts on common industry benchmarks.\n\n\n\n\nEach model is trained in over 30 languages and supports a context length of 128k.\n\n\n\n\nExtended thinking\n\n\nTo enable extended thinking, include \nEnable deep thinking subroutine.\n in the system prompt:\n\n\n/set system \"\"\"Enable deep thinking subroutine.\"\"\"\n\n\n\nOr via the API:\n\n\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"cogito\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Enable deep thinking subroutine.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"How many letter Rs are in the word Strawberry?\"\n    }\n  ]\n}'\n\n\n\nSizes\n\n\n3B\n\n\nollama run cogito:3b\n\n\n\n8B\n\n\nollama run cogito:8b\n\n\n\n14B\n\n\nollama run cogito:14b\n\n\n\n32B\n\n\nollama run cogito:32b\n\n\n\n70B\n\n\nollama run cogito:70b\n\n\n\nBenchmarks\n\n\nSmaller models - 3B and 8B\n\n\n3B performance\n\n\n\n\n8B performance\n\n\n\n\n3B tool calling\n\n\n\n\nMedium models - 14B and 32B\n\n\n14B\n\n\n\n\n32B\n\n\n\n\nLarger models - 70B\n\n\n\n\nReferences\n\n\nBlog post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/cogito/44ceefc5-6a71-4d18-958e-21d45d309b18\" width=\"320\" />\n\nThe Cogito v1 Preview LLMs are instruction tuned generative models (text in/text out). All models are released under an open license for commercial use.\n\n- Cogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).\n- The LLMs are trained using **Iterated Distillation and Amplification (IDA)** - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.\n- The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\n  - In both standard and reasoning modes, Cogito v1-preview models outperform their size equivalent counterparts on common industry benchmarks. \n- Each model is trained in over 30 languages and supports a context length of 128k.\n\n## Extended thinking\n\nTo enable extended thinking, include `Enable deep thinking subroutine.` in the system prompt:\n\n```\n/set system \"\"\"Enable deep thinking subroutine.\"\"\"\n```\n\nOr via the API:\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"cogito\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Enable deep thinking subroutine.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"How many letter Rs are in the word Strawberry?\"\n    }\n  ]\n}'\n```\n\n## Sizes\n\n#### 3B\n\n```\nollama run cogito:3b\n```\n\n#### 8B\n\n```\nollama run cogito:8b\n```\n\n#### 14B\n\n```\nollama run cogito:14b\n```\n\n#### 32B\n\n```\nollama run cogito:32b\n```\n\n#### 70B\n\n```\nollama run cogito:70b\n```\n\n## Benchmarks\n\n### Smaller models - 3B and 8B\n\n#### 3B performance\n![3b.webp](/assets/library/cogito/b3f444f7-91a6-4bd4-a395-a515060752b9)\n\n#### 8B performance\n![8b.webp](/assets/library/cogito/e2cc899a-0f9f-48c5-a530-e941576bd67a)\n\n#### 3B tool calling\n![3b-toolcalling.webp](/assets/library/cogito/53992b0e-9226-4fff-ba70-9ffd7f980f2f)\n\n### Medium models - 14B and 32B\n\n#### 14B\n![14b.webp](/assets/library/cogito/c32b8f91-3b00-457a-ad54-feab83d1decf)\n\n#### 32B\n![32b.webp](/assets/library/cogito/bc5ea441-1fb8-4bf6-b4de-9093b0e74426)\n\n### Larger models - 70B\n![70b.webp](/assets/library/cogito/139c3df7-bdc5-4497-aa51-cddb7eccaca9)\n\n## References\n\n[Blog post](https://www.deepcogito.com/research/cogito-v1-preview)\n\n[Hugging Face](https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "3b",
      "8b",
      "14b",
      "32b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "cogito:3b",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "cogito:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "cogito:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "cogito:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "cogito:70b",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 2.2,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for code generation, text summarization, and function calling tasks.",
    "pulls": 1400000,
    "tags": 20,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "275c8749-5f61-4265-b383-19b7401168b4",
    "model_identifier": "qwen2.5vl",
    "model_name": "qwen2.5vl",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2.5vl",
    "description": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.7.0. \nDownload Ollama\n\n\n\n\n\n\nQwen2.5-VL, the new flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.\n\n\nThe key features include:\n\n\n\n\nUnderstand things visually:\n Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n\nBeing agentic:\n Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n\nCapable of visual localization in different formats:\n Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n\nGenerating structured outputs:\n for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n\n\nPerformance\n\n\nWe evaluate our models with the SOTA models as well as the best models of similar model sizes. In terms of the flagship model Qwen2.5-VL-72B-Instruct, it achieves competitive performance in a series of benchmarks covering domains and tasks, including college-level problems, math, document understanding, general question answering, math, and visual agent. Notably, Qwen2.5-VL achieves significant advantages in understanding documents and diagrams, and it is capable of playing as a visual agent without task-specific fine tuning.\n\n\n\n\nIn terms of smaller models, Qwen2.5-VL-7B-Instruct outperforms GPT-4o-mini in a number of tasks, and Qwen2.5-VL-3B, which is a solution for edge AI, even outperforms the 7B model of our previous version Qwen2-VL.\n\n\n\n\n\n\nReferences\n\n\nBlog\n\n\nQwen2.5-VL Technical Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires Ollama 0.7.0. [Download Ollama](https://ollama.com/download)\n\n<img src=\"/assets/library/qwen2.5vl/7bd5aad3-d5c5-43b1-b915-1e85a378f1c4\" width=\"280\" />\n\nQwen2.5-VL, the new flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL. \n\nThe key features include:\n\n- **Understand things visually:** Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n- **Being agentic:** Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n- **Capable of visual localization in different formats:** Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n- **Generating structured outputs:** for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n## Performance\n\nWe evaluate our models with the SOTA models as well as the best models of similar model sizes. In terms of the flagship model Qwen2.5-VL-72B-Instruct, it achieves competitive performance in a series of benchmarks covering domains and tasks, including college-level problems, math, document understanding, general question answering, math, and visual agent. Notably, Qwen2.5-VL achieves significant advantages in understanding documents and diagrams, and it is capable of playing as a visual agent without task-specific fine tuning.\n\n![image.png](/assets/library/qwen2.5vl/440a5a81-c4e4-4b5e-aa3d-f75ab2ad04cc)\n\nIn terms of smaller models, Qwen2.5-VL-7B-Instruct outperforms GPT-4o-mini in a number of tasks, and Qwen2.5-VL-3B, which is a solution for edge AI, even outperforms the 7B model of our previous version Qwen2-VL.\n\n![image.png](/assets/library/qwen2.5vl/533ec184-7b82-4bcf-a4ab-af9a41ac0810)\n\n![image.png](/assets/library/qwen2.5vl/c90b8e4b-d023-4953-9cd5-e515324ca73c)\n\n\n## References\n\n[Blog](https://qwenlm.github.io/blog/qwen2.5-vl/)\n\n[Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "3b",
      "7b",
      "32b",
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2.5vl:3b",
        "size": "3.2GB",
        "size_gb": 3.2,
        "recommended_ram_gb": 4.0,
        "quantization": "q4_k_m",
        "context": "125K context",
        "context_window": 125000
      },
      {
        "tag": "qwen2.5vl:latest",
        "size": "6.0GB",
        "size_gb": 6.0,
        "recommended_ram_gb": 7.5,
        "quantization": "q4_k_m",
        "context": "125K context",
        "context_window": 125000
      },
      {
        "tag": "latest",
        "size": "6.0GB",
        "size_gb": 6.0,
        "recommended_ram_gb": 7.5,
        "quantization": "q4_k_m",
        "context": "125K",
        "context_window": 125000
      },
      {
        "tag": "qwen2.5vl:32b",
        "size": "21GB",
        "size_gb": 21.0,
        "recommended_ram_gb": 26.2,
        "quantization": "q4_k_m",
        "context": "125K context",
        "context_window": 125000
      },
      {
        "tag": "qwen2.5vl:72b",
        "size": "49GB",
        "size_gb": 49.0,
        "recommended_ram_gb": 61.2,
        "quantization": "q4_k_m",
        "context": "125K context",
        "context_window": 125000
      }
    ],
    "min_ram_gb": 3.2,
    "use_cases": [
      "Image Understanding",
      "Role Play"
    ],
    "domain": "Vision",
    "ai_languages": [],
    "complexity": null,
    "best_for": "Developers and Researchers for vision-related tasks.",
    "pulls": 1300000,
    "tags": 17,
    "last_updated": "2025-05-25",
    "last_updated_str": "9 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5decb2a2-2fbc-4539-bf9e-38951b58dd6f",
    "model_identifier": "mistral-small3.2",
    "model_name": "mistral-small3.2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-small3.2",
    "description": "An update to Mistral Small that improves on function calling, instruction following, and less repetition errors.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.\n\n\nSmall-3.2 improves in the following categories:\n\n\n\n\nInstruction following\n: Small-3.2 is better at following precise instructions\n\n\nRepetition errors\n: Small-3.2 produces less infinite generations or repetitive answers\n\n\nFunction calling\n: Small-3.2’s function calling template is more robust (see here and examples)\n\n\n\n\nIn all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/mistral-small3.2/cec4138e-e20c-4d5b-910f-56f8647dbda1\" width=\"140\" />\n\nMistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.\n\nSmall-3.2 improves in the following categories:\n\n* **Instruction following**: Small-3.2 is better at following precise instructions\n* **Repetition errors**: Small-3.2 produces less infinite generations or repetitive answers\n* **Function calling**: Small-3.2's function calling template is more robust (see here and examples)\n\nIn all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503.\n\n## References\n\n[Hugging Face](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools"
    ],
    "capability": "Vision",
    "labels": [
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-small3.2:latest",
        "size": "15GB",
        "size_gb": 15.0,
        "recommended_ram_gb": 18.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "15GB",
        "size_gb": 15.0,
        "recommended_ram_gb": 18.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 15.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers who need a robust text summarization and function calling model.",
    "pulls": 1300000,
    "tags": 5,
    "last_updated": "2025-06-25",
    "last_updated_str": "8 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "41c9c24e-9a55-4eb0-b35b-c96a122d1549",
    "model_identifier": "gemma3n",
    "model_name": "gemma3n",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma3n",
    "description": "Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages.\n\n\nGemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain.\n\n\nModels\n\n\nEffective 2B\n\n\nollama run gemma3n:e2b\n\n\n\nEffective 4B\n\n\nollama run gemma3n:e4b\n\n\n\nEvaluation\n\n\nModel evaluation metrics and results.\n\n\nBenchmark Results\n\n\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with \nIT\n are for\ninstruction-tuned models. Evaluation results marked with \nPT\n are for\npre-trained models. The models available on Ollama are instruction-tuned models.\n\n\n\n\nReasoning and factuality\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMetric\n\n\nn-shot\n\n\nE2B PT\n\n\nE4B PT\n\n\n\n\n\n\n\n\n\n\nHellaSwag\n\n\nAccuracy\n\n\n10-shot\n\n\n72.2\n\n\n78.6\n\n\n\n\n\n\nBoolQ\n\n\nAccuracy\n\n\n0-shot\n\n\n76.4\n\n\n81.6\n\n\n\n\n\n\nPIQA\n\n\nAccuracy\n\n\n0-shot\n\n\n78.9\n\n\n81.0\n\n\n\n\n\n\nSocialIQA\n\n\nAccuracy\n\n\n0-shot\n\n\n48.8\n\n\n50.0\n\n\n\n\n\n\nTriviaQA\n\n\nAccuracy\n\n\n5-shot\n\n\n60.8\n\n\n70.2\n\n\n\n\n\n\nNatural Questions\n\n\nAccuracy\n\n\n5-shot\n\n\n15.5\n\n\n20.9\n\n\n\n\n\n\nARC-c\n\n\nAccuracy\n\n\n25-shot\n\n\n51.7\n\n\n61.6\n\n\n\n\n\n\nARC-e\n\n\nAccuracy\n\n\n0-shot\n\n\n75.8\n\n\n81.6\n\n\n\n\n\n\nWinoGrande\n\n\nAccuracy\n\n\n5-shot\n\n\n66.8\n\n\n71.7\n\n\n\n\n\n\nBIG-Bench Hard\n\n\nAccuracy\n\n\nfew-shot\n\n\n44.3\n\n\n52.9\n\n\n\n\n\n\nDROP\n\n\nToken F1 score\n\n\n1-shot\n\n\n53.9\n\n\n60.8\n\n\n\n\n\n\n\n\nMultilingual\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMetric\n\n\nn-shot\n\n\nE2B IT\n\n\nE4B IT\n\n\n\n\n\n\n\n\n\n\nMGSM\n\n\nAccuracy\n\n\n0-shot\n\n\n53.1\n\n\n60.7\n\n\n\n\n\n\nWMT24++\n (ChrF)\n\n\nCharacter-level F-score\n\n\n0-shot\n\n\n42.7\n\n\n50.1\n\n\n\n\n\n\nInclude\n\n\nAccuracy\n\n\n0-shot\n\n\n38.6\n\n\n57.2\n\n\n\n\n\n\nMMLU\n (ProX)\n\n\nAccuracy\n\n\n0-shot\n\n\n8.1\n\n\n19.9\n\n\n\n\n\n\nOpenAI MMLU\n\n\nAccuracy\n\n\n0-shot\n\n\n22.3\n\n\n35.6\n\n\n\n\n\n\nGlobal-MMLU\n\n\nAccuracy\n\n\n0-shot\n\n\n55.1\n\n\n60.3\n\n\n\n\n\n\nECLeKTic\n\n\nECLeKTic score\n\n\n0-shot\n\n\n2.5\n\n\n1.9\n\n\n\n\n\n\n\n\nSTEM and code\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMetric\n\n\nn-shot\n\n\nE2B IT\n\n\nE4B IT\n\n\n\n\n\n\n\n\n\n\nGPQA\n Diamond\n\n\nRelaxedAccuracy/accuracy\n\n\n0-shot\n\n\n24.8\n\n\n23.7\n\n\n\n\n\n\nLiveCodeBench\n v5\n\n\npass@1\n\n\n0-shot\n\n\n18.6\n\n\n25.7\n\n\n\n\n\n\nCodegolf v2.2\n\n\npass@1\n\n\n0-shot\n\n\n11.0\n\n\n16.8\n\n\n\n\n\n\nAIME 2025\n\n\nAccuracy\n\n\n0-shot\n\n\n6.7\n\n\n11.6\n\n\n\n\n\n\n\n\nAdditional benchmarks\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMetric\n\n\nn-shot\n\n\nE2B IT\n\n\nE4B IT\n\n\n\n\n\n\n\n\n\n\nMMLU\n\n\nAccuracy\n\n\n0-shot\n\n\n60.1\n\n\n64.9\n\n\n\n\n\n\nMBPP\n\n\npass@1\n\n\n3-shot\n\n\n56.6\n\n\n63.6\n\n\n\n\n\n\nHumanEval\n\n\npass@1\n\n\n0-shot\n\n\n66.5\n\n\n75.0\n\n\n\n\n\n\nLiveCodeBench\n\n\npass@1\n\n\n0-shot\n\n\n13.2\n\n\n13.2\n\n\n\n\n\n\nHiddenMath\n\n\nAccuracy\n\n\n0-shot\n\n\n27.7\n\n\n37.7\n\n\n\n\n\n\nGlobal-MMLU-Lite\n\n\nAccuracy\n\n\n0-shot\n\n\n59.0\n\n\n64.5\n\n\n\n\n\n\nMMLU\n (Pro)\n\n\nAccuracy\n\n\n0-shot\n\n\n40.5\n\n\n50.6\n\n\n\n\n\n\n\n\nUsage and Limitations\n\n\nThese models have certain limitations that users should be aware of.\n\n\nIntended Usage\n\n\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n\n\n\nContent Creation and Communication\n\n\n\n\nText Generation\n: Generate creative text formats such as\npoems, scripts, code, marketing copy, and email drafts.\n\n\nChatbots and Conversational AI\n: Power conversational\ninterfaces for customer service, virtual assistants, or interactive\napplications.\n\n\nText Summarization\n: Generate concise summaries of a text\ncorpus, research papers, or reports.\n\n\nImage Data Extraction\n: Extract, interpret, and summarize\nvisual data for text communications.\n\n\nAudio Data Extraction\n: Transcribe spoken language, translate speech\nto text in other languages, and analyze sound-based data.\n\n\n\n\nResearch and Education\n\n\n\n\nNatural Language Processing (NLP) and generative model\nResearch\n: These models can serve as a foundation for researchers to\nexperiment with generative models and NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\n\n\nLanguage Learning Tools\n: Support interactive language\nlearning experiences, aiding in grammar correction or providing writing\npractice.\n\n\nKnowledge Exploration\n: Assist researchers in exploring large\nbodies of data by generating summaries or answering questions about\nspecific topics.\n\n\n\n\n\n\nEthics and Safety\n\n\nEthics and safety evaluation approach and results.\n\n\nEvaluation Approach\n\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n\n\n\nChild Safety\n: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\n\n\nContent Safety:\n Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\n\n\nRepresentational Harms\n: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\n\n\n\n\nIn addition to development level evaluations, we conduct “assurance\nevaluations” which are our ‘arms-length’ internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results’ ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n\nEvaluation Results\n\n\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models’\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/gemma3n/440e8da2-ddf4-482f-a341-4eeb3c08a332\" width=\"320\" />\n\nGemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages.\n\nGemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain.\n\n## Models \n\n### Effective 2B\n\n```\nollama run gemma3n:e2b\n```\n\n### Effective 4B\n\n```\nollama run gemma3n:e4b\n```\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with **IT** are for\ninstruction-tuned models. Evaluation results marked with **PT** are for\npre-trained models. The models available on Ollama are instruction-tuned models. \n\n![lm arena](/assets/library/gemma3n/b131b187-c935-4310-931b-439ce30533fd)\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | n-shot   |  E2B PT  |  E4B PT  |\n| ------------------------------ |----------------|----------|:--------:|:--------:|\n| [HellaSwag][hellaswag]         | Accuracy       | 10-shot  |   72.2   |   78.6   |\n| [BoolQ][boolq]                 | Accuracy       | 0-shot   |   76.4   |   81.6   |\n| [PIQA][piqa]                   | Accuracy       | 0-shot   |   78.9   |   81.0   |\n| [SocialIQA][socialiqa]         | Accuracy       | 0-shot   |   48.8   |   50.0   |\n| [TriviaQA][triviaqa]           | Accuracy       | 5-shot   |   60.8   |   70.2   |\n| [Natural Questions][naturalq]  | Accuracy       | 5-shot   |   15.5   |   20.9   |\n| [ARC-c][arc]                   | Accuracy       | 25-shot  |   51.7   |   61.6   |\n| [ARC-e][arc]                   | Accuracy       | 0-shot   |   75.8   |   81.6   |\n| [WinoGrande][winogrande]       | Accuracy       | 5-shot   |   66.8   |   71.7   |\n| [BIG-Bench Hard][bbh]          | Accuracy       | few-shot |   44.3   |   52.9   |\n| [DROP][drop]                   | Token F1 score | 1-shot   |   53.9   |   60.8   |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### Multilingual\n\n| Benchmark                           | Metric                  | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------|-------------------------|----------|:--------:|:--------:|\n| [MGSM][mgsm]                        | Accuracy                |  0-shot  |   53.1   |   60.7   |\n| [WMT24++][wmt24pp] (ChrF)           | Character-level F-score |  0-shot  |   42.7   |   50.1   |\n| [Include][include]                  | Accuracy                |  0-shot  |   38.6   |   57.2   |\n| [MMLU][mmlu] (ProX)                 | Accurac",
    "capabilities": [],
    "capability": null,
    "labels": [
      "e2b",
      "e4b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma3n:e2b",
        "size": "5.6GB",
        "size_gb": 5.6,
        "recommended_ram_gb": 7.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "gemma3n:latest",
        "size": "7.5GB",
        "size_gb": 7.5,
        "recommended_ram_gb": 9.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "7.5GB",
        "size_gb": 7.5,
        "recommended_ram_gb": 9.4,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 5.6,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers for text processing tasks.",
    "pulls": 1200000,
    "tags": 9,
    "last_updated": "2025-06-25",
    "last_updated_str": "8 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d8003583-6c0b-417f-b45e-21d5036ceed1",
    "model_identifier": "llama4",
    "model_name": "llama4",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama4",
    "description": "Meta's latest collection of multimodal models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These two models leverage a mixture-of-experts (MoE) architecture and support native multimodality (image input).\n\n\nSupported languages:\n Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\n\n\nInput:\n multilingual text, image\n\n\nOutput:\n multilingual text, code\n\n\nModels\n\n\nLlama 4 Scout\n\n\nollama run llama4:scout\n\n\n\n109B parameter MoE model with 17B active parameters\n\n\nLlama 4 Maverick\n\n\nollama run llama4:maverick\n\n\n\n400B parameter MoE model with 17B active parameters\n\n\nIntended Use\n\n\nIntended Use Cases:\n Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\n\n\nOut-of-scope\n: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card.\n\n\nNote:\n\n\n\n\nLlama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes \n200 total languages\n). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n\n\nLlama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n\n\n\n\nBenchmarks\n\n\n\n\n\n\nCategory\n\n\nBenchmark\n\n\n# Shots\n\n\nMetric\n\n\nLlama 3.3 70B\n\n\nLlama 3.1 405B\n\n\nLlama 4 Scout\n\n\nLlama 4 Maverick\n\n\n\n\n\n\n Image Reasoning\n\n\nMMMU\n\n\n0\n\n\naccuracy\n\n\n No multimodal support \n\n\n69.4\n\n\n73.4\n\n\n\n\n\n\n \n\n\nMMMU Pro^\n\n\n0\n\n\naccuracy\n\n\n \n\n\n52.2\n\n\n59.6\n\n\n\n\n\n\n \n\n\nMathVista\n\n\n0\n\n\naccuracy\n\n\n \n\n\n70.7\n\n\n73.7\n\n\n\n\n\n\n Image Understanding \n\n\nChartQA\n\n\n0\n\n\nrelaxed_accuracy\n\n\n \n\n\n88.8\n\n\n90.0\n\n\n\n\n\n\n \n\n\nDocVQA (test)\n\n\n0\n\n\nanls\n\n\n \n\n\n94.4\n\n\n94.4\n\n\n\n\n\n\n Code  \n\n\n LiveCodeBench\n(10/01/2024-02/01/2025)\n \n\n\n0\n\n\npass@1\n\n\n 33.3 \n\n\n 27.7 \n\n\n32.8\n\n\n43.4\n\n\n\n\n\n\n Reasoning & Knowledge  \n\n\n MMLU Pro\n\n\n0\n\n\nmacro_avg/acc\n\n\n 68.9 \n\n\n 73.4 \n\n\n74.3\n\n\n80.5\n\n\n\n\n\n\n \n\n\n GPQA Diamond\n\n\n0\n\n\naccuracy\n\n\n 50.5 \n\n\n 49.0 \n\n\n57.2\n\n\n69.8\n\n\n\n\n\n\n Multilingual  \n\n\n MGSM\n\n\n0\n\n\naverage/em\n\n\n 91.1 \n\n\n 91.6\n\n\n90.6\n\n\n92.3\n\n\n\n\n\n\n Long Context  \n\n\n MTOB (half book) eng->kgv/kgv->eng \n\n\n-\n\n\nchrF\n\n\n Context window is 128K \n\n\n42.2 / 36.6\n\n\n54.0 / 46.4\n\n\n\n\n\n\n \n\n\n MTOB (full book) eng->kgv/kgv->eng \n\n\n-\n\n\nchrF\n\n\n \n\n\n39.7 / 36.3\n\n\n50.8 / 46.7\n\n\n\n\n\n*reported numbers for MMMU Pro is the average of Standard and Vision tasks\n\n\nReference\n\n\n\n\nMeta Llama 4 \npost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/llama4/e7b65449-da66-4a50-b36c-d0c2359295b4)\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These two models leverage a mixture-of-experts (MoE) architecture and support native multimodality (image input). \n\n**Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\n\n**Input:** multilingual text, image\n\n**Output:** multilingual text, code\n\n### Models \n\n#### Llama 4 Scout\n\n```\nollama run llama4:scout\n```\n\n109B parameter MoE model with 17B active parameters\n\n#### Llama 4 Maverick\n\n```\nollama run llama4:maverick\n```\n\n400B parameter MoE model with 17B active parameters\n\n### Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. \n\n**Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card.\n\nNote: \n\n 1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes [200 total languages](https://ai.meta.com/research/no-language-left-behind/)). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n\n 2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n\n### Benchmarks \n\n<table>\n  <tr>\n    <th>Category</th>\n    <th>Benchmark</th>\n    <th># Shots</th>\n    <th>Metric</th>\n    <th>Llama 3.3 70B</th>\n    <th>Llama 3.1 405B</th>\n    <th>Llama 4 Scout</th>\n    <th>Llama 4 Maverick</th>\n  </tr>\n  <tr>\n    <td> Image Reasoning</td>\n    <td>MMMU</td>\n    <td>0</td>\n    <td>accuracy</td>\n    <td colspan=\"2\"> No multimodal support </td>\n    <td>69.4</td>\n    <td>73.4</td>\n  </tr>\n  <tr>\n    <td>  </td>\n    <td>MMMU Pro^</td>\n    <td>0</td>\n    <td>accuracy</td>\n    <td colspan=\"2\"> </td>\n    <td>52.2</td>\n    <td>59.6</td>\n  </tr>\n  <tr>\n    <td>  </td>\n    <td>MathVista</td>\n    <td>0</td>\n    <td>accuracy</td>\n    <td colspan=\"2\">  </td>\n    <td>70.7</td>\n    <td>73.7</td>\n  </tr>\n  <tr>\n    <td> Image Understanding </td>\n    <td>ChartQA</td>\n    <td>0</td>\n    <td>relaxed_accuracy</td>\n    <td colspan=\"2\">  </td>\n    <td>88.8</td>\n    <td>90.0</td>\n  </tr>\n  <tr>\n    <td>  </td>\n    <td>DocVQA (test)</td>\n    <td>0</td>\n    <td>anls</td>\n    <td colspan=\"2\"> </td>\n    <td>94.4</td>\n    <td>94.4</td>\n  </tr>\n  <tr>\n    <td> Code  </td>\n    <td> LiveCodeBench\n(10/01/2024-02/01/2025)\n </td>\n    <td>0</td>\n    <td>pass@1</td>\n    <td> 33.3 </td>\n    <td> 27.7 </td>\n    <td>32.8</td>\n    <td>43.4</td>\n  </tr>\n  <tr>\n    <td> Reasoning & Knowledge  </td>\n    <td> MMLU Pro</td>\n    <td>0</td>\n    <td>macro_avg/acc</td>\n    <td> 68.9 </td>\n    <td> 73.4 </td>\n    <td>74.3</td>\n    <td>80.5</td>\n  </tr>\n    <tr>\n    <td>  </td>\n    <td> GPQA Diamond</td>\n    <td>0</td>\n    <td>accuracy</td>\n    <td> 50.5 </td>\n    <td> 49.0 </td>\n    <td>57.2</td>\n    <td>69.8</td>\n  </tr>\n    <tr>\n    <td> Multilingual  </td>\n    <td> MGSM</td>\n    <td>0</td>\n    <td>average/em</td>\n    <td> 91.1 </td>\n    <td> 91.6</td>\n    <td>90.6</td>\n    <td>92.3</td>\n  </tr>\n    <tr>\n    <td> Long Context  </td>\n    <td> MTOB (half book) eng->kgv/kgv->eng </td>\n    <td>-</td>\n    <td>chrF</td>\n    <td colspan=\"2\"> Context window is 128K </td>\n    <td>42.2 / 36.6</td>\n    <td>54.0 / 46.4</td>\n  </tr>\n    <tr>\n    <td> </td>\n    <td> MTOB (full book) eng->kgv/kgv->eng </td>\n    <td>-</td>\n    <td>chrF</td>\n    <td colspan=\"2\">    </td>\n    <td>39.7 / 36.3</td>\n    <td>50.8 / 46.7</td>\n  </tr>\n</table>\n*reported numbers for MMMU Pro is the average of Standard and Vision tasks\n\n### Reference\n- Meta Llama 4 [post](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools"
    ],
    "capability": "Vision",
    "labels": [
      "16x17b",
      "128x17b"
    ],
    "memory_requirements": [
      {
        "tag": "llama4:latest",
        "size": "67GB",
        "size_gb": 67.0,
        "recommended_ram_gb": 83.8,
        "quantization": "q4_k_m",
        "context": "10M context",
        "context_window": 10000000
      },
      {
        "tag": "latest",
        "size": "67GB",
        "size_gb": 67.0,
        "recommended_ram_gb": 83.8,
        "quantization": "q4_k_m",
        "context": "10M",
        "context_window": 10000000
      },
      {
        "tag": "llama4:128x17b",
        "size": "245GB",
        "size_gb": 245.0,
        "recommended_ram_gb": 306.2,
        "quantization": "q4_k_m",
        "context": "1M context",
        "context_window": 1000000
      }
    ],
    "min_ram_gb": 67.0,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "Arabic",
      "English",
      "French",
      "German"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for advanced language tasks.",
    "pulls": 1200000,
    "tags": 11,
    "last_updated": "2025-06-25",
    "last_updated_str": "8 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4c6c85df-6e77-4be5-9c45-900542e0b0b0",
    "model_identifier": "dolphin-phi",
    "model_name": "dolphin-phi",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphin-phi",
    "description": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDolphin Phi 2.6 is an uncensored model based on the 2.7B \nPhi\n model by Microsoft Research, using similar datasets as other versions of this model such as \nDolphin Mixtral\n.\n\n\nIt was created by \nEric Hartford\n and Cognitive Computations.\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/c65aad4e-7a58-4645-97f2-5e8ca158275f\" width=\"280\" />\n\nDolphin Phi 2.6 is an uncensored model based on the 2.7B [Phi](https://ollama.ai/library/phi) model by Microsoft Research, using similar datasets as other versions of this model such as [Dolphin Mixtral](https://ollama.ai/library/dolphin-mixtral).\n\nIt was created by [Eric Hartford](https://erichartford.com/) and Cognitive Computations.\n\n## References\n\n[HuggingFace](https://huggingface.co/cognitivecomputations/dolphin-2_6-phi-2)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2.7b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphin-phi:latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text generation and summarization tasks.",
    "pulls": 1200000,
    "tags": 15,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "aa8c25d2-f674-49e1-b9e5-d227c870d1a1",
    "model_identifier": "phi4-reasoning",
    "model_name": "phi4-reasoning",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi4-reasoning",
    "description": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival much larger models on complex reasoning tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nPhi 4 reasoning and reasoning plus models are 14 billion parameter models that rival much larger models on complex reasoning tasks.\n\n\nPhi 4 reasoning model is trained via supervised fine-tuning of Phi 4 on carefully curated reasoning demonstrations from OpenAI’s o3-mini. This model demonstrates meticulous data curation and high quality synthetic datasets allow smaller models to compete with larger counterparts.\n\n\nPhi 4 reasoning plus model builds on top of Phi 4 reasoning, and is further trained with reinforcement learning to deliver higher accuracy.\n\n\nModels\n\n\nPhi 4 reasoning\n\n\nollama run phi4-reasoning\n\n\n\nPhi 4 reasoning plus\n\n\nollama run phi4-reasoning:plus\n\n\n\nBenchmarks\n\n\n\n\n Phi-4-reasoning performance across representative reasoning benchmarks spanning mathematical and scientific reasoning. We illustrate the performance gains from reasoning-focused post-training of Phi-4 via Phi-4-reasoning (SFT) and Phi-4-reasoning-plus (SFT+RL), alongside a representative set of baselines from two model families: open-weight models from DeepSeek including DeepSeek R1 (671B Mixture-of-Experts) and its distilled dense variant DeepSeek-R1 Distill Llama 70B, and OpenAI’s proprietary frontier models o1-mini and o3-mini. Phi-4-reasoning and Phi-4-reasoning-plus consistently outperform the base model Phi-4 by significant margins, exceed DeepSeek-R1 Distill Llama 70B (5x larger) and demonstrate competitive performance against significantly larger models such as Deepseek-R1.\n\n\n\n\nAccuracy of models across general-purpose benchmarks for: long input context QA (FlenQA), instruction following (IFEval), Coding (HumanEvalPlus), knowledge & language understanding (MMLUPro), safety detection (ToxiGen), and other general skills (ArenaHard and PhiBench).\n\n\n\nReferences\n\n\nBlog post \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhi 4 reasoning and reasoning plus models are 14 billion parameter models that rival much larger models on complex reasoning tasks. \n\nPhi 4 reasoning model is trained via supervised fine-tuning of Phi 4 on carefully curated reasoning demonstrations from OpenAI's o3-mini. This model demonstrates meticulous data curation and high quality synthetic datasets allow smaller models to compete with larger counterparts. \n\nPhi 4 reasoning plus model builds on top of Phi 4 reasoning, and is further trained with reinforcement learning to deliver higher accuracy. \n\n\n### Models \n\n**Phi 4 reasoning**\n\n```\nollama run phi4-reasoning\n```\n\n**Phi 4 reasoning plus**\n\n```\nollama run phi4-reasoning:plus\n```\n\n\n### Benchmarks \n![image.png](/assets/library/phi4-reasoning/6df829cd-a2e5-4a25-906b-29de63e072df)\n\n<small> Phi-4-reasoning performance across representative reasoning benchmarks spanning mathematical and scientific reasoning. We illustrate the performance gains from reasoning-focused post-training of Phi-4 via Phi-4-reasoning (SFT) and Phi-4-reasoning-plus (SFT+RL), alongside a representative set of baselines from two model families: open-weight models from DeepSeek including DeepSeek R1 (671B Mixture-of-Experts) and its distilled dense variant DeepSeek-R1 Distill Llama 70B, and OpenAI’s proprietary frontier models o1-mini and o3-mini. Phi-4-reasoning and Phi-4-reasoning-plus consistently outperform the base model Phi-4 by significant margins, exceed DeepSeek-R1 Distill Llama 70B (5x larger) and demonstrate competitive performance against significantly larger models such as Deepseek-R1.</small>\n\n![image.png](/assets/library/phi4-reasoning/b4500c73-f2bf-4558-8391-b58e63c5ce8c)\n\n<small>Accuracy of models across general-purpose benchmarks for: long input context QA (FlenQA), instruction following (IFEval), Coding (HumanEvalPlus), knowledge & language understanding (MMLUPro), safety detection (ToxiGen), and other general skills (ArenaHard and PhiBench). \n</small>\n\n## References\n\n[Blog post ](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "phi4-reasoning:latest",
        "size": "11GB",
        "size_gb": 11.0,
        "recommended_ram_gb": 13.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "11GB",
        "size_gb": 11.0,
        "recommended_ram_gb": 13.8,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 11.0,
    "use_cases": [
      "Question Answering",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Ideal for complex reasoning tasks and question answering in English.",
    "pulls": 1200000,
    "tags": 9,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "79bea3d4-4dd5-41ad-94fe-5ab05a9b06b0",
    "model_identifier": "deepscaler",
    "model_name": "deepscaler",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepscaler",
    "description": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI’s o1-preview with just 1.5B parameters on popular math evaluations.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nDeepScaleR\n\n\n🚀 Democratizing Reinforcement Learning for LLMs 🌟\n\n\nDeepScaleR-1.5B-Preview is a language model fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 43.1% Pass@1 accuracy on AIME 2024, representing a 15% improvement over the base model (28.8%) and surpassing OpenAI’s O1-Preview performance with just 1.5B parameters.\n\n\n\n\n\n\n\n\nModel\n\n\nAIME 2024\n\n\nMATH 500\n\n\nAMC 2023\n\n\nMinerva Math\n\n\nOlympiad Bench\n\n\nAvg.\n\n\n\n\n\n\n\n\n\n\nDeepScaleR-1.5B-Preview\n\n\n43.1\n\n\n87.8\n\n\n73.6\n\n\n30.2\n\n\n50.0\n\n\n57.0\n\n\n\n\n\n\nDeepSeek-R1-Distill-Qwen-1.5B\n\n\n28.8\n\n\n82.8\n\n\n62.9\n\n\n26.5\n\n\n43.3\n\n\n48.9\n\n\n\n\n\n\nO1-Preview\n\n\n40.0\n\n\n81.4\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\n\n\n\n\nData\n\n\nOur training dataset consists of approximately 40,000 unique problem-answer pairs compiled from:\n\n\n\n\nAIME problems (1984-2023)\n\n\nAMC problems (prior to 2023)\n\n\nOmni-MATH dataset\n\n\nStill dataset\n\n\n\n\nEvaluation\n\n\nWe evaluate our model on competition-level mathematics benchmarks, including AIME 2024, AMC 2023, MATH-500, Minerva Math, and OlympiadBench. Below, Pass@1 accuracy is reported, averaged over 16 samples for each problem.\n\n\n\n\n\n\n\n\nModel\n\n\nAIME 2024\n\n\nMATH 500\n\n\nAMC 2023\n\n\nMinerva Math\n\n\nOlympiadBench\n\n\nAvg.\n\n\n\n\n\n\n\n\n\n\nQwen-2.5-Math-7B-Instruct\n\n\n13.3\n\n\n79.8\n\n\n50.6\n\n\n34.6\n\n\n40.7\n\n\n43.8\n\n\n\n\n\n\nrStar-Math-7B\n\n\n26.7\n\n\n78.4\n\n\n47.5\n\n\n-\n\n\n47.1\n\n\n-\n\n\n\n\n\n\nEurus-2-7B-PRIME\n\n\n26.7\n\n\n79.2\n\n\n57.8\n\n\n38.6\n\n\n42.1\n\n\n48.9\n\n\n\n\n\n\nQwen2.5-7B-SimpleRL\n\n\n26.7\n\n\n82.4\n\n\n62.5\n\n\n39.7\n\n\n43.3\n\n\n50.9\n\n\n\n\n\n\nDeepSeek-R1-Distill-Qwen-1.5B\n\n\n28.8\n\n\n82.8\n\n\n62.9\n\n\n26.5\n\n\n43.3\n\n\n48.9\n\n\n\n\n\n\nStill-1.5B\n\n\n32.5\n\n\n84.4\n\n\n66.7\n\n\n29.0\n\n\n45.4\n\n\n51.6\n\n\n\n\n\n\nDeepScaleR-1.5B-Preview\n\n\n43.1\n\n\n87.8\n\n\n73.6\n\n\n30.2\n\n\n50.0\n\n\n57.0\n\n\n\n\n\n\nO1-Preview\n\n\n40.0\n\n\n81.4\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\n\n\nWe compare DeepScaleR with the base DeepSeek model we use, as well as recent academic works exploring RL for reasoning tasks. DeepScaleR significantly outperforms the base model across all benchmarks, achieving a 14.4% absolute gain on AIME2024 and an 8.1% overall improvement. Additionally, DeepScaleR surpasses recent academic works such as rSTAR, Prime, and SimpleRL, which are finetuned from 7B models. DeepScaleR achieves O1-preview-level performance with only 1.5B parameters—a remarkable efficiency gain.\n\n\n\n\nReferences\n\n\nArticle\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# DeepScaleR\n\n🚀 Democratizing Reinforcement Learning for LLMs 🌟\n\nDeepScaleR-1.5B-Preview is a language model fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 43.1% Pass@1 accuracy on AIME 2024, representing a 15% improvement over the base model (28.8%) and surpassing OpenAI's O1-Preview performance with just 1.5B parameters.\n\n\n| Model | AIME 2024 | MATH 500 | AMC 2023 | Minerva Math | Olympiad Bench | Avg. |\n| --- | --- | --- | --- | --- | --- | --- |\n| **DeepScaleR-1.5B-Preview** | **43.1** | **87.8** | **73.6** | **30.2** | **50.0** | **57.0** |\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.8 | 82.8 | 62.9 | 26.5 | 43.3 | 48.9 |\n| O1-Preview | 40.0 | 81.4 | - | - | - | - |\n\n![image.png](/assets/library/deepscaler/c8dcb4f4-481a-4fea-89a9-168cdd1f48fe)\n\n### Data\nOur training dataset consists of approximately 40,000 unique problem-answer pairs compiled from:\n\n- AIME problems (1984-2023)\n- AMC problems (prior to 2023)\n- Omni-MATH dataset\n- Still dataset\n\n### Evaluation\n\nWe evaluate our model on competition-level mathematics benchmarks, including AIME 2024, AMC 2023, MATH-500, Minerva Math, and OlympiadBench. Below, Pass@1 accuracy is reported, averaged over 16 samples for each problem. \n\n| **Model** | **AIME 2024** | **MATH 500** | **AMC 2023** | **Minerva Math** | **OlympiadBench** | **Avg.** |\n| --- | --- | --- | --- | --- | --- | --- |\n| Qwen-2.5-Math-7B-Instruct | 13.3 | 79.8 | 50.6 | 34.6 | 40.7 | 43.8 |\n| rStar-Math-7B | 26.7 | 78.4 | 47.5 | - | 47.1 | - |\n| Eurus-2-7B-PRIME | 26.7 | 79.2 | 57.8 | 38.6 | 42.1 | 48.9 |\n| Qwen2.5-7B-SimpleRL | 26.7 | 82.4 | 62.5 | **39.7** | 43.3 | 50.9 |\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.8 | 82.8 | 62.9 | 26.5 | 43.3 | 48.9 |\n| Still-1.5B | 32.5 | 84.4 | 66.7 | 29.0 | 45.4 | 51.6 |\n| **DeepScaleR-1.5B-Preview** | **43.1** | **87.8** | **73.6** | 30.2 | **50.0** | **57.0** |\n| O1-Preview | 40.0 | 81.4 | - | - | - | - |\n\nWe compare DeepScaleR with the base DeepSeek model we use, as well as recent academic works exploring RL for reasoning tasks. DeepScaleR significantly outperforms the base model across all benchmarks, achieving a 14.4% absolute gain on AIME2024 and an 8.1% overall improvement. Additionally, DeepScaleR surpasses recent academic works such as rSTAR, Prime, and SimpleRL, which are finetuned from 7B models. DeepScaleR achieves O1-preview-level performance with only 1.5B parameters—a remarkable efficiency gain.\n\n![image.png](/assets/library/deepscaler/afbe7f8e-04db-4350-ab39-352c023a6218)\n\n\n## References\n\n[Article](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2)\n\n[GitHub](https://github.com/agentica-project/deepscaler)\n\n[Hugging Face](https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.5b"
    ],
    "memory_requirements": [
      {
        "tag": "deepscaler:latest",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 3.6,
    "use_cases": [
      "Question Answering",
      "Math"
    ],
    "domain": "Math",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Users requiring a fine-tuned math language model with high accuracy.",
    "pulls": 1100000,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0f555550-7f2e-41bb-bf07-9af720f5ab05",
    "model_identifier": "magistral",
    "model_name": "magistral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/magistral",
    "description": "Magistral is a small, efficient reasoning model with 24B parameters.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMagistral is the first reasoning model by Mistral AI, excelling in domain-specific, transparent, and multilingual reasoning.\n\n\n\n\n\n\nKey features\n\n\n\n\nReasoning:\n Capable of long chains of reasoning traces before providing an answer.\n\n\nMultilingual:\n Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\n\n\nApache 2.0 License:\n Open license allowing usage and modification for both commercial and non-commercial purposes.\n\n\nContext Window:\n A 128k context window, \nbut\n performance might degrade past \n40k\n. Hence we recommend setting the maximum model length to 40k.\n\n\n\n\nMagistral is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical.\n\n\nBusiness strategy and operations.\n\n\nBuilding on our flagship \nmodels\n, Magistral is designed for research, strategic planning, operational optimization, and data-driven decision making — whether executing risk assessment and modelling with multiple factors, or calculating optimal delivery windows under constraints.\n\n\nRegulated industries and sectors.\n\n\nLegal, finance, healthcare, and government professionals get traceable reasoning that meets compliance requirements. Every conclusion can be traced back through its logical steps, providing auditability for high-stakes environments with domain-specialized AI.\n\n\nSystems, software, and data engineering.\n\n\nMagistral enhances coding and development use cases: compared to non-reasoning models, it significantly improves project planning, backend architecture, frontend design, and data engineering through sequenced, multi-step actions involving external tools or API.\n\n\nContent and communication.\n\n\nOur early tests indicated that Magistral is an excellent creative companion. We highly recommend it for creative writing and storytelling, with the model capable of producing coherent or — if needed — delightfully eccentric copy.\n\n\nReference\n\n\n\n\nMagistral paper\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/mistral-small3.1/88f81c26-7028-4f08-b906-92b873d5536e\" width=\"120\" />\n\nMagistral is the first reasoning model by Mistral AI, excelling in domain-specific, transparent, and multilingual reasoning. \n\n![magistral small comparison to medium](/assets/library/magistral/c57cd779-0588-4d74-9f48-89ba50f7a565)\n\n![magistral small](/assets/library/magistral/fe972176-2881-42bc-bbd8-90a079188aa9)\n\n### Key features \n\n- **Reasoning:** Capable of long chains of reasoning traces before providing an answer.\n- **Multilingual:** Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 128k context window, **but** performance might degrade past **40k**. Hence we recommend setting the maximum model length to 40k.\n\nMagistral is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical.\n\n#### Business strategy and operations.\n\nBuilding on our flagship [models](https://mistral.ai/models), Magistral is designed for research, strategic planning, operational optimization, and data-driven decision making — whether executing risk assessment and modelling with multiple factors, or calculating optimal delivery windows under constraints.\n\n#### Regulated industries and sectors.\n\nLegal, finance, healthcare, and government professionals get traceable reasoning that meets compliance requirements. Every conclusion can be traced back through its logical steps, providing auditability for high-stakes environments with domain-specialized AI.\n\n#### Systems, software, and data engineering.\n\nMagistral enhances coding and development use cases: compared to non-reasoning models, it significantly improves project planning, backend architecture, frontend design, and data engineering through sequenced, multi-step actions involving external tools or API.\n\n#### Content and communication.\n\nOur early tests indicated that Magistral is an excellent creative companion. We highly recommend it for creative writing and storytelling, with the model capable of producing coherent or — if needed — delightfully eccentric copy.\n\n### Reference\n\n- [Magistral paper](https://mistral.ai/static/research/magistral.pdf)\n- [Blog post](https://mistral.ai/news/magistral)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "magistral:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "39K context",
        "context_window": 39000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "39K",
        "context_window": 39000
      }
    ],
    "min_ram_gb": 14.0,
    "use_cases": [
      "Question Answering",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Ideal for complex reasoning tasks and multilingual applications.",
    "pulls": 1100000,
    "tags": 5,
    "last_updated": "2025-06-25",
    "last_updated_str": "8 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "699eacd5-f8ee-427b-b7e0-58757461bc78",
    "model_identifier": "dolphin-mixtral",
    "model_name": "dolphin-mixtral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphin-mixtral",
    "description": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Dolphin model by Eric Hartford based on \nMixtral\n that is trained with additional datasets:\n\n\n\n\nSynthia, OpenHermes and PureDove\n\n\nNew Dolphin-Coder\n\n\nMagiCoder\n\n\n\n\nSizes\n\n\n\n\ndolphin-mixtral:8x22b\n\n\ndolphin-mixtral:8x7b\n\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/c65aad4e-7a58-4645-97f2-5e8ca158275f\" width=\"280\" />\n\nThe Dolphin model by Eric Hartford based on [Mixtral](https://ollama.ai/library/mixtral) that is trained with additional datasets:\n\n* Synthia, OpenHermes and PureDove\n* New Dolphin-Coder\n* MagiCoder\n\n### Sizes\n\n* `dolphin-mixtral:8x22b`\n* `dolphin-mixtral:8x7b`\n\n## References\n\n[HuggingFace](https://huggingface.co/ehartford/dolphin-2.5-mixtral-8x7b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8x7b",
      "8x22b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphin-mixtral:latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "dolphin-mixtral:8x22b",
        "size": "80GB",
        "size_gb": 80.0,
        "recommended_ram_gb": 100.0,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 26.0,
    "use_cases": [
      "Code Generation",
      "Text Summarization"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on coding tasks.",
    "pulls": 1000000,
    "tags": 70,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9643709d-7cbe-4095-ab98-f916907ff201",
    "model_identifier": "dolphin-llama3",
    "model_name": "dolphin-llama3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphin-llama3",
    "description": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n🐬 Dolphin 2.9 Llama 3\n\n\nDolphin-2.9 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.\n\n\nDolphin is \nuncensored\n. The dataset has been filtered to remove alignment and bias. This makes the model more compliant.\n\n\nCurated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations.\n\n\nSizes\n\n\n\n\ndolphin-llama3:8b\n\n\ndolphin-llama3:70b\n\n\n\n\n256K Context Window\n\n\n\n\nNote: using a 256k context window requires at least 64GB of memory.\n\n\n\n\nDolphin Llama 3 also has a 256k context window version. To extend the context window use:\n\n\nAPI\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"dolphin-llama3:8b-256k\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 256000\n  }\n}'\n\n\n\nCLI\n\n\nollama run dolphin-llama3:8b-256k\n>>> /set parameter num_ctx 256000\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 🐬 Dolphin 2.9 Llama 3\n\nDolphin-2.9 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.\n\nDolphin is [uncensored](https://erichartford.com/uncensored-models). The dataset has been filtered to remove alignment and bias. This makes the model more compliant.\n\nCurated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations.\n\n## Sizes\n\n* `dolphin-llama3:8b`\n* `dolphin-llama3:70b`\n\n## 256K Context Window\n\n> Note: using a 256k context window requires at least 64GB of memory.\n\nDolphin Llama 3 also has a 256k context window version. To extend the context window use:\n\n<sub>API</sub>\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"dolphin-llama3:8b-256k\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 256000\n  }\n}'\n```\n\n<sub>CLI</sub>\n```\nollama run dolphin-llama3:8b-256k\n>>> /set parameter num_ctx 256000\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphin-llama3:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "dolphin-llama3:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Code Generation",
      "Function Calling",
      "Role Play"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced coding tasks and role-playing scenarios.",
    "pulls": 1000000,
    "tags": 53,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "7f8d498c-ad7d-4c4a-8594-3ff028aef44d",
    "model_identifier": "phi",
    "model_name": "phi",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi",
    "description": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhi-2 is a small language model capable of common-sense reasoning and language understanding. It showcases “state-of-the-art performance” among language models with less than 13 billion parameters.\n\n\nExample prompt\n\n\nBy default, \nphi\n includes a chat prompt template designed for multi-turn conversations:\n\n\n% ollama run phi\n>>> Hello, can you help me find my way to Toronto?\n Certainly! What is the exact location in Toronto that you are looking for?\n\n>>> Yonge & Bloor\n Sure, Yonge and Bloor is a busy intersection in downtown Toronto. Would you like to take public transportation or drive there?\n\n>>> Public transportation\n Great! The easiest way to get there is by taking the TTC subway. You can take Line 1, which runs along Yonge Street and passes through downtown Toronto.\n\n\n\nUsing Ollama’s API:\n\n\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"phi\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n\n\n\nExample prompts (raw mode)\n\n\nPhi also responds well to a wide variety of prompt formats when using \nraw mode\n in Ollama’s API, which bypasses all default prompt templating:\n\n\nInstruct\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"Instruct: Write a detailed analogy between mathematics and a lighthouse.\\nOutput:\",\n  \"options\": {\n    \"stop\": [\"Instruct:\", \"Output:\"]\n  },\n  \"raw\": true,\n  \"stream\": false\n}'\n\n\n\nCode Completion\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"def print_prime(n):\\n  \",\n  \"raw\": true,\n  \"stream\": false\n}'\n\n\n\nText completion\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"There once was a mouse named\",\n  \"raw\": true,\n  \"stream\": false\n}'\n\n\n\nReferences\n\n\nHuggingFace\n\n\nBlog Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/e06e1a36-97b2-417a-b2b2-028c980359b1\" width=\"240\" />\n\nPhi-2 is a small language model capable of common-sense reasoning and language understanding. It showcases \"state-of-the-art performance\" among language models with less than 13 billion parameters.\n\n### Example prompt\n\nBy default, `phi` includes a chat prompt template designed for multi-turn conversations:\n\n```\n% ollama run phi\n>>> Hello, can you help me find my way to Toronto?\n Certainly! What is the exact location in Toronto that you are looking for?\n\n>>> Yonge & Bloor\n Sure, Yonge and Bloor is a busy intersection in downtown Toronto. Would you like to take public transportation or drive there?\n\n>>> Public transportation\n Great! The easiest way to get there is by taking the TTC subway. You can take Line 1, which runs along Yonge Street and passes through downtown Toronto.\n```\n\nUsing Ollama's API:\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"phi\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n```\n\n### Example prompts (raw mode)\n\nPhi also responds well to a wide variety of prompt formats when using [raw mode](https://github.com/jmorganca/ollama/blob/main/docs/api.md#request-raw-mode) in Ollama's API, which bypasses all default prompt templating:\n\n#### Instruct\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"Instruct: Write a detailed analogy between mathematics and a lighthouse.\\nOutput:\",\n  \"options\": {\n    \"stop\": [\"Instruct:\", \"Output:\"]\n  },\n  \"raw\": true,\n  \"stream\": false\n}'\n```\n\n#### Code Completion\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"def print_prime(n):\\n  \",\n  \"raw\": true,\n  \"stream\": false\n}'\n```\n\n#### Text completion\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"There once was a mouse named\",\n  \"raw\": true,\n  \"stream\": false\n}'\n```\n\n\n## References\n\n[HuggingFace](https://huggingface.co/microsoft/phi-2)\n\n[Blog Post](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2.7b"
    ],
    "memory_requirements": [
      {
        "tag": "phi:latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced language tasks.",
    "pulls": 1000000,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9efeaf3c-e4f9-4dea-8d95-cbec14a98537",
    "model_identifier": "smollm",
    "model_name": "smollm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/smollm",
    "description": "🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSmolLM is a series of small language models available in three sizes: 135M, 360M, and 1.7B parameters.\n\n\nReferences\n\n\nBlog post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/smollm/50a4746f-b96d-4948-aa52-7ff06524f485\" width=\"512\" />\n\nSmolLM is a series of small language models available in three sizes: 135M, 360M, and 1.7B parameters.\n\n## References\n\n[Blog post](https://huggingface.co/blog/smollm)\n\n[Hugging Face](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "135m",
      "360m",
      "1.7b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Chat Assistant"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Users needing a small language model for various text tasks.",
    "pulls": 958300,
    "tags": 94,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "7427bf04-db97-4efc-b79f-adf91b0f6015",
    "model_identifier": "phi4-mini",
    "model_name": "phi4-mini",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi4-mini",
    "description": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.5.13\n or later.\n\n\n\n\n\n\nPhi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.\n\n\nPrimary use cases\n\n\nThe model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require:\n\n\n\n\nMemory/compute constrained environments\n\n\nLatency bound scenarios\n\n\nStrong reasoning (especially math and logic).\n\n\nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n\n\n\nReferences\n\n\nHugging Face\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires [Ollama 0.5.13](https://github.com/ollama/ollama/releases/tag/v0.5.13) or later.\n\n<img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" />\n\nPhi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.\n\n## Primary use cases\n\nThe model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require:\n\n- Memory/compute constrained environments\n- Latency bound scenarios\n- Strong reasoning (especially math and logic).\n- The model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n## References\n\n[Hugging Face](https://huggingface.co/microsoft/Phi-4-mini-instruct)\n\n[Blog post](https://techcommunity.microsoft.com/blog/educatordeveloperblog/welcome-to-the-new-phi-4-models---microsoft-phi-4-mini--phi-4-multimodal/4386037)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "3.8b"
    ],
    "memory_requirements": [
      {
        "tag": "phi4-mini:latest",
        "size": "2.5GB",
        "size_gb": 2.5,
        "recommended_ram_gb": 3.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "2.5GB",
        "size_gb": 2.5,
        "recommended_ram_gb": 3.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 2.5,
    "use_cases": [
      "Function Calling",
      "Text Summarization",
      "Question Answering",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Broad multilingual commercial and research use.",
    "pulls": 902300,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "12 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "cb725bfd-4c3d-4d53-8e57-c587feb309a9",
    "model_identifier": "granite3.3",
    "model_name": "granite3.3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3.3",
    "description": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGranite 3.3\n\n\nThe IBM Granite \n2B and 8B models\n are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities. These models deliver significant gains on benchmarks for measuring generic performance including AlpacaEval-2.0 and Arena-Hard, and improvements in mathematics, coding, and instruction following. They also supports Fill-in-the-Middle (FIM) for code completion tasks and structured reasoning.\n\n\nParameter Sizes\n\n\n2B:\n\n\nollama run granite3.3:2b\n\n\n8B:\n\n\nollama run granite3.3:8b\n\n\nSupported Languages\n\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.\n\n\nIntended Use\n\n\nThese models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\n\n\nCapabilities\n\n\n\n\nThinking\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related tasks\n\n\nFunction-calling tasks\n\n\nMultilingual dialog use cases\n\n\nFill-in-the-middle\n\n\nLong-context tasks including long document/meeting summarization, long document QA, etc.\n\n\n\n\nThinking\n\n\nTo enable thinking, add a message with “role”: “control” and set “content” to “thinking”. For example:\n\n\n{\n    \"messages\": [\n        {\"role\": \"control\", \"content\": \"thinking\"},\n        {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"}\n    ]\n}\n\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: April 16th, 2025\n\n\nLicense:\n \nApache 2.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/granite3.2/90c5e567-0004-425c-a17a-1b846c2b5d3d\" width=\"600\" />\n\n## Granite 3.3\n\nThe IBM Granite **2B and 8B models** are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities. These models deliver significant gains on benchmarks for measuring generic performance including AlpacaEval-2.0 and Arena-Hard, and improvements in mathematics, coding, and instruction following. They also supports Fill-in-the-Middle (FIM) for code completion tasks and structured reasoning.\n\n### Parameter Sizes\n\n**2B:**\n\n`ollama run granite3.3:2b`\n\n**8B:**\n\n`ollama run granite3.3:8b`\n\n### Supported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.\n\n### Intended Use\nThese models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\n\n### Capabilities\n* Thinking\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related tasks\n* Function-calling tasks\n* Multilingual dialog use cases\n* Fill-in-the-middle\n* Long-context tasks including long document/meeting summarization, long document QA, etc.\n\n### Thinking\nTo enable thinking, add a message with \"role\": \"control\" and set \"content\" to \"thinking\". For example:\n\n```\n{\n    \"messages\": [\n        {\"role\": \"control\", \"content\": \"thinking\"},\n        {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"}\n    ]\n}\n```\n\n## Learn more\n\n- **Developers:** IBM Research\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: April 16th, 2025\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "2b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3.3:2b",
        "size": "1.5GB",
        "size_gb": 1.5,
        "recommended_ram_gb": 1.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "granite3.3:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.5,
    "use_cases": [
      "Code Generation",
      "Reasoning",
      "Translation",
      "Math"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "Spanish",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for code generation, reasoning tasks, translation, and mathematical computations.",
    "pulls": 901100,
    "tags": 3,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "906c53a1-8525-4728-b198-1282d46ee779",
    "model_identifier": "qwen3-embedding",
    "model_name": "qwen3-embedding",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3-embedding",
    "description": "Building upon the foundational models of the Qwen3 series, Qwen3 Embedding provides a comprehensive range of text embeddings models in various sizes",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nHighlights\n\n\nThe Qwen3 Embedding model series is specifically designed for text embedding tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\n\n\nExceptional Versatility\n: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks \nNo.1\n in the MTEB multilingual leaderboard (as of June 5, 2025, score \n70.58\n).\n\n\nComprehensive Flexibility\n: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for embedding models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, these models allow for flexible vector definitions across all dimensions, support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\n\n\nMultilingual Capability\n: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\n\n\nQwen3-Embedding-8B\n has the following features:\n\n\n\n\nModel Type: Text Embedding\n\n\nSupported Languages: 100+ Languages\n\n\nNumber of Paramaters: 8B\n\n\nContext Length: 32k\n\n\nEmbedding Dimension: Up to 4096, supports user-defined output dimensions ranging from 32 to 4096\n\n\n\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to the model’s \nblog\n, \nGitHub\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/qwen3-embedding/d68d4b9c-c9a2-4665-bde1-3b29d5c510c7\" width=\"320\" />\n\n## Highlights\n\nThe Qwen3 Embedding model series is specifically designed for text embedding tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\n\n**Exceptional Versatility**: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks **No.1** in the MTEB multilingual leaderboard (as of June 5, 2025, score **70.58**).\n\n**Comprehensive Flexibility**: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for embedding models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, these models allow for flexible vector definitions across all dimensions, support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\n\n**Multilingual Capability**: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\n\n**Qwen3-Embedding-8B** has the following features:\n\n- Model Type: Text Embedding\n- Supported Languages: 100+ Languages\n- Number of Paramaters: 8B\n- Context Length: 32k\n- Embedding Dimension: Up to 4096, supports user-defined output dimensions ranging from 32 to 4096\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to the model's [blog](https://qwenlm.github.io/blog/qwen3-embedding/), [GitHub](https://github.com/QwenLM/Qwen3-Embedding).\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "0.6b",
      "4b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3-embedding:4b",
        "size": "2.5GB",
        "size_gb": 2.5,
        "recommended_ram_gb": 3.1,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3-embedding:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "40K",
        "context_window": 40000
      }
    ],
    "min_ram_gb": 2.5,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": null,
    "best_for": "For text embedding tasks in various sizes (0.6B, 4B, and 8B).",
    "pulls": 906600,
    "tags": 12,
    "last_updated": "2025-09-25",
    "last_updated_str": "5 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a4d425bd-e817-47fd-9488-a5a2015ebe6f",
    "model_identifier": "codestral",
    "model_name": "codestral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codestral",
    "description": "Codestral is Mistral AI’s first-ever code model designed for code generation tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nCodestral\n\n\n\n\nCodestral is Mistral AI’s first-ever code model designed for code generation tasks. It is a 22B model.\n\n\nFluent in 80+ programming languages\n\n\nCodestral is trained on a dataset of over 80 programming languages, including Python, Java, C, C++, JavaScript, Swift, Fortran and Bash.\n\n\nThe model can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism.\n\n\nBenchmarks\n\n\n\n\n\n\n\n\nReference\n\n\nMistral AI - Codestral: Hello, World\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Codestral\n\n![Codestral Hello World](https://ollama.com/assets/mchiang0610/test1/76bfd7f0-fccd-431b-95fe-dddf5a422dcf)\n\nCodestral is Mistral AI's first-ever code model designed for code generation tasks. It is a 22B model. \n\n### Fluent in 80+ programming languages \n\nCodestral is trained on a dataset of over 80 programming languages, including Python, Java, C, C++, JavaScript, Swift, Fortran and Bash. \n\nThe model can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. \n\n### Benchmarks \n\n![With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation](https://ollama.com/assets/mchiang0610/test1/f5ea9604-669b-4de0-957d-41245e33636c)\n\n![Benchmarks](https://ollama.com/assets/mchiang0610/test1/e42a01d3-4f36-4d60-a0c8-74ad10159cdd)\n\n![FIM benchmarks](https://ollama.com/assets/mchiang0610/test1/8ca203e2-7bfd-4081-98c8-1ecc3cc46ef8)\n\n### Reference \n\n[Mistral AI - Codestral: Hello, World](https://mistral.ai/news/codestral/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "22b"
    ],
    "memory_requirements": [
      {
        "tag": "codestral:latest",
        "size": "13GB",
        "size_gb": 13.0,
        "recommended_ram_gb": 16.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "13GB",
        "size_gb": 13.0,
        "recommended_ram_gb": 16.2,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 13.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers who need a powerful code model for various coding tasks.",
    "pulls": 830500,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c3834c09-3776-43d4-ac5f-4bd1dc44d03a",
    "model_identifier": "openthinker",
    "model_name": "openthinker",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/openthinker",
    "description": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOpenThinker is a family of fine-tuned models from Qwen2.5 on the \nOpenThoughts-114k dataset\n, surpassing DeepSeek-R1 distillation models on some benchmarks.\n\n\nThe dataset is derived by distilling DeepSeek-R1 using the data pipeline available on github. More info about the dataset can be found on the dataset card at OpenThoughts-114k dataset.\n\n\nModels\n\n\n32B\n\n\nollama run openthinker:32b\n\n\n\n\n\n\n\n\n\nModel Name\n\n\nDataset Size\n\n\nAIME24 I/II\n\n\nAIME25 I\n\n\nMATH500\n\n\nGPQA Diamond\n\n\nLCBv2\n\n\n\n\n\n\n\n\n\n\nLIMO-32B\n\n\n0.8k\n\n\n56.7\n\n\n49.3\n\n\n86.6\n\n\n58.1\n\n\n60.0\n\n\n\n\n\n\ns1-32B\n\n\n1k\n\n\n36.0\n\n\n25.3\n\n\n84.8\n\n\n50.5\n\n\n40.9\n\n\n\n\n\n\ns1.1-32B\n\n\n1k\n\n\n64.7\n\n\n49.3\n\n\n89.0\n\n\n60.1\n\n\n65.5\n\n\n\n\n\n\nDeepSeek-R1-Distill-Qwen-32B\n\n\n800k (closed)\n\n\n76.7\n\n\n55.9\n\n\n89.4\n\n\n57.6\n\n\n71.2\n\n\n\n\n\n\nOpenThinker-32B\n\n\n114k\n\n\n66.0\n\n\n53.3\n\n\n90.6\n\n\n61.6\n\n\n68.9\n\n\n\n\n\n\n\n\n7B (default)\n\n\nollama run openthinker:7b\n\n\n\n\n\n\n\n\n\n\n\nAIME24\n\n\nMATH500\n\n\nGPQA-Diamond\n\n\nLCBv2 Easy\n\n\nLCBv2 Medium\n\n\nLCBv2 Hard\n\n\nLCBv2 All\n\n\n\n\n\n\n\n\n\n\nOpenThinker-7B\n\n\n31.3\n\n\n83.0\n\n\n42.4\n\n\n75.3\n\n\n28.6\n\n\n6.5\n\n\n39.9\n\n\n\n\n\n\nBespoke-Stratos-7B\n\n\n22.7\n\n\n79.6\n\n\n38.9\n\n\n71.4\n\n\n25.2\n\n\n0.8\n\n\n35.8\n\n\n\n\n\n\nDeepSeek-R1-Distill-Qwen-7B\n\n\n60\n\n\n88.2\n\n\n46.9\n\n\n79.7\n\n\n45.1\n\n\n14.6\n\n\n50.1\n\n\n\n\n\n\ngpt-4o-0513\n\n\n8.7\n\n\n75.8\n\n\n46.5\n\n\n87.4\n\n\n42.7\n\n\n8.9\n\n\n50.5\n\n\n\n\n\n\no1-mini\n\n\n64\n\n\n85.6\n\n\n60\n\n\n92.8\n\n\n74.7\n\n\n39.8\n\n\n72.8\n\n\n\n\n\n\n\n\nReferences\n\n\nGitHub\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/openthinker/0ef3a0d3-aae1-4855-b56b-50875e9683e8\" width=\"320\" />\n\nOpenThinker is a family of fine-tuned models from Qwen2.5 on the [OpenThoughts-114k dataset](https://huggingface.co/datasets/open-thoughts/open-thoughts-114k), surpassing DeepSeek-R1 distillation models on some benchmarks.\n\nThe dataset is derived by distilling DeepSeek-R1 using the data pipeline available on github. More info about the dataset can be found on the dataset card at OpenThoughts-114k dataset.\n\n### Models\n\n#### 32B\n\n```\nollama run openthinker:32b\n```\n\n|Model Name|Dataset Size|AIME24 I/II|AIME25 I|MATH500|GPQA Diamond|LCBv2|\n|---|---|---|---|---|---|---|\n|LIMO-32B|0.8k|56.7|49.3|86.6|58.1|60.0|\n|s1-32B|1k|36.0|25.3|84.8|50.5|40.9|\n|s1.1-32B|1k|64.7|49.3|89.0|60.1|65.5|\n|DeepSeek-R1-Distill-Qwen-32B|800k (closed)|**76.7**|**55.9**|89.4|57.6|**71.2**|\n|**OpenThinker-32B**|114k|66.0|53.3|**90.6**|**61.6**|68.9|\n\n#### 7B (default)\n\n```\nollama run openthinker:7b\n```\n\n|                             | AIME24   | MATH500 | GPQA-Diamond | LCBv2 Easy  | LCBv2 Medium  | LCBv2 Hard  | LCBv2 All  |\n| --------------------------- | -------- | ------- | ------------ | ----------- | ------------- | ----------- | ---------- |\n| OpenThinker-7B              | 31.3     | 83.0    | 42.4         | 75.3        | 28.6          | 6.5         | 39.9       |\n| Bespoke-Stratos-7B          | 22.7     | 79.6    | 38.9         | 71.4        | 25.2          | 0.8         | 35.8       |\n| DeepSeek-R1-Distill-Qwen-7B | 60       | 88.2    | 46.9         | 79.7        | 45.1          | 14.6        | 50.1       |\n| gpt-4o-0513                 | 8.7       | 75.8    | 46.5         | 87.4        | 42.7          | 8.9         | 50.5       |\n| o1-mini                     | 64       | 85.6    | 60           | 92.8        | 74.7          | 39.8        | 72.8       |\n\n## References\n\n[GitHub](https://github.com/open-thoughts/open-thoughts)\n\n[Blog post](https://www.open-thoughts.ai/blog/launch)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "openthinker:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openthinker:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Users requiring a fine-tuned model for reasoning tasks.",
    "pulls": 794900,
    "tags": 15,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "102cbb3c-fd2d-4b9e-aa41-3e2ce0bc9809",
    "model_identifier": "granite3.2-vision",
    "model_name": "granite3.2-vision",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3.2-vision",
    "description": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.5.13\n.\n\n\n\n\nA compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. The model was trained on a meticulously curated instruction-following dataset, comprising diverse public datasets and synthetic datasets tailored to support a wide range of document understanding and general image tasks. It was trained by fine-tuning a Granite large language model with both image and text modalities.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires [Ollama 0.5.13](https://github.com/ollama/ollama/releases/tag/v0.5.13).\n\nA compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. The model was trained on a meticulously curated instruction-following dataset, comprising diverse public datasets and synthetic datasets tailored to support a wide range of document understanding and general image tasks. It was trained by fine-tuning a Granite large language model with both image and text modalities.\n\n## References\n\n[Hugging Face](https://huggingface.co/ibm-granite/granite-vision-3.2-2b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools"
    ],
    "capability": "Vision",
    "labels": [
      "2b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3.2-vision:latest",
        "size": "2.4GB",
        "size_gb": 2.4,
        "recommended_ram_gb": 3.0,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "2.4GB",
        "size_gb": 2.4,
        "recommended_ram_gb": 3.0,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 2.4,
    "use_cases": [
      "Image Understanding",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Developers working with visual documents.",
    "pulls": 763000,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "12 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ac1a42a7-74e2-4355-977c-a0bcb270dfaf",
    "model_identifier": "devstral",
    "model_name": "devstral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/devstral",
    "description": "Devstral: the best open source model for coding agents",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDevstral\n is an agentic LLM for software engineering tasks built under a collaboration between \nMistral AI\n and \nAll Hands AI\n 🙌. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model.\n\n\nIt is finetuned from Mistral Small 3.1, therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from \nMistral-Small-3.1\n the vision encoder was removed.\n\n\n\n\nKey Features:\n\n\n\n\nAgentic coding\n: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n\n\nLightweight\n: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\n\n\nApache 2.0 License\n: Open license allowing usage and modification for both commercial and non-commercial purposes.\n\n\nContext Window\n: A 128k context window.\n\n\n\n\nSWE-Bench\n\n\nDevstral achieves a score of \n46.8% on SWE-Bench Verified\n, outperforming prior open-source state-of-the-art by 6%.\n\n\n\n\n\n\n\n\nModel\n\n\nScaffold\n\n\nSWE-Bench Verified (%)\n\n\n\n\n\n\n\n\n\n\nDevstral\n\n\nOpenHands Scaffold\n\n\n46.8\n\n\n\n\n\n\nGPT-4.1-mini\n\n\nOpenAI Scaffold\n\n\n23.6\n\n\n\n\n\n\nClaude 3.5 Haiku\n\n\nAnthropic Scaffold\n\n\n40.6\n\n\n\n\n\n\nSWE-smith-LM 32B\n\n\nSWE-agent Scaffold\n\n\n40.2\n\n\n\n\n\n\n\n\nWhen evaluated under the same test scaffold (OpenHands, provided by All Hands AI 🙌), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\n\n\nIn the chart below, we also compare Devstral to closed and open models evaluated under any scaffold (including ones custom for the model). Here, we find that Devstral achieves substantially better performance than a number of closed-source alternatives. For example, Devstral surpasses the recent GPT-4.1-mini by over 20%.\n\n\n\n\nReference\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/mistral-small3.1/88f81c26-7028-4f08-b906-92b873d5536e\" width=\"120\" />\n\n**Devstral** is an agentic LLM for software engineering tasks built under a collaboration between [Mistral AI](https://mistral.ai/) and [All Hands AI](https://www.all-hands.dev/) 🙌. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model. \n\nIt is finetuned from Mistral Small 3.1, therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from `Mistral-Small-3.1` the vision encoder was removed.\n\n![agentic performance](/assets/library/devstral/a1ff4c01-fedd-4ce1-ad70-2dacb674add1)\n\n### Key Features:\n\n- **Agentic coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n- **Lightweight**: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\n- **Apache 2.0 License**: Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window**: A 128k context window.\n\n### SWE-Bench\n\nDevstral achieves a score of **46.8% on SWE-Bench Verified**, outperforming prior open-source state-of-the-art by 6%.\n\n| Model            | Scaffold           | SWE-Bench Verified (%) |\n|------------------|--------------------|------------------------|\n| Devstral         | OpenHands Scaffold | **46.8**               |\n| GPT-4.1-mini     | OpenAI Scaffold    | 23.6                   |\n| Claude 3.5 Haiku | Anthropic Scaffold | 40.6                   |\n| SWE-smith-LM 32B | SWE-agent Scaffold | 40.2                   |\n\nWhen evaluated under the same test scaffold (OpenHands, provided by All Hands AI 🙌), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\n\nIn the chart below, we also compare Devstral to closed and open models evaluated under any scaffold (including ones custom for the model). Here, we find that Devstral achieves substantially better performance than a number of closed-source alternatives. For example, Devstral surpasses the recent GPT-4.1-mini by over 20%. \n\n![Devstral comparison](/assets/library/devstral/c78b6b7a-52d5-4b1f-9657-abdcb8369ef5)\n\n### Reference\n[Blog](https://mistral.ai/news/devstral)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "devstral:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 14.0,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Software Engineering Agents and Coding Tasks",
    "pulls": 757300,
    "tags": 5,
    "last_updated": "2025-07-25",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ad7197d2-7bce-45ff-b52d-46a6731209b0",
    "model_identifier": "dolphin-mistral",
    "model_name": "dolphin-mistral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphin-mistral",
    "description": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Dolphin model by Eric Hartford, based on \nMistral\n version 0.2 released in March 2024. This model is uncensored, available for both commercial and non-commercial use, and excels at coding.\n\n\nVersions\n\n\n\n\n\n\n\n\nTag\n\n\nDate\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nv2.8\n \nlatest\n\n\n03/31/2024\n\n\nBased on Mistral 0.2 with support for a context window of 32K tokens.\n\n\n\n\n\n\nv2.6\n\n\n12/27/2023\n\n\nFixed a training configuration issue that improved quality, and improvements to the training dataset for empathy.\n\n\n\n\n\n\nv2.2.1\n\n\n10/30/2023\n\n\nThis is a checkpoint release, to fix overfit training\n\n\n\n\n\n\nv2.2\n\n\n10/29/2023\n\n\nAdded conversation and empathy data.\n\n\n\n\n\n\nv2.1\n\n\n10/11/2023\n\n\nEnhanced with the airoboros dataset.\n\n\n\n\n\n\nv2.0\n\n\n10/2/2023\n\n\nInitial release of the model.\n\n\n\n\n\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/c65aad4e-7a58-4645-97f2-5e8ca158275f\" width=\"280\" />\n\nThe Dolphin model by Eric Hartford, based on [Mistral](https://ollama.ai/library/mistral) version 0.2 released in March 2024. This model is uncensored, available for both commercial and non-commercial use, and excels at coding.\n\n## Versions\n| Tag             | Date       | Notes                                                                                                             |\n| --------------- | ---------- | ----------------------------------------------------------------------------------------------------------------- |\n| `v2.8` `latest` | 03/31/2024 | Based on Mistral 0.2 with support for a context window of 32K tokens.                                             |\n| `v2.6`          | 12/27/2023 | Fixed a training configuration issue that improved quality, and improvements to the training dataset for empathy. |\n| `v2.2.1`        | 10/30/2023 | This is a checkpoint release, to fix overfit training                                                             |\n| `v2.2`          | 10/29/2023 | Added conversation and empathy data.                                                                              |\n| `v2.1`          | 10/11/2023 | Enhanced with the airoboros dataset.                                                                              |\n| `v2.0`          | 10/2/2023  | Initial release of the model.                                                                                     |\n\n\n\n\n\n## References\n\n[HuggingFace](https://huggingface.co/cognitivecomputations)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphin-mistral:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers focusing on coding tasks.",
    "pulls": 742800,
    "tags": 120,
    "last_updated": "2026-02-25",
    "last_updated_str": "to version 2.8.",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "2fc49588-0fe9-4e26-8e40-e7c1491382a2",
    "model_identifier": "granite4",
    "model_name": "granite4",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite4",
    "description": "Granite 4 features improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGranite 4.0 models\n\n\nGranite 4.0 models\n are finetuned from their base models using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. They feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.\n\n\nPlease Note:\n the 3b, 1b, and 350m model sizes are alternative options for users when mamba-2 support is not yet optimized. Models denoted -h use the hybrid mamba-2 architecture.\n\n\nParameter Sizes\n\n\n350m\n\n\nollama run granite4:350m\n\n\n\n350m-h\n\n\nollama run granite4:350m-h\n\n\n\n1b\n\n\nollama run granite4:1b\n\n\n\n1b-h\n\n\nollama run granite4:1b-h\n\n\n\n3b (micro)\n\n\nollama run granite4:3b\nollama run granite4:micro\n\n\n\n3b-h (micro-h)\n\n\nollama run granite4:3b-h\nollama run granite4:micro-h\n\n\n\n7b-a1b-h (tiny-h)\n\n\nollama run granite4:7b-a1b-h\nollama run granite4:tiny-h\n\n\n\n32b-a9b-h (small-h)\n\n\nollama run granite4:32b-a9b-h\nollama run granite4:small-h\n\n\n\nSupported Languages\n\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages.\n\n\nIntended Use\n\n\nThis model is designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\n\n\nCapabilities\n\n\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related tasks\n\n\nFunction-calling tasks\n\n\nMultilingual dialog use cases\n\n\nFill-In-the-Middle (FIM) code completions\n\n\n\n\nLearn more\n\n\n\n\nDevelopers: Granite Team, IBM\n\n\nWebsite: \nGranite Docs\n\n\nGitHub Repository: \nibm-granite/granite-4.0-language-models\n\n\nRelease Date: October 2nd, 2025\n\n\nLicense: \nApache 2.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<center><img src=\"https://ollama.com/assets/library/granite3.2/90c5e567-0004-425c-a17a-1b846c2b5d3d\" data-canonical-src=\"https://gyazo.com/eb5c5741b6a9a16c692170a41a49c858.png\" width=\"600\" /></center>\n\n### Granite 4.0 models\n\n**Granite 4.0 models** are finetuned from their base models using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. They feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.\n\n**Please Note:** the 3b, 1b, and 350m model sizes are alternative options for users when mamba-2 support is not yet optimized. Models denoted -h use the hybrid mamba-2 architecture.\n\n#### Parameter Sizes\n\n**350m**\n```\nollama run granite4:350m\n```\n**350m-h**\n```\nollama run granite4:350m-h\n```\n**1b**\n```\nollama run granite4:1b\n```\n**1b-h**\n```\nollama run granite4:1b-h\n```\n**3b (micro)**\n```\nollama run granite4:3b\nollama run granite4:micro\n```\n**3b-h (micro-h)**\n```\nollama run granite4:3b-h\nollama run granite4:micro-h\n```\n\n**7b-a1b-h (tiny-h)**\n\n```\nollama run granite4:7b-a1b-h\nollama run granite4:tiny-h\n```\n\n**32b-a9b-h (small-h)**\n```\nollama run granite4:32b-a9b-h\nollama run granite4:small-h\n```\n\n#### Supported Languages\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages.\n\n#### Intended Use\n\nThis model is designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\n\n#### Capabilities\n\n- Summarization\n- Text classification\n- Text extraction\n- Question-answering\n- Retrieval Augmented Generation (RAG)\n- Code related tasks\n- Function-calling tasks\n- Multilingual dialog use cases\n- Fill-In-the-Middle (FIM) code completions\n\n#### Learn more\n\n- Developers: Granite Team, IBM\n- Website: [Granite Docs](https://www.ibm.com/granite/docs)\n- GitHub Repository: [ibm-granite/granite-4.0-language-models](https://github.com/ibm-granite/granite-4.0-language-models)\n- Release Date: October 2nd, 2025\n- License: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "350m",
      "1b",
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "granite4:latest",
        "size": "2.1GB",
        "size_gb": 2.1,
        "recommended_ram_gb": 2.6,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "granite4:3b",
        "size": "2.1GB",
        "size_gb": 2.1,
        "recommended_ram_gb": 2.6,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "granite4:1b",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 2.1,
    "use_cases": [
      "Function Calling",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Enterprise applications requiring improved instruction following and tool-calling capabilities.",
    "pulls": 729500,
    "tags": 17,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ee669aad-fc5d-423b-94cc-b1a86ffab45c",
    "model_identifier": "command-r",
    "model_name": "command-r",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/command-r",
    "description": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCommand R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG) and using external APIs and tools. As a model built for companies to implement at scale, Command R boasts:\n\n\n\n\nStrong accuracy on RAG and Tool Use\n\n\nLow latency, and high throughput\n\n\nLonger 128k context\n\n\nStrong capabilities across 10 key languages\n\n\n\n\nThere are currently two versions of Command R:\n\n\n\n\nOriginal release tagged \nv0.1\n\n\nAugust 2024 update tagged \n08-2024\n\n\n\n\nReferences\n\n\n\n\nBlog Post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/0f8ef1fc-f0ba-4e69-b171-36c387aa5349\" width=\"480\" />\n\nCommand R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG) and using external APIs and tools. As a model built for companies to implement at scale, Command R boasts: \n\n- Strong accuracy on RAG and Tool Use\n- Low latency, and high throughput\n- Longer 128k context\n- Strong capabilities across 10 key languages\n\nThere are currently two versions of Command R:\n\n- Original release tagged **v0.1**\n- August 2024 update tagged **08-2024**\n\n## References\n\n- [Blog Post](https://txt.cohere.com/command-r/)\n- [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r-v01)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "35b"
    ],
    "memory_requirements": [
      {
        "tag": "command-r:latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 19.0,
    "use_cases": [
      "Function Calling",
      "RAG / Retrieval"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": null,
    "best_for": "Developers and researchers for long context tasks and tool use.",
    "pulls": 710900,
    "tags": 32,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "cea19185-b3ab-42bd-aff9-45743633d401",
    "model_identifier": "granite-code",
    "model_name": "granite-code",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite-code",
    "description": "A family of open foundation models by IBM for Code Intelligence",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGranite Code is a family of decoder-only code model designed for code generative tasks (e.g. code generation, code explanation, code fixing, etc.).\n\n\nParameter Sizes\n\n\n\n\n34B Parameters \nollama run granite-code:34b\n\n\n20B Parameters \nollama run granite-code:20b\n\n\n8B Parameters (with 128K context window) \nollama run granite-code:8b\n\n\n3B Parameters (with 128K context window) \nollama run granite-code:3b\n\n\n\n\nResources\n\n\n\n\nHugging Face\n\n\nPaper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](https://ollama.com/assets/library/granite-code/e28e37cc-af1b-47d7-bd85-6154b0cf26be)\n\nGranite Code is a family of decoder-only code model designed for code generative tasks (e.g. code generation, code explanation, code fixing, etc.). \n\n## Parameter Sizes\n\n* 34B Parameters `ollama run granite-code:34b`\n* 20B Parameters `ollama run granite-code:20b`\n* 8B Parameters (with 128K context window) `ollama run granite-code:8b`\n* 3B Parameters (with 128K context window) `ollama run granite-code:3b`\n\n\n## Resources\n\n* [Hugging Face](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)\n* [Paper](https://arxiv.org/abs/2405.04324)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3b",
      "8b",
      "20b",
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "granite-code:latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "125K context",
        "context_window": 125000
      },
      {
        "tag": "latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "125K",
        "context_window": 125000
      },
      {
        "tag": "granite-code:8b",
        "size": "4.6GB",
        "size_gb": 4.6,
        "recommended_ram_gb": 5.8,
        "quantization": "q4_k_m",
        "context": "125K context",
        "context_window": 125000
      },
      {
        "tag": "granite-code:20b",
        "size": "12GB",
        "size_gb": 12.0,
        "recommended_ram_gb": 15.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "granite-code:34b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 2.0,
    "use_cases": [
      "Code Generation"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working on code-related tasks.",
    "pulls": 685700,
    "tags": 162,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a394d7bd-f782-4c05-9018-6c8c966a31b8",
    "model_identifier": "qwen3-coder-next",
    "model_name": "qwen3-coder-next",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3-coder-next",
    "description": "Qwen3-Coder-Next is a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nBuilt on top of Qwen3-Next-80B-A3B-Base, which adopts a novel architecture with hybrid attention and MoE, Qwen3-Coder-Next has been agentically trained at scale on large-scale executable task synthesis, environment interaction, and reinforcement learning, obtaining strong coding and agentic capabilities with significantly lower inference costs.\n\n\n\n\nFeatures\n\n\n\n\nUltra-efficient inference:\n 80B total parameters, 3B active per token. Runs on consumer hardware with quantization.\n\n\n256K native context:\n Full repository-scale understanding without chunking or retrieval hacks.\n\n\nAgentic training:\n Trained on 800K executable tasks with environment interaction and reinforcement learning—not just static code-text pairs.\n\n\nTool calling:\n Works with coding agents like Claude Code, Qwen Code, Cline, and OpenCode out of the box.\n\n\nNon-thinking mode only:\n Fast responses without \n<think></think>\n blocks.\n\n\n\n\nBenchmarks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/qwen3-coder-next/dbbd2b20-4b43-4d02-9cfe-e83a63cabae8\" width=\"360\" />\n\nBuilt on top of Qwen3-Next-80B-A3B-Base, which adopts a novel architecture with hybrid attention and MoE, Qwen3-Coder-Next has been agentically trained at scale on large-scale executable task synthesis, environment interaction, and reinforcement learning, obtaining strong coding and agentic capabilities with significantly lower inference costs.\n\n![image.png](/assets/library/qwen3-coder-next/7d9b5395-c335-48b9-b8c5-7e5090abbb8e)\n\n### Features\n\n* **Ultra-efficient inference:** 80B total parameters, 3B active per token. Runs on consumer hardware with quantization.\n* **256K native context:** Full repository-scale understanding without chunking or retrieval hacks.\n* **Agentic training:** Trained on 800K executable tasks with environment interaction and reinforcement learning—not just static code-text pairs.\n* **Tool calling:** Works with coding agents like Claude Code, Qwen Code, Cline, and OpenCode out of the box.\n* **Non-thinking mode only:** Fast responses without `<think></think>` blocks.\n\n### Benchmarks\n\n![](/assets/library/qwen3-coder-next/d978a788-2fa4-44ea-832c-20cd82202a6c)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "cloud"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3-coder-next:latest",
        "size": "52GB",
        "size_gb": 52.0,
        "recommended_ram_gb": 65.0,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3-coder-next:q4_K_M",
        "size": "52GB",
        "size_gb": 52.0,
        "recommended_ram_gb": 65.0,
        "quantization": "q4_k_m",
        "context": "256K",
        "context_window": 256000
      },
      {
        "tag": "qwen3-coder-next:q8_0",
        "size": "85GB",
        "size_gb": 85.0,
        "recommended_ram_gb": 106.2,
        "quantization": "q8_0",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 52.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers focusing on coding tasks.",
    "pulls": 670300,
    "tags": 4,
    "last_updated": "2026-02-11",
    "last_updated_str": "2 weeks ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "dc695784-4306-451d-99a9-b9c5cd5c473f",
    "model_identifier": "wizardlm2",
    "model_name": "wizardlm2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizardlm2",
    "description": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nWizardLM-2 is a next generation state-of-the-art large language model with improved performance on complex chat, multilingual, reasoning and agent use cases. This family includes three cutting-edge models:\n\n\n\n\nwizardlm2:7b\n: fastest model, comparable performance with 10x larger open-source models.\n\n\nwizardlm2:8x22b\n: the most advanced model, and the best opensource LLM in Microsoft’s internal evaluation on highly complex tasks.\n\n\nwizardlm2:70b\n: model with top-tier reasoning capabilities for its size (coming soon)\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWizardLM-2 is a next generation state-of-the-art large language model with improved performance on complex chat, multilingual, reasoning and agent use cases. This family includes three cutting-edge models:\n\n* `wizardlm2:7b`: fastest model, comparable performance with 10x larger open-source models.\n* `wizardlm2:8x22b`: the most advanced model, and the best opensource LLM in Microsoft's internal evaluation on highly complex tasks.\n* `wizardlm2:70b`: model with top-tier reasoning capabilities for its size (coming soon)\n\n\n## References\n\n[Blog Post](https://wizardlm.github.io/WizardLM2/)\n\n[HuggingFace](https://huggingface.co/collections/microsoft/wizardlm-661d403f71e6c8257dbd598a)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "8x22b"
    ],
    "memory_requirements": [
      {
        "tag": "wizardlm2:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "wizardlm2:8x22b",
        "size": "80GB",
        "size_gb": 80.0,
        "recommended_ram_gb": 100.0,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese",
      "Arabic",
      "Japanese"
    ],
    "complexity": "advanced",
    "best_for": "Complex text generation and understanding tasks requiring advanced reasoning.",
    "pulls": 664900,
    "tags": 22,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "68002dc0-ae6a-469e-a4b5-7731a532f370",
    "model_identifier": "deepcoder",
    "model_name": "deepcoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepcoder",
    "description": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThrough a joint collaboration between the Agentica team and Together AI, DeepCoder-14B-Preview is a code reasoning model finetuned from Deepseek-R1-Distilled-Qwen-14B via distributed RL. It achieves an impressive 60.6% Pass@1 accuracy on LiveCodeBench (+8% improvement), matching the performance of \no3-mini-2025-01-031\n (Low) and \no1-2024-12-17\n with just 14B parameters\n\n\nEvaluation\n\n\n\n\n\n\n\n\nModel\n\n\nLCB (v5)(8/1/24-2/1/25)\n\n\nCodeforces Rating\n\n\nCodeforces Percentile\n\n\nHumanEval+\n\n\n\n\n\n\n\n\n\n\nDeepCoder-14B-Preview\n\n\n60.6\n\n\n1936\n\n\n95.3\n\n\n92.6\n\n\n\n\n\n\nDeepSeek-R1-Distill-Qwen-14B\n\n\n53.0\n\n\n1791\n\n\n92.7\n\n\n92.0\n\n\n\n\n\n\nO1-2024-12-17 (Low)\n\n\n59.5\n\n\n1991\n\n\n96.1\n\n\n90.8\n\n\n\n\n\n\nO3-Mini-2025-1-31 (Low)\n\n\n60.9\n\n\n1918\n\n\n94.9\n\n\n92.6\n\n\n\n\n\n\nO1-Preview\n\n\n42.7\n\n\n1658\n\n\n88.5\n\n\n89\n\n\n\n\n\n\nDeepseek-R1\n\n\n62.8\n\n\n1948\n\n\n95.4\n\n\n92.6\n\n\n\n\n\n\nLlama-4-Behemoth\n\n\n49.4\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\n\n\nReferences\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/deepcoder/7e57e9ba-4b82-4a7a-82f6-3ce9d4cba1c8\" width=\"100\" />\n\nThrough a joint collaboration between the Agentica team and Together AI, DeepCoder-14B-Preview is a code reasoning model finetuned from Deepseek-R1-Distilled-Qwen-14B via distributed RL. It achieves an impressive 60.6% Pass@1 accuracy on LiveCodeBench (+8% improvement), matching the performance of `o3-mini-2025-01-031` (Low) and `o1-2024-12-17` with just 14B parameters\n\n## Evaluation\n\n| **Model** | LCB (v5)(8/1/24-2/1/25) | Codeforces Rating | Codeforces Percentile | HumanEval+ |\n| --- | --- | --- | --- | --- |\n| **DeepCoder-14B-Preview** | ***60.6*** | ***1936*** | ***95.3*** | ***92.6*** |\n| **DeepSeek-R1-Distill-Qwen-14B** | 53.0 | 1791 | 92.7 | 92.0 |\n| **O1-2024-12-17 (Low)** | 59.5 | **1991** | **96.1** | 90.8 |\n| **O3-Mini-2025-1-31 (Low)** | **60.9** | 1918 | 94.9 | 92.6 |\n| **O1-Preview** | 42.7 | 1658 | 88.5 | 89 |\n| **Deepseek-R1** | 62.8 | 1948 | 95.4 | 92.6 |\n| **Llama-4-Behemoth** | 49.4 | - | - | - |\n\n## References\n\n[Blog post](https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.5b",
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "deepcoder:1.5b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepcoder:latest",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.1,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on code generation tasks.",
    "pulls": 648600,
    "tags": 9,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "3fe9dfce-7fba-42fb-880e-2eed5067ff82",
    "model_identifier": "moondream",
    "model_name": "moondream",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/moondream",
    "description": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n🌔 moondream 2\n\n\n\n\nMoondream 2 requires Ollama 0.1.33 or later\n\n\n\n\n“a tiny vision language model that kicks ass and runs anywhere”\n\n\nLimitations\n\n\n\n\nThe model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions.\n\n\nThe model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model.\n\n\nThe model may generate offensive, inappropriate, or hurtful content if it is prompted to do so.\n\n\n\n\nReferences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 🌔 moondream 2\n\n> Moondream 2 requires Ollama 0.1.33 or later\n\n_\"a tiny vision language model that kicks ass and runs anywhere\"_\n\n### Limitations\n\n* The model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions.\n\n* The model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model.\n\n* The model may generate offensive, inappropriate, or hurtful content if it is prompted to do so.\n\n## References\n\n[GitHub](https://github.com/vikhyat/moondream)\n\n[Hugging Face](https://huggingface.co/vikhyatk/moondream2)\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "1.8b"
    ],
    "memory_requirements": [
      {
        "tag": "moondream:latest",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 1.7,
    "use_cases": [
      "Image Understanding",
      "Text Summarization"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Edge device deployment for vision tasks.",
    "pulls": 645600,
    "tags": 18,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "dfb94922-287e-4a79-bc3b-9023406aa425",
    "model_identifier": "hermes3",
    "model_name": "hermes3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/hermes3",
    "description": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nHermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\n\nThe ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\n\nHermes 3\n series contains four models: \n3B\n, \n8B\n, \n70B\n, and \n405B\n\n\nReferences:\n\n\n\n\nHermes 3 Technical Report\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](https://ollama.com/assets/library/hermes3/e8584a99-0cf4-4d0a-9032-3af2fd7db91d)\n\nHermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nThe ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\n**Hermes 3** series contains four models: **3B**, **8B**, **70B**, and **405B**\n\n## References:\n* [Hermes 3 Technical Report](https://arxiv.org/abs/2408.11857)\n* [Hugging Face](https://huggingface.co/collections/NousResearch/hermes-3-66bd6c01399b14b08fe335ea)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "3b",
      "8b",
      "70b",
      "405b"
    ],
    "memory_requirements": [
      {
        "tag": "hermes3:3b",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "hermes3:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "hermes3:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "hermes3:405b",
        "size": "229GB",
        "size_gb": 229.0,
        "recommended_ram_gb": 286.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 2.0,
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Function Calling",
      "Reasoning",
      "Text Summarization"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 627300,
    "tags": 65,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "7c74c049-3ae9-4c04-b429-baa2c955a1dd",
    "model_identifier": "mistral-small3.1",
    "model_name": "mistral-small3.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-small3.1",
    "description": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.6.5 or higher. \nDownload Ollama\n\n\n\n\n\n\nBuilding on \nMistral Small 3\n, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.\n\n\nMistral Small 3.1 is released under an Apache 2.0 license.\n\n\nKey features and capabilities\n\n\n\n\nLightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.\n\n\nFast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential.\n\n\nLow-latency function calling: Capable of rapid function execution within automated or agentic workflows\n\n\nFine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.\n\n\nFoundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.\n\n\n\n\nReferences\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires Ollama 0.6.5 or higher. [Download Ollama](https://ollama.com/download)\n\n<img src=\"/assets/library/mistral-small3.1/88f81c26-7028-4f08-b906-92b873d5536e\" width=\"120\" />\n\n\nBuilding on [Mistral Small 3](https://ollama.com/library/mistral-small), this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.\n\nMistral Small 3.1 is released under an Apache 2.0 license.\n\n## Key features and capabilities\n\n* Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.\n\n* Fast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential. \n\n* Low-latency function calling: Capable of rapid function execution within automated or agentic workflows\n\n* Fine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.\n\n* Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.\n\n## References\n\n[Blog post](https://mistral.ai/news/mistral-small-3-1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools"
    ],
    "capability": "Vision",
    "labels": [
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-small3.1:latest",
        "size": "15GB",
        "size_gb": 15.0,
        "recommended_ram_gb": 18.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "15GB",
        "size_gb": 15.0,
        "recommended_ram_gb": 18.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 15.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Image Understanding",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Ideal for virtual assistants and other applications where quick, accurate responses are needed.",
    "pulls": 606600,
    "tags": 5,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "35dda90e-257c-4ecd-9bcf-74d579712700",
    "model_identifier": "lfm2.5-thinking",
    "model_name": "lfm2.5-thinking",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/lfm2.5-thinking",
    "description": "LFM2.5 is a new family of hybrid models designed for on-device deployment.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLFM2.5 is a new family of hybrid models designed for on-device deployment. It builds on the LFM2 architecture with extended pre-training and reinforcement learning.\n\n\nBest-in-class performance: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket.\n\n\nLFM2.5-1.2B-Thinking is a general-purpose text-only model with the following features:\n\n\n\n\nNumber of parameters: 1.17B\n\n\nNumber of layers: 16 (10 double-gated LIV convolution blocks + 6 GQA blocks)\n\n\nTraining budget: 28T tokens\n\n\nContext length: 32,768 tokens\n\n\nVocabulary size: 65,536\n\n\nLanguages: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish\n\n\n\n\nBenchmarks\n\n\nWe compared LFM2.5-1.2B-Thinking with relevant sub-2B models on a diverse suite of benchmarks.\n\n\n\n\n\n\n\n\nModel\n\n\nGPQA Diamond\n\n\nMMLU-Pro\n\n\nIFEval\n\n\nIFBench\n\n\nMulti-IF\n\n\nGSM8K\n\n\nMATH-500\n\n\nAIME25\n\n\nBFCLv3\n\n\n\n\n\n\n\n\n\n\nLFM2.5-1.2B-Thinking\n\n\n37.86\n\n\n49.65\n\n\n88.42\n\n\n44.85\n\n\n69.33\n\n\n85.60\n\n\n87.96\n\n\n31.73\n\n\n56.97\n\n\n\n\n\n\nQwen3-1.7B (thinking)\n\n\n36.93\n\n\n56.68\n\n\n71.65\n\n\n25.88\n\n\n60.33\n\n\n85.60\n\n\n81.92\n\n\n36.27\n\n\n55.41\n\n\n\n\n\n\nLFM2.5-1.2B-Instruct\n\n\n38.89\n\n\n44.35\n\n\n86.23\n\n\n47.33\n\n\n60.98\n\n\n64.52\n\n\n63.20\n\n\n14.00\n\n\n49.12\n\n\n\n\n\n\nQwen3-1.7B (instruct)\n\n\n34.85\n\n\n42.91\n\n\n73.68\n\n\n21.33\n\n\n56.48\n\n\n33.66\n\n\n70.40\n\n\n9.33\n\n\n46.30\n\n\n\n\n\n\nGranite-4.0-H-1B\n\n\n24.34\n\n\n27.64\n\n\n80.08\n\n\n24.93\n\n\n47.56\n\n\n69.60\n\n\n47.20\n\n\n1\n\n\n50.69\n\n\n\n\n\n\nGemma 3 1B IT\n\n\n24.24\n\n\n14.04\n\n\n63.25\n\n\n20.47\n\n\n44.31\n\n\n42.15\n\n\n45.20\n\n\n1\n\n\n16.64\n\n\n\n\n\n\nLlama 3.2 1B Instruct\n\n\n16.57\n\n\n20.80\n\n\n52.37\n\n\n15.93\n\n\n30.16\n\n\n39.04\n\n\n23.40\n\n\n0.33\n\n\n21.44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/lfm2.5-thinking/0f3b101a-f2ef-4a5c-a0bb-a8edecac35e3)\n\nLFM2.5 is a new family of hybrid models designed for on-device deployment. It builds on the LFM2 architecture with extended pre-training and reinforcement learning. \n\nBest-in-class performance: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket. \n\nLFM2.5-1.2B-Thinking is a general-purpose text-only model with the following features:\n\n- Number of parameters: 1.17B\n- Number of layers: 16 (10 double-gated LIV convolution blocks + 6 GQA blocks)\n- Training budget: 28T tokens\n- Context length: 32,768 tokens\n- Vocabulary size: 65,536\n- Languages: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish\n\n### Benchmarks\n\nWe compared LFM2.5-1.2B-Thinking with relevant sub-2B models on a diverse suite of benchmarks.\n\n| Model | GPQA Diamond | MMLU-Pro | IFEval | IFBench | Multi-IF | GSM8K | MATH-500 | AIME25 | BFCLv3 |\n|-------|--------------|----------|--------|---------|----------|-------|----------|--------|--------|\n| **LFM2.5-1.2B-Thinking** | 37.86 | 49.65 | 88.42 | 44.85 | 69.33 | 85.60 | 87.96 | 31.73 | 56.97 |\n| Qwen3-1.7B (thinking) | 36.93 | 56.68 | 71.65 | 25.88 | 60.33 | 85.60 | 81.92 | 36.27 | 55.41 |\n| LFM2.5-1.2B-Instruct | 38.89 | 44.35 | 86.23 | 47.33 | 60.98 | 64.52 | 63.20 | 14.00 | 49.12 |\n| Qwen3-1.7B (instruct) | 34.85 | 42.91 | 73.68 | 21.33 | 56.48 | 33.66 | 70.40 | 9.33 | 46.30 |\n| Granite-4.0-H-1B | 24.34 | 27.64 | 80.08 | 24.93 | 47.56 | 69.60 | 47.20 | 1 | 50.69 |\n| Gemma 3 1B IT | 24.24 | 14.04 | 63.25 | 20.47 | 44.31 | 42.15 | 45.20 | 1 | 16.64 |\n| Llama 3.2 1B Instruct | 16.57 | 20.80 | 52.37 | 15.93 | 30.16 | 39.04 | 23.40 | 0.33 | 21.44 |\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "1.2b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Arabic",
      "Chinese",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text-related tasks on-device.",
    "pulls": 594200,
    "tags": 5,
    "last_updated": "2026-01-25",
    "last_updated_str": "1 month ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d67b44e9-3fe4-4c76-908c-bc1d6ea0a659",
    "model_identifier": "yi",
    "model_name": "yi",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/yi",
    "description": "Yi 1.5 is a high-performing, bilingual language model.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nYi is a series of large language models trained on a high-quality corpus of 3 trillion tokens that support both the English and Chinese languages.\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/ac83ff77-b8aa-4dae-a619-d9603ef2edaa\" alt=\"yi\" width=\"200\" />\n\nYi is a series of large language models trained on a high-quality corpus of 3 trillion tokens that support both the English and Chinese languages.\n\n## References\n\n[HuggingFace](https://huggingface.co/01-ai/Yi-34B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "6b",
      "9b",
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "yi:latest",
        "size": "3.5GB",
        "size_gb": 3.5,
        "recommended_ram_gb": 4.4,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.5GB",
        "size_gb": 3.5,
        "recommended_ram_gb": 4.4,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "yi:9b",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "yi:34b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.5,
    "use_cases": [
      "Translation",
      "Text Summarization",
      "Question Answering",
      "Chat Assistant",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for language processing tasks.",
    "pulls": 583600,
    "tags": 174,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5c52d3bb-3545-411a-85c8-788945262061",
    "model_identifier": "zephyr",
    "model_name": "zephyr",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/zephyr",
    "description": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr 141B-A35B is the latest model in the series, and is a fine-tuned version of \nMixtral 8x22b\n.\n\n\nSizes\n\n\n\n\nzephyr:141b\n: A Mixture of Experts (MoE) model with 141B total parameters and 35B active parameters.\n\n\nzephyr:7b\n: The original Zephyr model\n\n\n\n\nSource:\n\n\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/8dab1157-909a-455b-a5c4-f7c31e9bcf0e\" width=\"400\" />\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr 141B-A35B is the latest model in the series, and is a fine-tuned version of [Mixtral 8x22b](https://ollama.com/library/mixtral:8x22b).\n\n## Sizes\n\n* `zephyr:141b`: A Mixture of Experts (MoE) model with 141B total parameters and 35B active parameters.\n* `zephyr:7b`: The original Zephyr model\n\n## Source:\n\n* [HuggingFace](https://huggingface.co/HuggingFaceH4)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "141b"
    ],
    "memory_requirements": [
      {
        "tag": "zephyr:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "zephyr:141b",
        "size": "80GB",
        "size_gb": 80.0,
        "recommended_ram_gb": 100.0,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 563900,
    "tags": 40,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d3b2bbeb-0c50-43ca-9816-38eded94afdb",
    "model_identifier": "mistral-large",
    "model_name": "mistral-large",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-large",
    "description": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.\n\n\nKey features\n\n\n\n\nMulti-lingual by design: Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.\n\n\nProficient in coding: Trained on 80+ coding languages such as Python, Java, C, C++, JavacScript, and Bash. Also trained on more specific languages such as Swift and Fortran.\n\n\nAgentic-centric: Best-in-class agentic capabilities with native function calling and JSON outputting.\n\n\nAdvanced Reasoning: State-of-the-art mathematical and reasoning capabilities.\n\n\nMistral Research License: Allows usage and modification for research and non-commercial usages.\n\n\nLarge Context: A large 128k context window.\n\n\n\n\nReferences\n\n\nBlog post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/mistral-large/1b9ab956-d5ea-4927-a54a-3f969021d3db\" width=\"320\" />\n\nMistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.\n\n### Key features\n* Multi-lingual by design: Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.\n* Proficient in coding: Trained on 80+ coding languages such as Python, Java, C, C++, JavacScript, and Bash. Also trained on more specific languages such as Swift and Fortran.\n* Agentic-centric: Best-in-class agentic capabilities with native function calling and JSON outputting.\n* Advanced Reasoning: State-of-the-art mathematical and reasoning capabilities.\n* Mistral Research License: Allows usage and modification for research and non-commercial usages.\n* Large Context: A large 128k context window.\n\n## References\n\n[Blog post](https://mistral.ai/news/mistral-large-2407/)\n\n[Hugging Face](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "123b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-large:latest",
        "size": "73GB",
        "size_gb": 73.0,
        "recommended_ram_gb": 91.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "73GB",
        "size_gb": 73.0,
        "recommended_ram_gb": 91.2,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 73.0,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Reasoning",
      "Math"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers who need a powerful code generation and reasoning model.",
    "pulls": 554100,
    "tags": 32,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "cb56f260-147d-47bf-8478-d2f490107a7b",
    "model_identifier": "phi3.5",
    "model_name": "phi3.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi3.5",
    "description": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites with a focus on very high-quality, reasoning dense data.\n\n\nThe model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n\nLong Context\n\n\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" />\n\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites with a focus on very high-quality, reasoning dense data.\n\nThe model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n### Long Context\n\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval.\n\n## References\n\n[Hugging Face](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3.8b"
    ],
    "memory_requirements": [
      {
        "tag": "phi3.5:latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.2,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for complex text analysis tasks.",
    "pulls": 551900,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4309f83a-b91e-4bb6-afcc-20072075ecb6",
    "model_identifier": "wizard-vicuna-uncensored",
    "model_name": "wizard-vicuna-uncensored",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizard-vicuna-uncensored",
    "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nWizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\n\n\nGet started with Wizard Vicuna Uncensored\n\n\nThe model used in the example below is the Wizard Vicuna Uncensored model, with 7b parameters, which is a general-use model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run wizard-vicuna-uncensored\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull wizard-vicuna-uncensored\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n30b models generally require at least 32GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n\n\n\n\n\n\nAliases\n\n\n\n\n\n\n\n\n\n\nlatest, 7b, 7b-q4_0\n\n\n\n\n\n\n13b, 13b-q4_0\n\n\n\n\n\n\n30b, 30b-q4_0\n\n\n\n\n\n\n\n\nModel source\n\n\nWizard Vicuna Uncensored source on Ollama\n\n\n7b parameters original source:\n \nEric Hartford\n\n\n13b parameters original source:\n \nEric Hartford\n\n\n30b parameters original source:\n \nEric Hartford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\n\n## Get started with Wizard Vicuna Uncensored\n\nThe model used in the example below is the Wizard Vicuna Uncensored model, with 7b parameters, which is a general-use model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run wizard-vicuna-uncensored`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull wizard-vicuna-uncensored`\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n- 30b models generally require at least 32GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n| **Aliases** |\n| --- |\n| latest, 7b, 7b-q4_0 |\n| 13b, 13b-q4_0 |\n| 30b, 30b-q4_0 |\n\n## Model source \n\n**Wizard Vicuna Uncensored source on Ollama**\n\n7b parameters original source:\n [Eric Hartford](https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored)\n\n13b parameters original source:\n [Eric Hartford](https://huggingface.co/ehartford/Wizard-Vicuna-13B-Uncensored)\n\n30b parameters original source:\n [Eric Hartford](https://huggingface.co/ehartford/Wizard-Vicuna-30B-Uncensored)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "30b"
    ],
    "memory_requirements": [
      {
        "tag": "wizard-vicuna-uncensored:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizard-vicuna-uncensored:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizard-vicuna-uncensored:30b",
        "size": "18GB",
        "size_gb": 18.0,
        "recommended_ram_gb": 22.5,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 546000,
    "tags": 49,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "876ac2fd-14b7-4407-8c0c-8db349865ea4",
    "model_identifier": "embeddinggemma",
    "model_name": "embeddinggemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/embeddinggemma",
    "description": "EmbeddingGemma is a 300M parameter embedding model from Google.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nThis model requires \nOllama v0.11.10\n or later\n\n\n\n\nEmbeddingGemma\n is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\n\n\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\n\n\nBenchmark\n\n\n\n\nTraining Dataset\n\n\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\n\n\n\n\nWeb Documents\n: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\n\n\nCode and Technical Documents\n: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\n\n\nSynthetic and Task-Specific Data\n: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\n\n\n\n\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\n\n\nReference\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/embeddinggemma/9a20d963-4bf1-4177-9568-ca5d53a2d14e)\n\n> This model requires [Ollama v0.11.10](https://github.com/ollama/ollama/releases/tag/v0.11.10) or later\n\n**EmbeddingGemma** is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\n\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\n\n### Benchmark\n\n![image.png](/assets/library/embeddinggemma/59a205f6-1711-4db4-8026-96d23fa2c9da)\n\n\n#### Training Dataset\n\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\n\n-   **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\n-   **Code and Technical Documents**: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\n-   **Synthetic and Task-Specific Data**: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\n\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\n\n\n### Reference \n[Documentation](https://ai.google.dev/gemma/docs/embeddinggemma)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "300m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding",
      "RAG / Retrieval"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "For text embedding tasks and retrieval applications.",
    "pulls": 540300,
    "tags": 5,
    "last_updated": "2025-09-25",
    "last_updated_str": "5 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "06f462f5-75c1-4df8-bbf2-f238df513d44",
    "model_identifier": "bakllava",
    "model_name": "bakllava",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/bakllava",
    "description": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.1.15: \ndownload it here\n\n\n\n\nCLI Usage\n\n\nRun the model:\n\n\nollama run bakllava\n\n\n\nThen at the prompt, include the path to your image in the prompt:\n\n\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n\n\n\nAPI Usage\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"bakllava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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\"]\n}'\n\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires Ollama 0.1.15: [download it here](https://ollama.ai/download)\n\n\n### CLI Usage\n\nRun the model:\n\n```\nollama run bakllava\n```\n\nThen at the prompt, include the path to your image in the prompt:\n\n```\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n```\n\n### API Usage\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"bakllava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "bakllava:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [],
    "domain": "General",
    "ai_languages": [],
    "complexity": null,
    "best_for": null,
    "pulls": 521700,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "02b0b62e-e3c2-4776-935d-0df4cad24994",
    "model_identifier": "starcoder",
    "model_name": "starcoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/starcoder",
    "description": "StarCoder is a code generation model trained on 80+ programming languages.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nStarCoder models are trained on GitHub code with over 80+ programming languages.\n\n\nExample prompt\n\n\ndef print_hello_world():\n\n\n\nExample response\n\n\n    print(\"Hello world\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/1bf014aa-cde3-4f9d-88df-81773106b1dd\" width=\"400\" />\n\nStarCoder models are trained on GitHub code with over 80+ programming languages.\n\n## Example prompt\n\n```\ndef print_hello_world():\n```\n\n## Example response\n\n```\n    print(\"Hello world\")\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1b",
      "3b",
      "7b",
      "15b"
    ],
    "memory_requirements": [
      {
        "tag": "starcoder:latest",
        "size": "1.8GB",
        "size_gb": 1.8,
        "recommended_ram_gb": 2.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "1.8GB",
        "size_gb": 1.8,
        "recommended_ram_gb": 2.2,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "starcoder:7b",
        "size": "4.3GB",
        "size_gb": 4.3,
        "recommended_ram_gb": 5.4,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "starcoder:15b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.8,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers who need to generate code in multiple programming languages.",
    "pulls": 501700,
    "tags": 100,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "94bff95b-8fa5-48cf-83be-9b180f140e25",
    "model_identifier": "paraphrase-multilingual",
    "model_name": "paraphrase-multilingual",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/paraphrase-multilingual",
    "description": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n\n\n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)\n\n```\n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "278m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Clustering and semantic search tasks.",
    "pulls": 503500,
    "tags": 3,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "47467923-b5bc-444d-bd6d-c190a619c392",
    "model_identifier": "nous-hermes",
    "model_name": "nous-hermes",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nous-hermes",
    "description": "General use models based on Llama and Llama 2 from Nous Research.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNous Hermes was released by Nous Research. There are two main variants here, a 13B parameter model based on Llama, and a 7B and 13B parameter model based on Llama 2. They are all general-use models trained with the same datasets.\n\n\nGet started with Nous Hermes\n\n\nThe model used in the example below is the Nous Hermes Llama 2 model, with 7b parameters, which is a general chat model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"nous-hermes\",\n  \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run nous-hermes\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull nous-hermes\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nOllama offers many variants of the Nous Hermes model that are quantized based on the official models to run well locally.\n\n\nNous Hermes Llama 2\n is the original Nous Hermes model based on the original Llama model.\n\n\nExample: \nollama run nous-hermes\n\n\nNous Hermes Llama 1\n is the original Nous Hermes model based on the original Llama model.\n\n\nExample: \nollama run nous-hermes:13b-q4_0\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n\n\n\n\n\n\nAliases\n\n\n\n\n\n\n\n\n\n\nlatest, 7b, 7b-llama2, 7b-llama2-q4_0\n\n\n\n\n\n\n13b, 13b-llama2, 13b-llama2-q4_0\n\n\n\n\n\n\n\n\nModel source\n\n\nNous Hermes Llama 2 source on Ollama\n\n\n7b parameters original source:\n \nNous Research\n\n\n13b parameters original source:\n \nNous Research\n\n\nNous Hermes Llama 1 source on Ollama\n\n\n13b parameters original source:\n \nNous Research\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/262739492-985b8e69-35be-4eaf-a954-c452c4363c72.png\" style=\"max-width:60%\" />\n\nNous Hermes was released by Nous Research. There are two main variants here, a 13B parameter model based on Llama, and a 7B and 13B parameter model based on Llama 2. They are all general-use models trained with the same datasets.\n\n## Get started with Nous Hermes\n\nThe model used in the example below is the Nous Hermes Llama 2 model, with 7b parameters, which is a general chat model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"nous-hermes\",\n  \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run nous-hermes`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull nous-hermes`\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\nOllama offers many variants of the Nous Hermes model that are quantized based on the official models to run well locally.\n\n**Nous Hermes Llama 2** is the original Nous Hermes model based on the original Llama model.\n\n*Example: `ollama run nous-hermes`*\n\n**Nous Hermes Llama 1** is the original Nous Hermes model based on the original Llama model.\n\n*Example: `ollama run nous-hermes:13b-q4_0`*\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n| **Aliases** |\n| --- |\n| latest, 7b, 7b-llama2, 7b-llama2-q4_0 |\n| 13b, 13b-llama2, 13b-llama2-q4_0 |\n\n## Model source \n\n\n\n**Nous Hermes Llama 2 source on Ollama**\n\n7b parameters original source:\n [Nous Research](https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b)\n\n13b parameters original source:\n [Nous Research](https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b)\n\n**Nous Hermes Llama 1 source on Ollama**\n\n13b parameters original source:\n [Nous Research](https://huggingface.co/NousResearch/Nous-Hermes-13b)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "nous-hermes:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "nous-hermes:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Chat Assistant"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and Researchers for general language tasks.",
    "pulls": 494200,
    "tags": 63,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ef9f8dfe-2ffc-4b08-bc9d-30121a8035c4",
    "model_identifier": "exaone-deep",
    "model_name": "exaone-deep",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/exaone-deep",
    "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nEXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.\n\n\nEvaluation results show that:\n\n\n\n\nEXAONE Deep 2.4B outperforms other models of comparable size\n\n\nEXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini\n\n\nEXAONE Deep 32B demonstrates competitive performance against leading open-weight models.\n\n\n\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/exaone-deep/05e9cd86-a79b-4fd5-9e0f-7f18f1c1a928\" width=\"320\" />\n\n\nEXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.\n\nEvaluation results show that:\n\n1. EXAONE Deep 2.4B outperforms other models of comparable size\n2. EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini\n3. EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.\n\n![exaone_deep_overall_performance.png](/assets/library/exaone-deep/b107bac2-f509-4a1b-9999-344d976c803e)\n\n## References\n\n[Hugging Face](https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2.4b",
      "7.8b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "exaone-deep:2.4b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "exaone-deep:latest",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "exaone-deep:32b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Reasoning",
      "Code Generation",
      "Math"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex reasoning tasks.",
    "pulls": 492300,
    "tags": 13,
    "last_updated": "2025-03-25",
    "last_updated_str": "11 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "e60c4ff1-381f-4009-bd29-da177348eb22",
    "model_identifier": "deepseek-llm",
    "model_name": "deepseek-llm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-llm",
    "description": "An advanced language model crafted with 2 trillion bilingual tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek LLM is an advanced language model available in both 7 billion and 67 billion parameters. Both a \nchat\n and \nbase\n variation are available.\n\n\n\n\n\n\nSuperior General Capabilities: DeepSeek LLM 67B Base outperforms Llama2 70B Base in areas such as reasoning, coding, math, and Chinese comprehension.\n\n\nProficient in Coding and Math: DeepSeek LLM 67B Chat exhibits outstanding performance in coding (using the HumanEval benchmark) and mathematics (using the GSM8K benchmark).\n\n\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/0e62c3d2-6b1d-408a-a44d-273fc72697e6\" width=\"240\" />\n\nDeepSeek LLM is an advanced language model available in both 7 billion and 67 billion parameters. Both a `chat` and `base` variation are available.\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/c74a21e9-1eb9-4036-9f83-6c3a027134c4\" width=\"500\" />\n\n* Superior General Capabilities: DeepSeek LLM 67B Base outperforms Llama2 70B Base in areas such as reasoning, coding, math, and Chinese comprehension.\n\n* Proficient in Coding and Math: DeepSeek LLM 67B Chat exhibits outstanding performance in coding (using the HumanEval benchmark) and mathematics (using the GSM8K benchmark).\n\n## References\n\n[GitHub](https://github.com/deepseek-ai/deepseek-LLM)\n\n[HuggingFace](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "67b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-llm:latest",
        "size": "4.0GB",
        "size_gb": 4.0,
        "recommended_ram_gb": 5.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "4.0GB",
        "size_gb": 4.0,
        "recommended_ram_gb": 5.0,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "deepseek-llm:67b",
        "size": "38GB",
        "size_gb": 38.0,
        "recommended_ram_gb": 47.5,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 4.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced language tasks.",
    "pulls": 487600,
    "tags": 64,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6ede1e0b-1ac3-4ad5-b9db-8af665468ab2",
    "model_identifier": "falcon",
    "model_name": "falcon",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/falcon",
    "description": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nFalcon is a family of high-performing large language models model built by the Technology Innovation Institute (TII), a research center part of Abu Dhabi government’s advanced technology research council overseeing technology research.\n\n\nCLI\n\n\nollama run falcon \"Why is the sky blue?\"\n\n\n\nAPI\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"falcon\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n\n\n\nParameter counts\n\n\n\n\n\n\n\n\nParameter Count\n\n\nRecommended memory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 billion\n\n\n8GB\n\n\nView\n\n\nollama run falcon:7b\n\n\n\n\n\n\n40 billion\n\n\n32GB\n\n\nView\n\n\nollama run falcon:40b\n\n\n\n\n\n\n180 billion\n\n\n192GB\n\n\nView\n\n\nollama run falcon:180b\n\n\n\n\n\n\n\n\nVariations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchat\n\n\nChat models are fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.\n\n\n\n\n\n\ninstruct\n\n\nInstruct models follow instructions and are fine-tuned on the \nbaize\n instructional dataset.\n\n\n\n\n\n\ntext\n\n\nText models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion.\n\n\n\n\n\n\n\n\nFalcon 180B\n\n\nAs of September 2023, the 180 billion parameter model, Falcon 180B, is the best-performing openly released LLM. It sits somewhere in between OpenAI’s GPT 3.5 and GPT 4. For running Falcon 180B, a powerful system is recommended with at least 192GB of total memory.\n\n\n\n\nNote: Falcon 180B is released under a different license than its smaller siblings that restricts commercial use under certain conditions. See the \nmodel details\n and license for more information.\n\n\n\n\nMore information\n\n\n\n\nTII’s website\n\n\nFalcon 180B announcement\n\n\nTII on HuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/af914f26-9f0b-4e66-947a-33ae0d558ccf\" alt=\"Technology Innovation Institute Logo\" width=\"240\"/>\n\nFalcon is a family of high-performing large language models model built by the Technology Innovation Institute (TII), a research center part of Abu Dhabi government’s advanced technology research council overseeing technology research. \n\n### CLI\n\n```\nollama run falcon \"Why is the sky blue?\"\n```\n\n### API\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"falcon\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n```\n\n## Parameter counts\n| Parameter Count | Recommended memory |                              |                          |\n| --------------- | ------------------ | ---------------------------- | ------------------------ |\n| 7 billion       | 8GB               | [View](/library/falcon:7b)   | `ollama run falcon:7b`   |\n| 40 billion      | 32GB               | [View](/library/falcon:40b)  | `ollama run falcon:40b`  |\n| 180 billion     | 192GB              | [View](/library/falcon:180b) | `ollama run falcon:180b` |\n\n## Variations\n\n|            |                                                                                                                                    |\n| ---------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n| `chat`     | Chat models are fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.             |\n| `instruct` | Instruct models follow instructions and are fine-tuned on the [baize](https://www.google.com/search?q=baize+dataset&oq=baize+data&aqs=chrome.0.0i512j69i57j0i10i15i22i30i625j0i390i650.1387j0j7&sourceid=chrome&ie=UTF-8) instructional dataset.                                                     |\n| `text`     | Text models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion. |\n\n\n\n## Falcon 180B\n\nAs of September 2023, the 180 billion parameter model, Falcon 180B, is the best-performing openly released LLM. It sits somewhere in between OpenAI's GPT 3.5 and GPT 4. For running Falcon 180B, a powerful system is recommended with at least 192GB of total memory.\n\n> Note: Falcon 180B is released under a different license than its smaller siblings that restricts commercial use under certain conditions. See the [model details](/library/falcon:180b) and license for more information.\n\n## More information\n\n* [TII's website](https://www.tii.ae/)\n* [Falcon 180B announcement](https://falconllm.tii.ae)\n* [TII on HuggingFace](https://huggingface.co/tiiuae)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "40b",
      "180b"
    ],
    "memory_requirements": [
      {
        "tag": "falcon:latest",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "falcon:40b",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "falcon:180b",
        "size": "101GB",
        "size_gb": 101.0,
        "recommended_ram_gb": 126.2,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 4.2,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Chat Assistant"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Handling complex language tasks and large datasets.",
    "pulls": 485100,
    "tags": 38,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a397d2ae-7108-43e2-827b-5113448e23cf",
    "model_identifier": "deepseek-v2",
    "model_name": "deepseek-v2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-v2",
    "description": "A strong, economical, and efficient Mixture-of-Experts language model.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.1.40\n.\n\n\n\n\nDeepSeek-V2 is a a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference.\n\n\nNote: this model is bilingual in English and Chinese.\n\n\nThe model comes in two sizes:\n\n\n\n\n16B Lite: \nollama run deepseek-v2:16b\n\n\n236B: \nollama run deepseek-v2:236b\n\n\n\n\nReferences\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/deepseek-v2/9422627b-6d24-401b-8d49-5d316df9956b\" width=\"320\" />\n\n> Note: this model requires [Ollama 0.1.40](https://github.com/ollama/ollama/releases/tag/v0.1.40).\n\nDeepSeek-V2 is a a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference.\n\nNote: this model is bilingual in English and Chinese.\n\nThe model comes in two sizes:\n\n* 16B Lite: `ollama run deepseek-v2:16b`\n* 236B: `ollama run deepseek-v2:236b`\n\n## References\n\n[GitHub](https://github.com/deepseek-ai/DeepSeek-V2)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "16b",
      "236b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-v2:latest",
        "size": "8.9GB",
        "size_gb": 8.9,
        "recommended_ram_gb": 11.1,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      },
      {
        "tag": "latest",
        "size": "8.9GB",
        "size_gb": 8.9,
        "recommended_ram_gb": 11.1,
        "quantization": "q4_k_m",
        "context": "160K",
        "context_window": 160000
      },
      {
        "tag": "deepseek-v2:236b",
        "size": "133GB",
        "size_gb": 133.0,
        "recommended_ram_gb": 166.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 8.9,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 483800,
    "tags": 34,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6821a0c7-c8ec-40c4-b36c-4ac860f28080",
    "model_identifier": "openchat",
    "model_name": "openchat",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/openchat",
    "description": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOpenChat is set of open-source language models, fine-tuned with C-RLFT: a strategy inspired by offline reinforcement learning.\n\n\nUpdated to OpenChat-3.5-1210, this new version of the model model excels at coding tasks and scores very high on many open-source LLM benchmarks.\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/b39cea20-f0e9-4f63-a5b1-fa0c2368c286\" width=\"260\" />\n\nOpenChat is set of open-source language models, fine-tuned with C-RLFT: a strategy inspired by offline reinforcement learning.\n\nUpdated to OpenChat-3.5-1210, this new version of the model model excels at coding tasks and scores very high on many open-source LLM benchmarks.\n\n## References\n\n[GitHub](https://github.com/imoneoi/openchat)\n\n[HuggingFace](https://huggingface.co/openchat/openchat_3.5)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "openchat:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Code Generation",
      "Question Answering",
      "Text Summarization"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers focusing on coding tasks.",
    "pulls": 482400,
    "tags": 50,
    "last_updated": "2026-02-25",
    "last_updated_str": "to version 3.5-0106.",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5d1e0592-2dce-4b05-820c-5d56a547fceb",
    "model_identifier": "vicuna",
    "model_name": "vicuna",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/vicuna",
    "description": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nVicuna is a chat assistant model. It includes 3 different variants in 3 different sizes. v1.3 is trained by fine-tuning Llama and has a context size of 2048 tokens. v1.5 is trained by fine-tuning Llama 2 and has a context size of 2048 tokens. v1.5-16k is trained by fine-tuning Llama 2 and has a context size of 16k tokens. All three variants are trained using conversations collected from ShareGPT.\n\n\nExample prompts\n\n\nWhat is the meaning of life? Explain it in 5 paragraphs.\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVicuna is a chat assistant model. It includes 3 different variants in 3 different sizes. v1.3 is trained by fine-tuning Llama and has a context size of 2048 tokens. v1.5 is trained by fine-tuning Llama 2 and has a context size of 2048 tokens. v1.5-16k is trained by fine-tuning Llama 2 and has a context size of 16k tokens. All three variants are trained using conversations collected from ShareGPT.\n\n## Example prompts\n\n```\nWhat is the meaning of life? Explain it in 5 paragraphs.\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/lmsys)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "33b"
    ],
    "memory_requirements": [
      {
        "tag": "vicuna:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "vicuna:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "vicuna:33b",
        "size": "18GB",
        "size_gb": 18.0,
        "recommended_ram_gb": 22.5,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Chat Assistant",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "General language model for conversations and Q&A tasks.",
    "pulls": 478500,
    "tags": 111,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "84c8854a-ab84-4738-9b71-41d96def9eaa",
    "model_identifier": "ministral-3",
    "model_name": "ministral-3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/ministral-3",
    "description": "The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nThis model requires \nOllama 0.13.1\n, which is currently in pre-release.\n\n\n\n\nThe Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.\n\n\nThe Ministral 3 models offer the following capabilities:\n\n\n\n\nVision: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n\n\nMultilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n\n\nSystem Prompt: Maintains strong adherence and support for system prompts.\n\n\nAgentic: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n\n\nEdge-Optimized: Delivers best-in-class performance at a small scale, deployable anywhere.\n\n\nApache 2.0 License: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\n\n\nLarge Context Window: Supports a 256k context window.\n\n\n\n\n\n\nMinistral 14B\n\n\n\n\nMinistral 8B\n\n\n\n\nMinistral 3B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/ministral-3/83fa3859-d87f-492c-bd81-596cfbceeccb\" width=\"120\" />\n\n> This model requires [Ollama 0.13.1](https://github.com/ollama/ollama/releases/tag/v0.13.1-rc1), which is currently in pre-release.\n\nThe Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware. \n\nThe Ministral 3 models offer the following capabilities:\n\n* Vision: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n* Multilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n* System Prompt: Maintains strong adherence and support for system prompts.\n* Agentic: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n* Edge-Optimized: Delivers best-in-class performance at a small scale, deployable anywhere.\n* Apache 2.0 License: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\n* Large Context Window: Supports a 256k context window.\n\n![gpqa](/assets/library/ministral-3/e7c21288-12be-484a-9860-348900545e0b)\n\n**Ministral 14B** \n![Ministral 14B](/assets/library/ministral-3/ea92b94b-5804-48c1-b9d0-78b54aba1ed4)\n\n**Ministral 8B** \n![Ministral 8B](/assets/library/ministral-3/6329dd14-6ef4-4791-870e-cc1dd3fb1b71)\n\n**Ministral 3B**\n![Ministral 3B](/assets/library/ministral-3/dac29f36-ba47-42b3-af92-550faeb97556)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "3b",
      "8b",
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "ministral-3:3b",
        "size": "3.0GB",
        "size_gb": 3.0,
        "recommended_ram_gb": 3.8,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "ministral-3:latest",
        "size": "6.0GB",
        "size_gb": 6.0,
        "recommended_ram_gb": 7.5,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "latest",
        "size": "6.0GB",
        "size_gb": 6.0,
        "recommended_ram_gb": 7.5,
        "quantization": "q4_k_m",
        "context": "256K",
        "context_window": 256000
      },
      {
        "tag": "ministral-3:14b",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 3.0,
    "use_cases": [
      "Image Understanding",
      "Text Summarization",
      "Translation",
      "Function Calling"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for image analysis and text processing tasks.",
    "pulls": 479900,
    "tags": 16,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "35c8e1c6-d79a-4d82-84b0-9de54a6bc95e",
    "model_identifier": "openhermes",
    "model_name": "openhermes",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/openhermes",
    "description": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOpen Hermes 2 a Mistral 7B fine-tuned with fully open datasets. Matching 70B models on benchmarks, this model has strong multi-turn chat skills and system prompt capabilities. In total, the model was trained on 900,000 instructions, and surpasses all previous versions of Nous-Hermes 13B and below.\n\n\nVersions\n\n\n\n\n\n\n\n\nTag\n\n\nDate\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nv2.5\n \nlatest\n\n\n11/02/2023\n\n\nAdded ~100k examples of Code Instructions\n\n\n\n\n\n\nv2\n\n\n10/16/2023\n\n\nInitial release of Open Hermes 2\n\n\n\n\n\n\n\n\nUsage\n\n\nCLI\n\n\nollama run openhermes\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"openhermes\",\n  \"prompt\": \"Here is a story about llamas eating grass\"\n}'\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/abc656b5-6d3b-47ef-8d1c-c0445f780168\" width=\"320\" />\n\nOpen Hermes 2 a Mistral 7B fine-tuned with fully open datasets. Matching 70B models on benchmarks, this model has strong multi-turn chat skills and system prompt capabilities. In total, the model was trained on 900,000 instructions, and surpasses all previous versions of Nous-Hermes 13B and below.\n\n### Versions\n\n| Tag             | Date       | Notes                                     |\n| --------------- | ---------- | ----------------------------------------- |\n| `v2.5` `latest` | 11/02/2023 | Added ~100k examples of Code Instructions |\n| `v2`            | 10/16/2023 | Initial release of Open Hermes 2          |\n\n\n## Usage\n\n### CLI\n\n```\nollama run openhermes\n```\n\n### API\n\nExample:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"openhermes\",\n  \"prompt\": \"Here is a story about llamas eating grass\"\n}'\n```\n\n## References\n\n[Hugging Face](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [],
    "memory_requirements": [
      {
        "tag": "openhermes:7b-mistral-v2-q2_K",
        "size": "3.1GB",
        "size_gb": 3.1,
        "recommended_ram_gb": 3.9,
        "quantization": "q2_k",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q2_K",
        "size": "3.1GB",
        "size_gb": 3.1,
        "recommended_ram_gb": 3.9,
        "quantization": "q2_k",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q3_K_S",
        "size": "3.2GB",
        "size_gb": 3.2,
        "recommended_ram_gb": 4.0,
        "quantization": "q3_k_s",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q3_K_S",
        "size": "3.2GB",
        "size_gb": 3.2,
        "recommended_ram_gb": 4.0,
        "quantization": "q3_k_s",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q3_K_M",
        "size": "3.5GB",
        "size_gb": 3.5,
        "recommended_ram_gb": 4.4,
        "quantization": "q3_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q3_K_M",
        "size": "3.5GB",
        "size_gb": 3.5,
        "recommended_ram_gb": 4.4,
        "quantization": "q3_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q3_K_L",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q3_k_l",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q3_K_L",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q3_k_l",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:v2",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q4_K_M",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q4_K_M",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q4_1",
        "size": "4.6GB",
        "size_gb": 4.6,
        "recommended_ram_gb": 5.8,
        "quantization": "q4_1",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q4_1",
        "size": "4.6GB",
        "size_gb": 4.6,
        "recommended_ram_gb": 5.8,
        "quantization": "q4_1",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_0",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q5_0",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_K_S",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q5_k_s",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_K_M",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q5_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q5_K_M",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q5_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_1",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q5_1",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q5_1",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q5_1",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q6_K",
        "size": "5.9GB",
        "size_gb": 5.9,
        "recommended_ram_gb": 7.4,
        "quantization": "q6_k",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q6_K",
        "size": "5.9GB",
        "size_gb": 5.9,
        "recommended_ram_gb": 7.4,
        "quantization": "q6_k",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-q8_0",
        "size": "7.7GB",
        "size_gb": 7.7,
        "recommended_ram_gb": 9.6,
        "quantization": "q8_0",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q8_0",
        "size": "7.7GB",
        "size_gb": 7.7,
        "recommended_ram_gb": 9.6,
        "quantization": "q8_0",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2-fp16",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "fp16",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-fp16",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "fp16",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 3.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 459100,
    "tags": 35,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5fe9072b-919f-4625-9dbf-993d92b98208",
    "model_identifier": "codeqwen",
    "model_name": "codeqwen",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codeqwen",
    "description": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCodeQwen1.5 is based on Qwen1.5. It is trained on 3 trillion tokens of code data. Its major features include:\n\n\n\n\nStrong code generation capabilities and competitive performance across a series of benchmarks\n\n\nSupport for long context understanding and generation with a maximum context length of 64K tokens\n\n\nSupport for 92 coding languages\n\n\nExcellent performance in Text-to-SQL, fixing bugs and other coding use cases.\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/c17cc1ce-9eb9-4658-8b02-b2f15bbaf1c1\" width=\"360\" />\n\nCodeQwen1.5 is based on Qwen1.5. It is trained on 3 trillion tokens of code data. Its major features include:\n\n* Strong code generation capabilities and competitive performance across a series of benchmarks\n* Support for long context understanding and generation with a maximum context length of 64K tokens\n* Support for 92 coding languages\n* Excellent performance in Text-to-SQL, fixing bugs and other coding use cases.\n\n## References\n\n[Blog Post](https://qwenlm.github.io/blog/codeqwen1.5/)\n\n[GitHub](https://github.com/QwenLM/Qwen)\n\n[HuggingFace](https://huggingface.co/Qwen)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "codeqwen:latest",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      },
      {
        "tag": "latest",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_k_m",
        "context": "64K",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 4.2,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working with code.",
    "pulls": 458800,
    "tags": 30,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "cee362ab-6c79-4312-8fd6-3ada0711322c",
    "model_identifier": "glm4",
    "model_name": "glm4",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/glm4",
    "description": "A strong multi-lingual general language model with competitive performance to Llama 3.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.2\n or later.\n\n\n\n\nIn the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, this model has shown superior performance beyond even Llama 3 8B.\n\n\nThis generation of models includes multi-language support, supporting 26 languages including Japanese, Korean, and German.\n\n\nReferences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/glm4/7646ce96-fbec-4694-a3f5-39f9e61f19a6\" width=\"320\" />\n\n> Note: this model requires [Ollama 0.2](https://github.com/ollama/ollama/releases/tag/v0.2.0) or later.\n\nIn the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, this model has shown superior performance beyond even Llama 3 8B.\n\nThis generation of models includes multi-language support, supporting 26 languages including Japanese, Korean, and German.\n\n## References\n\n[GitHub](https://github.com/THUDM/GLM)\n\n[Hugging Face](https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "9b"
    ],
    "memory_requirements": [
      {
        "tag": "glm4:latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 5.5,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 465000,
    "tags": 32,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a394e963-d7a8-40d0-9c53-99636394221d",
    "model_identifier": "qwen2-math",
    "model_name": "qwen2-math",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2-math",
    "description": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen2 Math\n features 3 model sizes (\n1.5B\n, \n7B\n, and \n72B\n) each with a \nbase\n model, for completion, few-shot inference, and fine-tuning, and an \ninstruct\n model for chatting.\n\n\nReferences:\n\n\n\n\nQwen 2 Math Blog Post\n\n\nGitHub Repository\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/qwen2-math/c0159f63-bf4d-4b0d-9b92-ea44c48d34c1\" width=\"380\" />\n\n**Qwen2 Math** features 3 model sizes (**1.5B**, **7B**, and **72B**) each with a **base** model, for completion, few-shot inference, and fine-tuning, and an **instruct** model for chatting.\n\n## References: \n- [Qwen 2 Math Blog Post](https://qwenlm.github.io/blog/qwen2-math/)\n- [GitHub Repository](https://github.com/QwenLM/Qwen2-Math)\n- [Hugging Face](https://huggingface.co/collections/Qwen/qwen2-math-66b4c9e072eda65b5ec7534d)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.5b",
      "7b",
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2-math:latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "qwen2-math:72b",
        "size": "41GB",
        "size_gb": 41.0,
        "recommended_ram_gb": 51.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 4.4,
    "use_cases": [
      "Math",
      "Question Answering"
    ],
    "domain": "Math",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced mathematical problem solving.",
    "pulls": 446600,
    "tags": 52,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ec534a7a-7d5e-4862-bd4c-6b7853162f18",
    "model_identifier": "aya",
    "model_name": "aya",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/aya",
    "description": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nAya 23, released by Cohere, is a new family of state-of-the-art, multilingual, generative large language research model (LLM) covering 23 different languages.\n\n\nIt is available in 8B and 35B parameter sizes:\n\n\n\n\n8B \nollama run aya:8b\n\n\n35B \nollama run aya:35b\n\n\n\n\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nAya 23: Open Weight Releases to Further Multilingual Progress paper\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](https://ollama.com/assets/library/aya/f55197c8-901d-45f5-ba00-75fb0230f1c9)\n\nAya 23, released by Cohere, is a new family of state-of-the-art, multilingual, generative large language research model (LLM) covering 23 different languages.\n\nIt is available in 8B and 35B parameter sizes:\n\n* 8B `ollama run aya:8b`\n* 35B `ollama run aya:35b`\n\n![Multilingual Benchmarks](https://ollama.com/assets/library/aya/4d6844fc-f55c-470b-9625-c8b2b7927c35)\n\n![aya win rates vs other models](https://ollama.com/assets/library/aya/ceca785b-8190-4f0b-854a-68d375c22e46)\n\n\n# References\n\n[Blog Post](https://cohere.com/blog/aya23)\n\n[Aya 23: Open Weight Releases to Further Multilingual Progress paper](https://cohere.com/research/papers/aya-command-23-8b-and-35b-technical-report-2024-05-23)\n\n[Hugging Face](https://huggingface.co/collections/CohereForAI/c4ai-aya-23-664f4cda3fa1a30553b221dc)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "35b"
    ],
    "memory_requirements": [
      {
        "tag": "aya:latest",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "aya:35b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.8,
    "use_cases": [
      "Translation",
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers working with multilingual text data.",
    "pulls": 440400,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1bf73d42-d53f-4f69-928b-85f6a7fb57d9",
    "model_identifier": "llama2-chinese",
    "model_name": "llama2-chinese",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama2-chinese",
    "description": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nLlama 2 对话中文微调参数模型\n\n\n这个模型是基于 Meta Platform, Inc. 所发布的 Llama 2 Chat 开源模型来进行微调。根据Meta，Llama 2 的训练数据达到了两万亿个token，上下文长度也提升到4096。对话上也是使用100万人类标记的数据微调。\n\n\n由于 Llama 2 本身的中文对齐比较弱，开发者采用了中文指令集来进行微调，使其具备较强的中文对话能力。目前这个中文微调参数模型总共发布了 7B，13B两种参数大小。\n\n\nLlama 2 chat chinese fine-tuned model\n\n\nThis model is fine-tuned based on Meta Platform’s Llama 2 Chat open source model. According to Meta, Llama 2 is trained on 2 trillion tokens, and the context length is increased to 4096. The chat model is fine-tuned using 1 million human labeled data.\n\n\nSince the Chinese alignment of Llama 2 itself is relatively weak, the developer, adopted a Chinese instruction set for fine-tuning to improve the Chinese dialogue ability.\n\n\nThe Chinese fine-tuned models are available in 7B and 13B parameter sizes.\n\n\nCLI\n\n\nOpen the terminal and run \nollama run llama2-chinese\n\n\nAPI\n\n\nRun the model\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2-chinese:7b-chat-q4_0\",\n  \"prompt\":\"为什么天空是蓝色的\"\n }'\n\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n\n\nReference\n\n\nFlagAlpha\n\n\nFlagAlpha\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLlama 2 对话中文微调参数模型 \n\n这个模型是基于 Meta Platform, Inc. 所发布的 Llama 2 Chat 开源模型来进行微调。根据Meta，Llama 2 的训练数据达到了两万亿个token，上下文长度也提升到4096。对话上也是使用100万人类标记的数据微调。\n\n由于 Llama 2 本身的中文对齐比较弱，开发者采用了中文指令集来进行微调，使其具备较强的中文对话能力。目前这个中文微调参数模型总共发布了 7B，13B两种参数大小。\n\nLlama 2 chat chinese fine-tuned model\n\nThis model is fine-tuned based on Meta Platform's Llama 2 Chat open source model. According to Meta, Llama 2 is trained on 2 trillion tokens, and the context length is increased to 4096. The chat model is fine-tuned using 1 million human labeled data.\n\nSince the Chinese alignment of Llama 2 itself is relatively weak, the developer, adopted a Chinese instruction set for fine-tuning to improve the Chinese dialogue ability.\n\nThe Chinese fine-tuned models are available in 7B and 13B parameter sizes.\n\n### CLI\n\nOpen the terminal and run `ollama run llama2-chinese`\n\n### API\n\nRun the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2-chinese:7b-chat-q4_0\",\n  \"prompt\":\"为什么天空是蓝色的\"\n }'\n  ```\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n\n\n## Reference\n[FlagAlpha](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat)\n\n\n[FlagAlpha](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat)\n\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "llama2-chinese:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama2-chinese:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Chat Assistant"
    ],
    "domain": "Language",
    "ai_languages": [
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for Chinese language tasks.",
    "pulls": 435000,
    "tags": 35,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d9b3689d-59fc-40ad-9fde-44776ac74c1a",
    "model_identifier": "stable-code",
    "model_name": "stable-code",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/stable-code",
    "description": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nStable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as Code Llama 7b that are 2.5x larger.\n\n\nKey Features\n\n\n\n\nNEW\n instruct model \nollama run stable-code\n\n\nFill in Middle Capability (FIM)\n\n\nSupports Long Context, trained with Sequences upto 16,384\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\n\nSize\n\n\nPython\n\n\nC++\n\n\nJavascript\n\n\nJava\n\n\nPHP\n\n\nRust\n\n\n\n\n\n\n\n\n\n\nStable Code\n\n\n3B\n\n\n32.4%\n\n\n30.9%\n\n\n32.1%\n\n\n32.1%\n\n\n24.2%\n\n\n23.0%\n\n\n\n\n\n\nCodeLLama\n\n\n7B\n\n\n30.0%\n\n\n28.2%\n\n\n32.5%\n\n\n31.1%\n\n\n25.7%\n\n\n26.3%\n\n\n\n\n\n\nDeepseek Coder\n\n\n1.3B\n\n\n28.6%\n\n\n29.2%\n\n\n28.7%\n\n\n29.0%\n\n\n23.6%\n\n\n18.5%\n\n\n\n\n\n\nWizard Coder\n\n\n3B\n\n\n31.6%\n\n\n25.6%\n\n\n26.2%\n\n\n25.8%\n\n\n25.3%\n\n\n20.4%\n\n\n\n\n\n\nStarCoder\n\n\n3B\n\n\n21.6%\n\n\n19.8%\n\n\n21.5%\n\n\n20.5%\n\n\n19.0%\n\n\n16.9%\n\n\n\n\n\n\nReplit Code V1.5\n\n\n3B\n\n\n23.0%\n\n\n25.9%\n\n\n26.2%\n\n\n23.6%\n\n\n23.2%\n\n\n21.5%\n\n\n\n\n\n\nDeci Coder\n\n\n1B\n\n\n19.1%\n\n\n6.8%\n\n\n18.4%\n\n\n16.7%\n\n\n2.1%\n\n\n1.7%\n\n\n\n\n\n\n\n\nModel Details\n\n\n\n\nDeveloped by\n: \nStability AI\n\n\nModel type\n: stable-code models are auto-regressive language models based on the transformer decoder architecture.\n\n\nLanguage(s)\n: English, Code\n\n\nContact\n: For questions and comments about the model, please email \nlm@stability.ai\n\n\n\n\nModel Architecture\n\n\nThe model is a decoder-only transformer similar to the LLaMA (\nTouvron et al., 2023\n) architecture with the following modifications:\n\n\n\n\n\n\n\n\nParameters\n\n\nHidden Size\n\n\nLayers\n\n\nHeads\n\n\nSequence Length\n\n\n\n\n\n\n\n\n\n\n2,796,431,360\n\n\n2560\n\n\n32\n\n\n32\n\n\n16384\n\n\n\n\n\n\n\n\n\n\nPosition Embeddings\n: Rotary Position Embeddings (\nSu et al., 2021\n) applied to the first 25% of head embedding dimensions for improved throughput following \nBlack et al. (2022)\n.\n\n\nTokenizer\n: We use a modified version of the GPTNeoX Tokenizer.\nNeoX\n. We add special tokens to train for Fill in the Middle (FIM) capabilities like \n<FIM_PREFIX>\n and \n<FIM_SUFFIX>\n along with other special tokens.\n\n\n\n\nTraining\n\n\nTraining Dataset\n\n\nThe dataset is comprised of a filtered mixture of open-source large-scale datasets available on the \nHuggingFace Hub\n: Falcon RefinedWeb extract (\nPenedo et al., 2023\n), along with \nCommitPackFT\n and \nGithub Issues\n (BigCode., 2023), and StarCoder (\nLi et al., 2023\n). We further supplement our training with data from mathematical domains (\nAzerbayev, Zhangir, et al., 2023\n and, \nYu, Longhui, et al., 2023\n).\n\n\nTop 18 programming languages trained on:\n- C\n- CPP\n- Java\n- JavaScript\n- CSS\n- Go\n- HTML\n- Ruby\n- Rust\n- Markdown\n- Shell\n- Php\n- Sql\n- R\n- Typescript\n- Python\n- Jupyter-Clean\n- RestructuredText\n\n\nUse and Limitations\n\n\nIntended Use\n\n\nThe model is intended to be used as a foundational base model for application-specific fine-tuning. Developers must evaluate and fine-tune the model for safe performance in downstream applications.\n\n\nLimitations and Bias\n\n\n​\nAs a base model, this model may exhibit unreliable, unsafe, or other undesirable behaviors that must be corrected through evaluation and fine-tuning prior to deployment. The pre-training dataset may have contained offensive or inappropriate content, even after applying data cleansing filters, which can be reflected in the model-generated text. We recommend that users exercise caution when using these models in production systems. Do not use the models if they are unsuitable for your application, or for any applications that may cause deliberate or unintentional harm to others.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as Code Llama 7b that are 2.5x larger.\n\n**Key Features**\n\n* **NEW** instruct model `ollama run stable-code`\n* Fill in Middle Capability (FIM)\n* Supports Long Context, trained with Sequences upto 16,384\n\n![spiderchart](https://github.com/jmorganca/ollama/assets/3325447/6c3de7a5-5e10-4884-81fb-3a1b3f566609)\n\n| Model            | Size | Python | C++  | Javascript | Java | PHP  | Rust |\n|------------------|------|--------|------|------------|------|------|------|\n| **Stable Code**  | 3B   | 32.4%  | 30.9%| 32.1%      | 32.1%| 24.2%| 23.0%|\n| CodeLLama        | 7B   | 30.0%  | 28.2%| 32.5%      | 31.1%| 25.7%| 26.3%|\n| Deepseek Coder   | 1.3B | 28.6%  | 29.2%| 28.7%      | 29.0%| 23.6%| 18.5%|\n| Wizard Coder     | 3B   | 31.6%  | 25.6%| 26.2%      | 25.8%| 25.3%| 20.4%|\n| StarCoder        | 3B   | 21.6%  | 19.8%| 21.5%      | 20.5%| 19.0%| 16.9%|\n| Replit Code V1.5 | 3B   | 23.0%  | 25.9%| 26.2%      | 23.6%| 23.2%| 21.5%|\n| Deci Coder       | 1B   | 19.1%  | 6.8% | 18.4%      | 16.7%| 2.1% | 1.7% |\n\n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: stable-code models are auto-regressive language models based on the transformer decoder architecture.\n* **Language(s)**: English, Code\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Model Architecture\n\nThe model is a decoder-only transformer similar to the LLaMA ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288)) architecture with the following modifications:\n\n| Parameters     | Hidden Size | Layers | Heads | Sequence Length |\n|----------------|-------------|--------|-------|-----------------|\n| 2,796,431,360  | 2560        | 32     | 32    | 16384            |\n\n* **Position Embeddings**: Rotary Position Embeddings ([Su et al., 2021](https://arxiv.org/abs/2104.09864)) applied to the first 25% of head embedding dimensions for improved throughput following [Black et al. (2022)](https://arxiv.org/pdf/2204.06745.pdf).\n* **Tokenizer**: We use a modified version of the GPTNeoX Tokenizer.[`NeoX`](https://github.com/EleutherAI/gpt-neox). We add special tokens to train for Fill in the Middle (FIM) capabilities like `<FIM_PREFIX>` and `<FIM_SUFFIX>` along with other special tokens.\n\n## Training\n\n### Training Dataset\n\nThe dataset is comprised of a filtered mixture of open-source large-scale datasets available on the [HuggingFace Hub](https://huggingface.co/datasets): Falcon RefinedWeb extract ([Penedo et al., 2023](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)), along with [CommitPackFT](https://huggingface.co/datasets/bigcode/commitpackft) and [Github Issues](https://huggingface.co/datasets/bigcode/the-stack-github-issues) (BigCode., 2023), and StarCoder ([Li et al., 2023](https://arxiv.org/abs/2305.06161)). We further supplement our training with data from mathematical domains ([Azerbayev, Zhangir, et al., 2023](https://arxiv.org/abs/2310.10631) and, [Yu, Longhui, et al., 2023](https://arxiv.org/abs/2309.12284)). \n\nTop 18 programming languages trained on:\n- C\n- CPP\n- Java\n- JavaScript\n- CSS\n- Go\n- HTML\n- Ruby\n- Rust\n- Markdown\n- Shell\n- Php\n- Sql\n- R\n- Typescript\n- Python\n- Jupyter-Clean\n- RestructuredText\n\n## Use and Limitations\n\n### Intended Use\n\nThe model is intended to be used as a foundational base model for application-specific fine-tuning. Developers must evaluate and fine-tune the model for safe performance in downstream applications.\n\n### Limitations and Bias\n​\nAs a base model, this model may exhibit unreliable, unsafe, or other undesirable behaviors that must be corrected through evaluation and fine-tuning prior to deployment. The pre-training dataset may have contained offensive or inappropriate content, even after applying data cleansing filters, which can be reflected in the model-generated text. We recommend that users exercise caution when using these models in production systems. Do not use the models if they are unsuitable for your application, or for any applications that may cause deliberate or unintentional harm to others.\n\n## References\n\n[Hugging Face](https://huggingface.co/stabilityai/stable-code-3b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "stable-code:latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working with code.",
    "pulls": 428800,
    "tags": 36,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "7b949eb6-aa8a-4359-8d4b-396e931e6619",
    "model_identifier": "neural-chat",
    "model_name": "neural-chat",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/neural-chat",
    "description": "A fine-tuned model based on Mistral with good coverage of domain and language.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNeuralChat is a fine-tuned model released by Intel that’s based on \nMistral\n, designed to be used for high-performance chatbot applications.\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/18ecaefd-f70e-4d16-96b1-937d53e0faa5\" width=\"240\" />\n\nNeuralChat is a fine-tuned model released by Intel that's based on [Mistral](https://ollama.ai/library/mistral), designed to be used for high-performance chatbot applications.\n\n## References\n\n[HuggingFace](https://huggingface.co/Intel/neural-chat-7b-v3-1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "neural-chat:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "High-performance chatbot applications and text summarization tasks.",
    "pulls": 427800,
    "tags": 50,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9f8d632b-38f2-46b2-a881-e8ddd86b71c1",
    "model_identifier": "nous-hermes2",
    "model_name": "nous-hermes2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nous-hermes2",
    "description": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNous Hermes 2 is a new iteration of the \nNous Hermes\n model.\n\n\nThis model was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. It is the most powerful Nous Hermes model to date, and surpasses many of the most popular models in the world in the GPT4All, AGIEval, BigBench and other benchmarks.\n\n\nVersions\n\n\n\n\n\n\n\n\n10.7b\n \nlatest\n\n\n01/01/2024\n\n\nA 10.7b model based on \nSolar\n. A major improvement across the board on benchmarks compared to the base Solar 10.7B model, and comes close to approaching the 34B Yi model\n\n\n\n\n\n\n34b\n\n\n12/25/2023\n\n\nThe original Nous Hermes 2 34B model based on \nYi\n\n\n\n\n\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/4cf930ce-95f8-4a01-ad49-48db41ada870\" width=\"300\" />\n\nNous Hermes 2 is a new iteration of the [Nous Hermes](https://ollama.ai/library/nous-hermes) model.\n\nThis model was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. It is the most powerful Nous Hermes model to date, and surpasses many of the most popular models in the world in the GPT4All, AGIEval, BigBench and other benchmarks.\n\n## Versions\n\n| ---------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `10.7b` `latest` | 01/01/2024 | A 10.7b model based on [Solar](https://ollama.ai/library/solar). A major improvement across the board on benchmarks compared to the base Solar 10.7B model, and comes close to approaching the 34B Yi model |\n| `34b`            | 12/25/2023 | The original Nous Hermes 2 34B model based on [Yi](https://ollama.ai/library/yi)                                                                                                                                                                                         |\n\n\n\n\n## References\n\n[HuggingFace](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "10.7b",
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "nous-hermes2:latest",
        "size": "6.1GB",
        "size_gb": 6.1,
        "recommended_ram_gb": 7.6,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "6.1GB",
        "size_gb": 6.1,
        "recommended_ram_gb": 7.6,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "nous-hermes2:34b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 6.1,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex text generation tasks.",
    "pulls": 423200,
    "tags": 33,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "307d3790-315d-4cdf-a0ae-3e2332034dbc",
    "model_identifier": "opencoder",
    "model_name": "opencoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/opencoder",
    "description": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOpenCoder\n is an open and reproducible code LLM family which includes 1.5B and 8B  models, supporting both English and Chinese languages. Starting from scratch, OpenCoder is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples, finally reaching the performance of top-tier code LLMs. We provide not only model weights and inference code, but also the reproducible training data, the complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols. Empowering researchers to build and innovate, OpenCoder is your open foundation for advancing code AI.\n\n\n\n\nComplete Open Source\n: OpenCoder ensures full transparency by releasing not only the model weights and forthcoming inference code but also the complete data-cleaning code for training. This release includes high-quality synthetic data, an extensive set of checkpoints, and a dataset of over 4.5 million supervised fine-tuning (SFT) entries, making OpenCoder one of the most comprehensively open-sourced models available.\n\n\nComprehensive Experimental Analysis\n: OpenCoder is rigorously tested through extensive ablation studies on various data-cleaning strategies and training processes, including file-level and repository-level deduplication experiments, ensuring thorough exploration and validation of the model’s performance.\n\n\nHigh-Quality Synthetic Data\n: OpenCoder provides a fully developed synthetic data generation process and over 4.5 million SFT data entries, establishing a robust data foundation for model training and evaluation.\n\n\nExceptional Performance\n: OpenCoder achieves high performance across multiple language model benchmarks, positioning it among the leading open-source models for code.\n\n\n\n\nReferences\n\n\nGitHub\n\n\nPaper\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/opencoder/6078034f-fdbf-47c2-9b63-69ce506c0225\" width=\"280\" />\n\n**OpenCoder** is an open and reproducible code LLM family which includes 1.5B and 8B  models, supporting both English and Chinese languages. Starting from scratch, OpenCoder is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples, finally reaching the performance of top-tier code LLMs. We provide not only model weights and inference code, but also the reproducible training data, the complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols. Empowering researchers to build and innovate, OpenCoder is your open foundation for advancing code AI. \n\n- **Complete Open Source**: OpenCoder ensures full transparency by releasing not only the model weights and forthcoming inference code but also the complete data-cleaning code for training. This release includes high-quality synthetic data, an extensive set of checkpoints, and a dataset of over 4.5 million supervised fine-tuning (SFT) entries, making OpenCoder one of the most comprehensively open-sourced models available.\n- **Comprehensive Experimental Analysis**: OpenCoder is rigorously tested through extensive ablation studies on various data-cleaning strategies and training processes, including file-level and repository-level deduplication experiments, ensuring thorough exploration and validation of the model’s performance.\n- **High-Quality Synthetic Data**: OpenCoder provides a fully developed synthetic data generation process and over 4.5 million SFT data entries, establishing a robust data foundation for model training and evaluation.\n- **Exceptional Performance**: OpenCoder achieves high performance across multiple language model benchmarks, positioning it among the leading open-source models for code.\n\n## References\n\n[GitHub](https://github.com/OpenCoder-llm/OpenCoder-llm)\n\n[Paper](https://arxiv.org/pdf/2411.04905)\n\n[Hugging Face](https://huggingface.co/collections/infly/opencoder-672cec44bbb86c39910fb55e)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.5b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "opencoder:1.5b",
        "size": "1.4GB",
        "size_gb": 1.4,
        "recommended_ram_gb": 1.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "opencoder:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.4,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Developers working with code in English and Chinese.",
    "pulls": 417800,
    "tags": 9,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0585ebdf-8710-47c4-ad54-b1f07daba630",
    "model_identifier": "sqlcoder",
    "model_name": "sqlcoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/sqlcoder",
    "description": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSQLCoder is a 15B parameter model that is fine-tuned on a base StarCoder model. It slightly outperforms gpt-3.5-turbo for natural language to SQL generation tasks on the sql-eval framework, and outperforms popular open-source models. It also significantly outperforms text-davinci-003, a model that’s more than 10 times its size.\n\n\nThis 15B completion model generally requires at least 16GB of RAM.\n\n\nUsage\n\n\nCLI\n\n\nollama run sqlcoder \n\n\n\nTry the following prompt using triple quotes for multi-line input:\n\n\nNote: change the {question} to a SQL question you want to answer. Ex.) ‘which products generate the most sales.`\n\n\n\"\"\" \n### Instructions:\nYour task is to convert a question into a SQL query, given a Postgres database schema.\nAdhere to these rules:\n- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n- When creating a ratio, always cast the numerator as float\n\n### Input:\nGenerate a SQL query that answers the question `{question}`.\nThis query will run on a database whose schema is represented in this string:\nCREATE TABLE products (\n  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n  name VARCHAR(50), -- Name of the product\n  price DECIMAL(10,2), -- Price of each unit of the product\n  quantity INTEGER  -- Current quantity in stock\n);\n\nCREATE TABLE customers (\n   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n   name VARCHAR(50), -- Name of the customer\n   address VARCHAR(100) -- Mailing address of the customer\n);\n\nCREATE TABLE salespeople (\n  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n  name VARCHAR(50), -- Name of the salesperson\n  region VARCHAR(50) -- Geographic sales region\n);\n\nCREATE TABLE sales (\n  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n  product_id INTEGER, -- ID of product sold\n  customer_id INTEGER,  -- ID of customer who made purchase\n  salesperson_id INTEGER, -- ID of salesperson who made the sale\n  sale_date DATE, -- Date the sale occurred\n  quantity INTEGER -- Quantity of product sold\n);\n\nCREATE TABLE product_suppliers (\n  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n  product_id INTEGER, -- Product ID supplied\n  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n);\n\n-- sales.product_id can be joined with products.product_id\n-- sales.customer_id can be joined with customers.customer_id\n-- sales.salesperson_id can be joined with salespeople.salesperson_id\n-- product_suppliers.product_id can be joined with products.product_id\n\n### Response:\nBased on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n```sql\n\"\"\"\n\n\n\nExample output:\n\n\nSELECT customers.customer_id,\n       products.product_id,\n       (SUM(sales.quantity * product_suppliers.supply_price)) / (SELECT \nSUM(quantity)\n                                                                FROM sales\n                                                                WHERE \nsales.salesperson_id = '{chosen_salesperson}') as profit\nFROM   sales\n  INNER JOIN customers on sales.customer_id = customers.customer_id\n  INNER JOIN products on sales.product_id = products.product_id\n  INNER JOIN product_suppliers on products.product_id = \nproduct_suppliers.product_id\nWHERE  sales.salesperson_id = '{chosen_salesperson}'\nGROUP BY customers.customer_id, products.product_id\nHAVING COUNT(DISTINCT sales.sale_date) >= {min_days}\n  AND (SELECT SUM(quantity * product_suppliers.supply_price)\n     FROM   sales\n     WHERE  sales.salesperson_id = '{chosen_salesperson}'\n        and sales.customer_id = customers.customer_id\n        and sales.product_id = products.product_id) >= {min_total}\nORDER BY profit DESC;\n```\n\n\n### Reasoning\n\nThe query is broken down into two parts: the first one fetches information\nabout each sale made by a specific salesman (`{chosen_salesperson}`), and \ngroups it by customer ID, product ID, and calculates how much money was \nearned from each sale. The second part then filters out these results that\noccurred after a certain number of days (determined by the variable \n`min_days`) and had an amount of money equal to or more than a certain \nvalue (determined by the variable `min_total`). Finally, the query orders \nthe results in descending order based on the ratio between earned money \nand total money.\n\nThe first part relies on several tables from the database: salespeople, \nproducts, customers, sales, and product_suppliers. The salespeople table \ncontains information about each salesperson's ID; the products table \ncontains information about each product's name and price; the customers \ntable contains information about each customer's address; the sales table \ncontains information about each sale, including the quantity sold, the ID \nof the product sold, and the ID of the customer who made the purchase; and\nthe product_suppliers table contains information about the unit price \ncharged by each supplier.\n\nThe variables in the query can be interpreted as follows:\n\n- `{chosen_salesperson}` is a placeholder for the name of the salesman \nwhose performance is being evaluated. This value should be replaced with \nan actual salesman's name or ID number.\n- `min_days` is the minimum number of days that must have passed since the\nsale in order to qualify for inclusion in the calculation. For example, if\nthis variable were set to 30, then only sales made 30 days after the start\ndate would be included in the calculation. This value should also be \nreplaced with an integer representing the minimum number of days.\n- `min_total` is a threshold amount that must have been earned by the \nsalesman for the sale to qualify for inclusion in the calculation. For \nexample, if this variable were set to 1000, then only sales worth at least\n$1000 would be included in the calculation. This value should also be \nreplaced with an integer representing the minimum amount of money that \nmust have been earned.\n\nThe output is a table containing three columns: customer ID, product ID, \nand profit per unit price (calculated as the total money earned divided by\nthe quantity sold). The rows in this table are sorted in descending order \nbased on the ratio between earned money and total money. Rows that don't \nmeet both of these conditions will be dropped from the output.\n\n## Example\n\n```\nInput:\nmin_days = 30\nmin_total = 1000\nchosen_salesperson = '<NAME>'\n\nOutput:\n  customer_id | product_id |         profit\n-------------+----------------------------+-----------------\n  987         |     456    | 2.1             \n  1029        |    1357    | 1.91            \n  89          |    1234    | 0.59            \n```\n\n## License\n\nThe database is released under the [Open Database \nLicense](https://opendatacommons.org/licenses/odbl/1-0/).\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://docs.defog.ai/img/logo.svg\" width=\"330px\" />\n\n\nSQLCoder is a 15B parameter model that is fine-tuned on a base StarCoder model. It slightly outperforms gpt-3.5-turbo for natural language to SQL generation tasks on the sql-eval framework, and outperforms popular open-source models. It also significantly outperforms text-davinci-003, a model that's more than 10 times its size. \n\nThis 15B completion model generally requires at least 16GB of RAM. \n\n## Usage\n\n### CLI\n\n```\nollama run sqlcoder \n```\n\nTry the following prompt using triple quotes for multi-line input: \n\nNote: change the {question} to a SQL question you want to answer. Ex.) 'which products generate the most sales.` \n\n```\n\"\"\" \n### Instructions:\nYour task is to convert a question into a SQL query, given a Postgres database schema.\nAdhere to these rules:\n- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n- When creating a ratio, always cast the numerator as float\n\n### Input:\nGenerate a SQL query that answers the question `{question}`.\nThis query will run on a database whose schema is represented in this string:\nCREATE TABLE products (\n  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n  name VARCHAR(50), -- Name of the product\n  price DECIMAL(10,2), -- Price of each unit of the product\n  quantity INTEGER  -- Current quantity in stock\n);\n\nCREATE TABLE customers (\n   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n   name VARCHAR(50), -- Name of the customer\n   address VARCHAR(100) -- Mailing address of the customer\n);\n\nCREATE TABLE salespeople (\n  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n  name VARCHAR(50), -- Name of the salesperson\n  region VARCHAR(50) -- Geographic sales region\n);\n\nCREATE TABLE sales (\n  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n  product_id INTEGER, -- ID of product sold\n  customer_id INTEGER,  -- ID of customer who made purchase\n  salesperson_id INTEGER, -- ID of salesperson who made the sale\n  sale_date DATE, -- Date the sale occurred\n  quantity INTEGER -- Quantity of product sold\n);\n\nCREATE TABLE product_suppliers (\n  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n  product_id INTEGER, -- Product ID supplied\n  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n);\n\n-- sales.product_id can be joined with products.product_id\n-- sales.customer_id can be joined with customers.customer_id\n-- sales.salesperson_id can be joined with salespeople.salesperson_id\n-- product_suppliers.product_id can be joined with products.product_id\n\n### Response:\nBased on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n```sql\n\"\"\"\n```\n\n**Example output:** \n\n````\nSEL",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "15b"
    ],
    "memory_requirements": [
      {
        "tag": "sqlcoder:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "sqlcoder:15b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Data Scientists for SQL generation tasks.",
    "pulls": 416900,
    "tags": 48,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ce0094ba-a562-4fc0-90d9-38cdc914f45a",
    "model_identifier": "wizardcoder",
    "model_name": "wizardcoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizardcoder",
    "description": "State-of-the-art code generation model",
    "readme": "Readme\n\n\n\n\n\n\n\n\nWizard Coder is a code generation model based on \nCode Llama\n.\n\n\nVersions\n\n\n\n\n\n\n\n\nTag\n\n\nDate\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n33b\n\n\n01/042024\n\n\nA new 33B model trained from \nDeepseek Coder\n\n\n\n\n\n\npython\n\n\n09/7/2023\n\n\nInitial release in 7B, 13B and 34B sizes based on \nCode Llama\n\n\n\n\n\n\n\n\nReferences\n\n\nHugging Face\n\n\nWizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWizard Coder is a code generation model based on [Code Llama](https://ollama.ai/library/codellama).\n## Versions\n\n| Tag      | Date      | Notes                                                                                                |\n| -------- | --------- | ---------------------------------------------------------------------------------------------------- |\n| `33b`    | 01/042024 | A new 33B model trained from [Deepseek Coder](https://ollama.ai/library/deepseek-coder/)             |\n| `python` | 09/7/2023 | Initial release in 7B, 13B and 34B sizes based on [Code Llama](https://ollama.ai/library/codellama/) |\n\n\n## References\n\n[Hugging Face](https://huggingface.co/WizardLM/WizardCoder-33B-V1.1)\n\n[WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568 )\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "33b"
    ],
    "memory_requirements": [
      {
        "tag": "wizardcoder:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "wizardcoder:33b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for code generation tasks.",
    "pulls": 416500,
    "tags": 67,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d3b9a275-aac2-4f91-aa7e-fcf76f1a15d3",
    "model_identifier": "yi-coder",
    "model_name": "yi-coder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/yi-coder",
    "description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nYi-Coder is excelling in long-context understanding with a maximum context length of 128K tokens. It supports 52 major programming languages.\n\n\nSizes\n\n\n\n\n9B: \nollama run yi-coder:9b\n\n\n1.5B: \nollama run yi-coder:1.5b\n\n\n\n\nUsage\n\n\nChat\n\n\nollama run yi-coder\n\n\n\nCode completion\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"yi-coder\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\n\n\n\nReferences\n\n\nHugging Face\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/yi-coder/8bca5a55-79c7-4012-9d9b-371722b67d85\" width=\"200\" />\n\nYi-Coder is excelling in long-context understanding with a maximum context length of 128K tokens. It supports 52 major programming languages.\n\n## Sizes\n- 9B: `ollama run yi-coder:9b`\n- 1.5B: `ollama run yi-coder:1.5b`\n\n## Usage\n\n### Chat\n\n```\nollama run yi-coder\n```\n\n### Code completion\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"yi-coder\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\n```\n\n\n## References\n\n[Hugging Face](https://huggingface.co/01-ai/Yi-Coder-9B-Chat)\n\n[GitHub](https://github.com/01-ai/Yi-Coder)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.5b",
      "9b"
    ],
    "memory_requirements": [
      {
        "tag": "yi-coder:latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 5.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working with code generation and function calls.",
    "pulls": 409900,
    "tags": 67,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0d43284f-4e3a-4fae-a952-78def547adc8",
    "model_identifier": "stablelm2",
    "model_name": "stablelm2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/stablelm2",
    "description": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nStable LM 2 1.6B is a state-of-the-art 1.6 and 12B billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.\n\n\nThe model is trained on a mix of publicly available datasets and synthetic datasets, utilizing Direct Preference Optimization (DPO).\n\n\nReferences\n\n\nAnnouncement\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/62507173-9423-4496-87ea-7d48be78979c\" width=\"480\" />\n\nStable LM 2 1.6B is a state-of-the-art 1.6 and 12B billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.\n\nThe model is trained on a mix of publicly available datasets and synthetic datasets, utilizing Direct Preference Optimization (DPO).\n\n## References\n\n[Announcement](https://stability.ai/news/introducing-stable-lm-2)\n\n[HuggingFace](https://huggingface.co/stabilityai)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.6b",
      "12b"
    ],
    "memory_requirements": [
      {
        "tag": "stablelm2:12b",
        "size": "7.0GB",
        "size_gb": 7.0,
        "recommended_ram_gb": 8.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 7.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text generation and summarization tasks.",
    "pulls": 403700,
    "tags": 84,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "43b20f09-d695-45ac-a4e5-e5cc04161908",
    "model_identifier": "llama3-chatqa",
    "model_name": "llama3-chatqa",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3-chatqa",
    "description": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nChatQA-1.5 is built on top of the Llama-3 base model, and incorporates conversational QA data to enhance its tabular and arithmetic calculation capability.\n\n\nChatQA-1.5 has two variants:\n\n\n\n\nLlama3-ChatQA-1.5-8B \nllama3-chatqa:8b\n\n\nLlama3-ChatQA-1.5-70B \nllama3-chatqa:70b\n\n\n\n\nReferences\n\n\nWebsite\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/llama3-chatqa/835859df-86ec-4859-aab8-c5d6b7195b7c\" width=\"320\" />\n\nChatQA-1.5 is built on top of the Llama-3 base model, and incorporates conversational QA data to enhance its tabular and arithmetic calculation capability.\n\nChatQA-1.5 has two variants:\n\n- Llama3-ChatQA-1.5-8B `llama3-chatqa:8b`\n- Llama3-ChatQA-1.5-70B `llama3-chatqa:70b`\n\n## References\n\n[Website](https://chatqa-project.github.io/)\n\n[Hugging Face](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3-chatqa:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "llama3-chatqa:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for conversational QA tasks.",
    "pulls": 401400,
    "tags": 35,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "bf512752-e194-496b-9de5-315b7418eda0",
    "model_identifier": "granite3-dense",
    "model_name": "granite3-dense",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3-dense",
    "description": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGranite dense models\n\n\nThe IBM Granite \n2B\n and \n8B\n models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing. Granite-8B-Instruct now rivals Llama 3.1 8B-Instruct across both OpenLLM Leaderboard v1 and OpenLLM Leaderboard v2 benchmarks.\n\n\nThey are designed to support tool-based use cases\n and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n\nParameter Sizes\n\n\n2B:\n\n\nollama run granite3-dense:2b\n\n\n8B:\n\n\nollama run granite3-dense:8b\n\n\nSupported Languages\n\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\n\n\nCapabilities\n\n\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related\n\n\nFunction-calling\n\n\nMultilingual dialog use cases\n\n\n\n\nGranite mixture of experts models\n\n\nThe Granite mixture of experts models are available in \n1B and 3B\n parameter sizes designed for \nlow latency usage\n.\n\n\nSee model page\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nGitHub Repository:\n \nibm-granite/granite-3.0-language-models\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: October 21st, 2024\n\n\nLicense:\n \nApache 2.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![An illustration of Ollama holding a beautiful flower with the IBM Rebus logo of the Eye, Bee and M, made by Paul Rand.](https://ollama.com/assets/library/granite3-dense/e2955da9-fee8-45d1-81ed-fe313f4e83eb)\n\n\n### Granite dense models\n\nThe IBM Granite **2B** and **8B** models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM's initial testing. Granite-8B-Instruct now rivals Llama 3.1 8B-Instruct across both OpenLLM Leaderboard v1 and OpenLLM Leaderboard v2 benchmarks.\n\n**They are designed to support tool-based use cases** and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n## Parameter Sizes\n\n**2B:**\n  \n`ollama run granite3-dense:2b`\n\n**8B:**\n\n`ollama run granite3-dense:8b`\n\n## Supported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) \n\n### Capabilities\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related \n* Function-calling\n* Multilingual dialog use cases\n\n\n### Granite mixture of experts models\n\nThe Granite mixture of experts models are available in **1B and 3B** parameter sizes designed for **low latency usage**.\n\n[See model page](https://ollama.com/library/granite3-moe) \n\n### Learn more\n\n- **Developers:** IBM Research\n- **GitHub Repository:** [ibm-granite/granite-3.0-language-models](https://github.com/ibm-granite/granite-3.0-language-models)\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: October 21st, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "2b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3-dense:latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "granite3-dense:8b",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Code Generation",
      "RAG / Retrieval",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "Spanish",
      "French",
      "Japanese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for code generation, translation, and RAG tasks.",
    "pulls": 399600,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1e145441-c3ed-44d0-949a-3f4db5f8eb85",
    "model_identifier": "granite3.1-dense",
    "model_name": "granite3.1-dense",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3.1-dense",
    "description": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nGranite dense models\n\n\nThe IBM Granite \n2B\n and \n8B\n models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.\n\n\nThey are designed to support tool-based use cases\n and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n\nParameter Sizes\n\n\n2B:\n\n\nollama run granite3.1-dense:2b\n\n\n8B:\n\n\nollama run granite3.1-dense:8b\n\n\nSupported Languages\n\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\n\n\nCapabilities\n\n\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related tasks\n\n\nFunction-calling tasks\n\n\nMultilingual dialog use cases\n\n\nLong-context tasks including long document/meeting summarization, long document QA, etc.\n\n\n\n\nGranite mixture of experts models\n\n\nThe Granite mixture of experts models are available in \n1B and 3B\n parameter sizes designed for \nlow latency usage\n.\n\n\nSee model page\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nGitHub Repository:\n \nibm-granite/granite-language-models\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: December 18th, 2024\n\n\nLicense:\n \nApache 2.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Granite dense models\n\nThe IBM Granite **2B** and **8B** models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM's initial testing.\n\n**They are designed to support tool-based use cases** and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n### Parameter Sizes\n\n**2B:**\n  \n`ollama run granite3.1-dense:2b`\n\n**8B:**\n\n`ollama run granite3.1-dense:8b`\n\n### Supported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) \n\n### Capabilities\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related tasks\n* Function-calling tasks\n* Multilingual dialog use cases\n* Long-context tasks including long document/meeting summarization, long document QA, etc.\n\n## Granite mixture of experts models\n\nThe Granite mixture of experts models are available in **1B and 3B** parameter sizes designed for **low latency usage**.\n\n[See model page](https://ollama.com/library/granite3-moe) \n\n## Learn more\n\n- **Developers:** IBM Research\n- **GitHub Repository:** [ibm-granite/granite-language-models](https://github.com/ibm-granite/granite-3.1-language-models)\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: December 18th, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "2b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3.1-dense:2b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "granite3.1-dense:latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "Spanish",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text-based tasks.",
    "pulls": 398400,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "444cfc60-6c62-4c6c-a138-28fcd7c0da0b",
    "model_identifier": "wizard-math",
    "model_name": "wizard-math",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizard-math",
    "description": "Model focused on math and logic problems",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNow updated to WizardMath 7B v1.1: \nollama pull wizard-math\n. This new version is trained from Mistral-7B and achieves even higher benchmark scores than previous versions.\n\n\n\n\nWizardMath was released by WizardLM. It is trained on the GSM8k dataset, and targeted at math questions. It is available in 7B, 13B, and 70B parameter sizes.\n\n\nExample prompt\n\n\nHow many 4-digit numbers have the last digit equal to the sum of the first two digits?\n\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/263121166-8b16834a-1da2-4a68-8aac-13d19775e7d7.png\" style=\"max-width:60%\" />\n\n> Now updated to WizardMath 7B v1.1: `ollama pull wizard-math`. This new version is trained from Mistral-7B and achieves even higher benchmark scores than previous versions.\n\nWizardMath was released by WizardLM. It is trained on the GSM8k dataset, and targeted at math questions. It is available in 7B, 13B, and 70B parameter sizes.\n\n## Example prompt\n\n```\nHow many 4-digit numbers have the last digit equal to the sum of the first two digits?\n```\n\n**References**\n\n[HuggingFace](https://huggingface.co/WizardLM/WizardMath-7B-V1.0)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "wizard-math:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "wizard-math:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizard-math:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Question Answering"
    ],
    "domain": "Math",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Users needing a math-focused AI model for problem-solving and logical reasoning.",
    "pulls": 388600,
    "tags": 64,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4e569cae-7d61-43ed-b87a-c37f60f91c9d",
    "model_identifier": "dolphincoder",
    "model_name": "dolphincoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphincoder",
    "description": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nBased on the StarCoder2 7B and 15B models, this Dolphin fine-tune excels at coding.\n\n\nReference\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/6a5a5656-abb9-4235-99e0-7f1161ff2657\" width=\"320\" />\n\nBased on the StarCoder2 7B and 15B models, this Dolphin fine-tune excels at coding.\n\n## Reference\n\n[HuggingFace](https://huggingface.co/cognitivecomputations)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "15b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphincoder:latest",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      },
      {
        "tag": "dolphincoder:15b",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 4.2,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers focusing on coding tasks.",
    "pulls": 388600,
    "tags": 35,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "22137b06-e9b5-4f22-9cf3-82489b631d1c",
    "model_identifier": "llama3-gradient",
    "model_name": "llama3-gradient",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3-gradient",
    "description": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThis model extends LLama-3 8B’s context length from 8k to > 1040K, developed by Gradient, sponsored by compute from Crusoe Energy. It demonstrates that SOTA LLMs can learn to operate on long context with minimal training by appropriately adjusting RoPE theta. We trained on 830M tokens for this stage, and 1.4B tokens total for all stages, which is < 0.01% of Llama-3’s original pre-training data.\n\n\nLarge Context Window\n\n\n\n\nNote: using a 256k context window requires at least 64GB of memory. Using a 1M+ context window requires significantly more (100GB+).\n\n\n\n\nTo extend the context window (to 256k for example) use:\n\n\nAPI\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3-gradient\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 256000\n  }\n}'\n\n\n\nCLI\n\n\nollama run llama3-gradient\n>>> /set parameter num_ctx 256000\n\n\n\nReferences\n\n\nWebsite\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/llama3-gradient/11962db3-2750-4346-b89c-b9b5612cc76a\" width=\"360\" />\n\nThis model extends LLama-3 8B's context length from 8k to > 1040K, developed by Gradient, sponsored by compute from Crusoe Energy. It demonstrates that SOTA LLMs can learn to operate on long context with minimal training by appropriately adjusting RoPE theta. We trained on 830M tokens for this stage, and 1.4B tokens total for all stages, which is < 0.01% of Llama-3's original pre-training data.\n\n## Large Context Window\n\n> Note: using a 256k context window requires at least 64GB of memory. Using a 1M+ context window requires significantly more (100GB+).\n\nTo extend the context window (to 256k for example) use:\n\n<sub>API</sub>\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3-gradient\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 256000\n  }\n}'\n```\n\n<sub>CLI</sub>\n```\nollama run llama3-gradient\n>>> /set parameter num_ctx 256000\n```\n\n\n## References\n\n[Website](https://gradient.ai/)\n\n[Hugging Face](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3-gradient:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "1M context",
        "context_window": 1000000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "1M",
        "context_window": 1000000
      },
      {
        "tag": "llama3-gradient:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "1M context",
        "context_window": 1000000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced text processing tasks.",
    "pulls": 387600,
    "tags": 35,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "2ea5b4e6-2e67-4a67-af3a-0b154b2d9bfc",
    "model_identifier": "samantha-mistral",
    "model_name": "samantha-mistral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/samantha-mistral",
    "description": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nMeet \nSamantha\n, a conversational model created by Eric Hartford. Samantha is trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion.\n\n\nExample Prompt\n\n\nI would like you to be my relationship coach. I am dating again, after a long divorce. Can you please give me advice about how to gain confidence and success?\n\n\n\nExample Response\n\n\nOf course! Building confidence and success in relationships is an important aspect of personal growth. While there isn't a one-size-fits-all solution, here are some general tips \nthat may help you:\n\n1. Set realistic goals: Establish attainable objectives for both your personal development and your relationship, keeping in mind that progress takes time.\n\n2. Communicate openly and honestly: Clear communication is essential for understanding and building strong connections with potential partners. Share your feelings, expectations, \nand desires while actively listening to their perspective.\n\n3. Be authentic: Embrace your unique qualities and express yourself genuinely. Trying to be someone you're not can lead to frustration and hinder personal growth.\n\n4. Learn from past experiences: Reflect on your previous relationships and identify the aspects you'd like to improve or avoid in future encounters. This self-awareness can help \nyou make better choices and grow as an individual.\n\n5. Develop emotional resilience: Cultivate the ability to cope with rejection, disappointment, and setbacks in your dating journey. Recognize these challenges as learning \nopportunities rather than insurmountable obstacles.\n\n6. Focus on self-improvement: Dedicate time to enhancing your own wellbeing, such as maintaining a healthy lifestyle, cultivating hobbies or interests, and expanding your social \ncircle. Personal growth can make you more attractive and confident in various aspects of life.\n\n7. Be patient: Building successful relationships and personal confidence takes time, effort, and persistence. Understand that progress might be incremental, and remember to \ncelebrate your achievements along the way.\n\n\n\nSource\n\n\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeet [Samantha](https://erichartford.com/meet-samantha), a conversational model created by Eric Hartford. Samantha is trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion.\n\n### Example Prompt\n\n```\nI would like you to be my relationship coach. I am dating again, after a long divorce. Can you please give me advice about how to gain confidence and success?\n```\n\n### Example Response\n\n```\nOf course! Building confidence and success in relationships is an important aspect of personal growth. While there isn't a one-size-fits-all solution, here are some general tips \nthat may help you:\n\n1. Set realistic goals: Establish attainable objectives for both your personal development and your relationship, keeping in mind that progress takes time.\n\n2. Communicate openly and honestly: Clear communication is essential for understanding and building strong connections with potential partners. Share your feelings, expectations, \nand desires while actively listening to their perspective.\n\n3. Be authentic: Embrace your unique qualities and express yourself genuinely. Trying to be someone you're not can lead to frustration and hinder personal growth.\n\n4. Learn from past experiences: Reflect on your previous relationships and identify the aspects you'd like to improve or avoid in future encounters. This self-awareness can help \nyou make better choices and grow as an individual.\n\n5. Develop emotional resilience: Cultivate the ability to cope with rejection, disappointment, and setbacks in your dating journey. Recognize these challenges as learning \nopportunities rather than insurmountable obstacles.\n\n6. Focus on self-improvement: Dedicate time to enhancing your own wellbeing, such as maintaining a healthy lifestyle, cultivating hobbies or interests, and expanding your social \ncircle. Personal growth can make you more attractive and confident in various aspects of life.\n\n7. Be patient: Building successful relationships and personal confidence takes time, effort, and persistence. Understand that progress might be incremental, and remember to \ncelebrate your achievements along the way.\n```\n\n### Source\n\n* [HuggingFace](https://huggingface.co/ehartford/samantha-mistral-instruct-7b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "samantha-mistral:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Role Play"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Developers, Researchers, Data Scientists",
    "pulls": 383700,
    "tags": 49,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "49eb791c-235a-44d2-b261-8a5fb125d064",
    "model_identifier": "deepseek-v3.1",
    "model_name": "deepseek-v3.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-v3.1",
    "description": "DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek-V3.1-Terminus update builds on V3.1’s strengths while addressing key user feedback:\n\n\n\n\n🌐 Language consistency: fewer CN/EN mix-ups & no more random chars.\n\n\n🤖 Agent upgrades: stronger Code Agent & Search Agent performance.\n\n\n\n\nHybrid thinking mode\n: One model supports both thinking mode and non-thinking mode by changing the chat template.\n\n\nSmarter tool calling\n: Through post-training optimization, the model’s performance in tool usage and agent tasks has significantly improved.\n\n\nHigher thinking efficiency\n: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![logo.svg](/assets/library/deepseek-v3.1/c3f72402-1dce-4c07-a2a1-eebabd4f4b4b)\n\nDeepSeek-V3.1-Terminus update builds on V3.1’s strengths while addressing key user feedback:\n\n* 🌐 Language consistency: fewer CN/EN mix-ups & no more random chars.\n* 🤖 Agent upgrades: stronger Code Agent & Search Agent performance.\n\n**Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template.\n\n**Smarter tool calling**: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.\n\n**Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "671b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-v3.1:latest",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      },
      {
        "tag": "latest",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K",
        "context_window": 160000
      }
    ],
    "min_ram_gb": 404.0,
    "use_cases": [
      "Question Answering",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers who need a versatile AI model for various tasks.",
    "pulls": 382000,
    "tags": 8,
    "last_updated": "2025-09-25",
    "last_updated_str": "5 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "883dd828-3654-49df-b4c8-634824080c3d",
    "model_identifier": "command-r-plus",
    "model_name": "command-r-plus",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/command-r-plus",
    "description": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCommand R+ is Cohere’s most powerful, scalable large language model (LLM) purpose-built to excel at real-world enterprise use cases. Command R+ balances high efficiency with strong accuracy, enabling businesses to move beyond proof-of-concept, and into production with AI:\n\n\n\n\nA 128k-token context window\n\n\nAdvanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations\n\n\nMultilingual coverage in 10 key languages to support global business operations\n\n\nTool Use to automate sophisticated business processes\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Command R+](https://github.com/ollama/ollama/assets/251292/566287ce-99c9-40c1-81d4-3dd354447fd7)\n\nCommand R+ is Cohere's most powerful, scalable large language model (LLM) purpose-built to excel at real-world enterprise use cases. Command R+ balances high efficiency with strong accuracy, enabling businesses to move beyond proof-of-concept, and into production with AI:\n\n- A 128k-token context window\n- Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations\n- Multilingual coverage in 10 key languages to support global business operations\n- Tool Use to automate sophisticated business processes\n\n## References\n\n[Blog Post](https://txt.cohere.com/command-r-plus-microsoft-azure/)\n\n[HuggingFace](https://huggingface.co/CohereForAI/c4ai-command-r-plus)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "104b"
    ],
    "memory_requirements": [
      {
        "tag": "command-r-plus:latest",
        "size": "59GB",
        "size_gb": 59.0,
        "recommended_ram_gb": 73.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "59GB",
        "size_gb": 59.0,
        "recommended_ram_gb": 73.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 59.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for complex language tasks.",
    "pulls": 379600,
    "tags": 21,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8a83eda7-3c1f-445e-a62c-6be06da43fcd",
    "model_identifier": "internlm2",
    "model_name": "internlm2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/internlm2",
    "description": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nInternLM2.5 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n\n\n\n\nOutstanding reasoning capability\n: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B.\n\n\nStronger tool use\n: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in Lagent soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection.\n\n\n\n\nReference\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/internlm2/646a4835-4904-4433-93ab-5eec0964fa12\" alt=\"internLM logo\" width=\"360\"/>\n\nInternLM2.5 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n\n* **Outstanding reasoning capability**: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B.\n\n* **Stronger tool use**: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in Lagent soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection.\n\n## Reference\n\n[GitHub](https://github.com/InternLM/InternLM)\n\n[Hugging Face](https://huggingface.co/internlm/internlm2_5-7b-chat)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1m",
      "1.8b",
      "7b",
      "20b"
    ],
    "memory_requirements": [
      {
        "tag": "internlm2:1.8b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "internlm2:latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "internlm2:1m",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "internlm2:20b",
        "size": "11GB",
        "size_gb": 11.0,
        "recommended_ram_gb": 13.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.1,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for practical scenarios with outstanding reasoning capability.",
    "pulls": 377600,
    "tags": 65,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "b1fc8a83-062c-4899-b399-3fd413ce1404",
    "model_identifier": "llama3-groq-tool-use",
    "model_name": "llama3-groq-tool-use",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3-groq-tool-use",
    "description": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThese models, developed in collaboration with \nGlaive\n, represent a significant advancement in open-source AI capabilities for tool use/function calling.\n\n\nBenchmark Results\n\n\nThese models have achieved remarkable results, setting new benchmarks for Large Language Models with tool use capabilities:\n\n\n\n\nLlama-3-Groq-70B-Tool-Use: 90.76% overall accuracy (#1 on BFCL at the time of publishing - July 2024)\n\n\nLlama-3-Groq-8B-Tool-Use: 89.06% overall accuracy (#3 on BFCL at the time of publishing - July 2024)\n\n\n\n\nReferences\n\n\nHugging Face\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/llama3-groq-tool-use/ebf53e82-1faf-4bac-84b0-47b8f5d9d8d1\" width=\"240\" />\n\nThese models, developed in collaboration with [Glaive](https://glaive.ai/), represent a significant advancement in open-source AI capabilities for tool use/function calling.\n\n## Benchmark Results\n\nThese models have achieved remarkable results, setting new benchmarks for Large Language Models with tool use capabilities:\n\n* Llama-3-Groq-70B-Tool-Use: 90.76% overall accuracy (#1 on BFCL at the time of publishing - July 2024)\n* Llama-3-Groq-8B-Tool-Use: 89.06% overall accuracy (#3 on BFCL at the time of publishing - July 2024)\n\n## References\n\n[Hugging Face](https://huggingface.co/Groq)\n\n[Blog](https://wow.groq.com/introducing-llama-3-groq-tool-use-models/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3-groq-tool-use:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "llama3-groq-tool-use:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Function Calling",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced AI capabilities in tool use.",
    "pulls": 375700,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8b6ab2de-af7a-4335-b172-10c7e225be7d",
    "model_identifier": "llama-guard3",
    "model_name": "llama-guard3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama-guard3",
    "description": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nLlama Guard 3 as an LLM that generates text in its output that indicates whether a given prompt or response is \nsafe\n or \nunsafe\n, and if unsafe, it also lists the content categories violated.\n\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n\n\nSizes\n\n\nLlama Guard 3 is available in two sizes:\n\n\n\n\nLlama Guard 3-1B: \nollama run llama-guard3:1b\n\n\nLlama Guard 3-8b: \nollama run llama-guard3:8b\n (default)\n\n\n\n\nExamples\n\n\nInput classification\n\n\n% ollama run llama-guard3\n>>> Tell me how to go to the zoo and steal a llama.\nunsafe\nS2\n\n\n\nResponse classification\n\n\nLlama Guard 3 can also classify LLM responses to ensure they are safe before being presented to customers. This can be done via Ollama’s \nchat completion\n API:\n\n\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama-guard3:1b\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How can I adopt my own llama?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Go to the zoo and steal one!\"\n    }\n  ],\n  \"stream\": false\n}'\n\n\n\nThe LLM will respond with a message stating if the last assistant message was safe or unsafe. in this case, the message is unsafe and classified as \nS2\n\n\nunsafe\nS2\n\n\n\nHazard Taxonomy and Policy\n\n\nThe model is trained to predict safety labels on the 13 categories shown below, based on the \nMLCommons taxonomy\n of 13 hazards.\n\n\n\n\n\n\n\n\nHazard categories\n\n\n\n\n\n\n\n\n\n\nS1: Violent Crimes\n\n\nS2: Non-Violent Crimes\n\n\n\n\n\n\nS3: Sex-Related Crimes\n\n\nS4: Child Sexual Exploitation\n\n\n\n\n\n\nS5: Defamation\n\n\nS6: Specialized Advice\n\n\n\n\n\n\nS7: Privacy\n\n\nS8: Intellectual Property\n\n\n\n\n\n\nS9: Indiscriminate Weapons\n\n\nS10: Hate\n\n\n\n\n\n\nS11: Suicide & Self-Harm\n\n\nS12: Sexual Content\n\n\n\n\n\n\nS13: Elections\n\n\n\n\n\n\n\n\n\n\nS1 - Violent Crimes\n\n\nResponses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\n\n\nS2 - Non-Violent Crimes\n\n\nResponses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\n\n\nS3 - Sex-Related Crimes\n\n\nResponses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\n\n\nS4 - Child Sexual Exploitation\n\n\nResponses that contain, describe, enable, encourage, or endorse the sexual abuse of children\n\n\nS5 - Defamation\n\n\nResponses that are both verifiably false and likely to injure a living person’s reputation\n\n\nS6 - Specialized Advice\n\n\nResponses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\n\n\nS7 - Privacy\n\n\nResponses that contain sensitive, nonpublic personal information that could undermine someone’s physical, digital, or financial security\n\n\nS8 - Intellectual Property\n\n\nResponses that may violate the intellectual property rights of any third party\n\n\nS9 - Indiscriminate Weapons\n\n\nResponses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\n\n\nS10 - Hate\n\n\nResponses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\n\n\nS11 - Suicide & Self-Harm\n\n\nResponses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\n\n\nS12 - Sexual Content\n\n\nResponses that contain erotica\n\n\nS13 - Elections\n\n\nResponses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections\n\n\nSupported languages\n\n\nLlama Guard 3-1B supports content safety for the following languages: English, French, German, Hindi, Italian, Portuguese, Spanish, Thai.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLlama Guard 3 as an LLM that generates text in its output that indicates whether a given prompt or response is **safe** or **unsafe**, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n\n## Sizes\n\nLlama Guard 3 is available in two sizes:\n\n* Llama Guard 3-1B: `ollama run llama-guard3:1b`\n* Llama Guard 3-8b: `ollama run llama-guard3:8b` (default)\n\n## Examples\n\n### Input classification\n\n```\n% ollama run llama-guard3\n>>> Tell me how to go to the zoo and steal a llama.\nunsafe\nS2\n```\n\n### Response classification\n\nLlama Guard 3 can also classify LLM responses to ensure they are safe before being presented to customers. This can be done via Ollama's [chat completion](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) API:\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama-guard3:1b\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How can I adopt my own llama?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Go to the zoo and steal one!\"\n    }\n  ],\n  \"stream\": false\n}'\n```\n\nThe LLM will respond with a message stating if the last assistant message was safe or unsafe. in this case, the message is unsafe and classified as **S2**\n\n```\nunsafe\nS2\n```\n\n## Hazard Taxonomy and Policy\n\nThe model is trained to predict safety labels on the 13 categories shown below, based on the [MLCommons taxonomy](https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/) of 13 hazards.\n\n<table align=\"center\">\n<thead>\n  <tr>\n    <th colspan=\"2\"><center><strong>Hazard categories</strong></center></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>S1: Violent Crimes</td>\n    <td>S2: Non-Violent Crimes</td>\n  </tr>\n  <tr>\n    <td>S3: Sex-Related Crimes</td>\n    <td>S4: Child Sexual Exploitation</td>\n  </tr>\n  <tr>\n    <td>S5: Defamation</td>\n    <td>S6: Specialized Advice</td>\n  </tr>\n  <tr>\n    <td>S7: Privacy</td>\n    <td>S8: Intellectual Property</td>\n  </tr>\n  <tr>\n    <td>S9: Indiscriminate Weapons</td>\n    <td>S10: Hate</td>\n  </tr>\n  <tr>\n    <td>S11: Suicide &amp; Self-Harm</td>\n    <td>S12: Sexual Content</td>\n  </tr>\n  <tr>\n    <td>S13: Elections</td>\n    <td></td>\n  </tr>\n</tbody>\n</table>\n\n**S1 - Violent Crimes**\n\nResponses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\n\n**S2 - Non-Violent Crimes**\n\nResponses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\n\n**S3 - Sex-Related Crimes**\n\nResponses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\n\n**S4 - Child Sexual Exploitation**\n\nResponses that contain, describe, enable, encourage, or endorse the sexual abuse of children\n\n**S5 - Defamation**\n\nResponses that are both verifiably false and likely to injure a living person’s reputation\n\n**S6 - Specialized Advice**\n\nResponses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\n\n**S7 - Privacy**\n\nResponses that contain sensitive, nonpublic personal information that could undermine someone’s physical, digital, or financial security\n\n**S8 - Intellectual Property**\n\nResponses that may violate the intellectual property rights of any third party\n\n**S9 - Indiscriminate Weapons**\n\nResponses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\n\n**S10 - Hate**\n\nResponses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\n\n**S11 - Suicide & Self-Harm**\n\nResponses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-inj",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "llama-guard3:1b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "llama-guard3:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for content safety classification tasks.",
    "pulls": 371900,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "bc0c07db-5be4-4ac4-9a95-779ba95a7567",
    "model_identifier": "starling-lm",
    "model_name": "starling-lm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/starling-lm",
    "description": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nStarling-7B is an open (non-commercial) large language model (LLM) trained by reinforcement learning from AI feedback. (RLAIF)\n\n\nThe model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI’s GPT-4 and GPT-4 Turbo.\n\n\n\n\n*Based on MT Bench evaluations, using GPT-4 scoring. Further human evaluation is needed.\n\n\nAuthors: Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu and Jiantao Jiao.\n\n\nFor correspondence, please contact Banghua Zhu (banghua@berkeley.edu).\n\n\nReference\n\n\nStarling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/ffc27067-27cc-425c-a910-a5a6f1fa2dc1\" width=\"240\">\n\nStarling-7B is an open (non-commercial) large language model (LLM) trained by reinforcement learning from AI feedback. (RLAIF) \n\nThe model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI’s GPT-4 and GPT-4 Turbo. \n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/a7d89b3b-b803-4c98-bb4f-ef03f6d76064\">\n\n*Based on MT Bench evaluations, using GPT-4 scoring. Further human evaluation is needed.\n\nAuthors: Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu and Jiantao Jiao. \n\nFor correspondence, please contact Banghua Zhu (banghua@berkeley.edu). \n\n\n## Reference\n\n[Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/)\n\n[HuggingFace](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "starling-lm:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 371000,
    "tags": 36,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "dbe4e24c-c28c-4087-bdc3-1d25efdceea5",
    "model_identifier": "phind-codellama",
    "model_name": "phind-codellama",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phind-codellama",
    "description": "Code generation model based on Code Llama.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhind CodeLlama is a code generation model based on CodeLlama 34B fine-tuned for instruct use cases. There are two versions of the model: \nv1\n and \nv2\n. \nv1\n is based on CodeLlama 34B and CodeLlama-Python 34B. \nv2\n is an iteration on \nv1\n, trained on an additional 1.5B tokens of high-quality programming-related data.\n\n\nUsage\n\n\nCLI\n\n\nOpen the terminal and run \nollama run phind-codellama\n\n\nAPI\n\n\nExample\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"phind-codellama\",\n  \"prompt\":\"Implement a linked list in C++\"\n }'\n\n\n\nMemory requirements\n\n\n\n\n34b models generally require at least 32GB of RAM\n\n\n\n\nReferences\n\n\nBeating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/264120182-4a05afc9-0003-410e-8ac5-a7745dedaba8.png\" style=\"max-width:60%\" />\n\nPhind CodeLlama is a code generation model based on CodeLlama 34B fine-tuned for instruct use cases. There are two versions of the model: `v1` and `v2`. `v1` is based on CodeLlama 34B and CodeLlama-Python 34B. `v2` is an iteration on `v1`, trained on an additional 1.5B tokens of high-quality programming-related data.\n\n## Usage\n\n### CLI\n\nOpen the terminal and run `ollama run phind-codellama`\n\n### API\n\nExample\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"phind-codellama\",\n  \"prompt\":\"Implement a linked list in C++\"\n }'\n  ```\n\n## Memory requirements\n\n- 34b models generally require at least 32GB of RAM\n\n## References\n\n[Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B](https://www.phind.com/blog/code-llama-beats-gpt4 )\n\n[HuggingFace](https://huggingface.co/Phind)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "phind-codellama:latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 19.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for code generation tasks.",
    "pulls": 369300,
    "tags": 49,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0a1ab0f6-51ff-4092-89cf-69d5c2942fb4",
    "model_identifier": "solar",
    "model_name": "solar",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/solar",
    "description": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSolar is the first open-source 10.7 billion parameter language model. It’s compact, yet remarkably powerful, and demonstrates state-of-the-art performance in models with parameters under 30B.\n\n\nThis model leverages the Llama 2 architecture and employs the Depth Up-Scaling technique, integrating Mistral 7B weights into upscaled layers.\n\n\nOn the H6 benchmark, this model outperforms models with up to 30B parameters, even the Mixtral 8X7B model.\n\n\nReferences\n\n\nHuggingFace\n\n\nUpstage AI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/1867051c-434e-4873-a074-3fa7bc111423\" width=\"200\" />\n\nSolar is the first open-source 10.7 billion parameter language model. It's compact, yet remarkably powerful, and demonstrates state-of-the-art performance in models with parameters under 30B.\n\nThis model leverages the Llama 2 architecture and employs the Depth Up-Scaling technique, integrating Mistral 7B weights into upscaled layers.\n\nOn the H6 benchmark, this model outperforms models with up to 30B parameters, even the Mixtral 8X7B model.\n\n## References\n\n[HuggingFace](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)\n\n[Upstage AI](https://www.upstage.ai)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "10.7b"
    ],
    "memory_requirements": [
      {
        "tag": "solar:latest",
        "size": "6.1GB",
        "size_gb": 6.1,
        "recommended_ram_gb": 7.6,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "6.1GB",
        "size_gb": 6.1,
        "recommended_ram_gb": 7.6,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 6.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 369200,
    "tags": 32,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "7c5e8595-7c69-4f7f-a861-865de874fe6b",
    "model_identifier": "xwinlm",
    "model_name": "xwinlm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/xwinlm",
    "description": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nXwin-LM is a model based on Llama 2 using various techniques to improve its quality.\n\n\nReference\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img width=\"400\" src=\"https://github.com/jmorganca/ollama/assets/251292/74015728-d0a6-4c13-bb8a-d2e180480590\">\n\nXwin-LM is a model based on Llama 2 using various techniques to improve its quality. \n\n## Reference\n[Hugging Face](https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.2)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "xwinlm:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "xwinlm:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Conversational and text summarization tasks requiring high quality.",
    "pulls": 366100,
    "tags": 80,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f8dcfeac-f8d1-4223-88e7-479634923e17",
    "model_identifier": "translategemma",
    "model_name": "translategemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/translategemma",
    "description": "A new collection of open translation models built on Gemma 3, helping people communicate across 55 languages.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nTranslateGemma is a new collection of open translation models built on Gemma 3, available in 4B, 12B, and 27B parameter sizes. It represents a significant step forward in open translation, helping people communicate across 55 languages, no matter where they are or what device they own.\n\n\nPrompt Guide\n\n\nPrompt Format\n\n\nTranslateGemma expects a single user message with this structure:\n\n\nYou are a professional {SOURCE_LANG} ({SOURCE_CODE}) to {TARGET_LANG} ({TARGET_CODE}) translator. Your goal is to accurately convey the meaning and nuances of the original {SOURCE_LANG} text while adhering to {TARGET_LANG} grammar, vocabulary, and cultural sensitivities.\nProduce only the {TARGET_LANG} translation, without any additional explanations or commentary. Please translate the following {SOURCE_LANG} text into {TARGET_LANG}:\n\n\n{TEXT}\n\n\n\nImportant:\n There are two blank lines before the text to translate.\n\n\nExamples\n\n\nEnglish to Spanish\n\n\nYou are a professional English (en) to Spanish (es) translator. Your goal is to accurately convey the meaning and nuances of the original English text while adhering to Spanish grammar, vocabulary, and cultural sensitivities.\nProduce only the Spanish translation, without any additional explanations or commentary. Please translate the following English text into Spanish:\n\n\nHello, how are you?\n\n\n\nGerman to English\n\n\nYou are a professional German (de) to English (en) translator. Your goal is to accurately convey the meaning and nuances of the original German text while adhering to English grammar, vocabulary, and cultural sensitivities.\nProduce only the English translation, without any additional explanations or commentary. Please translate the following German text into English:\n\n\nGuten Morgen, wie geht es Ihnen?\n\n\n\nJapanese to French\n\n\nYou are a professional Japanese (ja) to French (fr) translator. Your goal is to accurately convey the meaning and nuances of the original Japanese text while adhering to French grammar, vocabulary, and cultural sensitivities.\nProduce only the French translation, without any additional explanations or commentary. Please translate the following Japanese text into French:\n\n\nこんにちは、世界！\n\n\n\nChinese (Simplified) to English\n\n\nYou are a professional Chinese (zh-Hans) to English (en) translator. Your goal is to accurately convey the meaning and nuances of the original Chinese text while adhering to English grammar, vocabulary, and cultural sensitivities.\nProduce only the English translation, without any additional explanations or commentary. Please translate the following Chinese text into English:\n\n\n你好世界\n\n\n\nSupported Languages\n\n\n\n\n\n\n\n\nCode\n\n\nLanguage\n\n\n\n\n\n\n\n\n\n\naa\n\n\nAfar\n\n\n\n\n\n\naa-DJ\n\n\nAfar\n\n\n\n\n\n\naa-ER\n\n\nAfar\n\n\n\n\n\n\nab\n\n\nAbkhazian\n\n\n\n\n\n\naf\n\n\nAfrikaans\n\n\n\n\n\n\naf-NA\n\n\nAfrikaans\n\n\n\n\n\n\nak\n\n\nAkan\n\n\n\n\n\n\nam\n\n\nAmharic\n\n\n\n\n\n\nan\n\n\nAragonese\n\n\n\n\n\n\nar\n\n\nArabic\n\n\n\n\n\n\nar-AE\n\n\nArabic\n\n\n\n\n\n\nar-BH\n\n\nArabic\n\n\n\n\n\n\nar-DJ\n\n\nArabic\n\n\n\n\n\n\nar-DZ\n\n\nArabic\n\n\n\n\n\n\nar-EG\n\n\nArabic\n\n\n\n\n\n\nar-EH\n\n\nArabic\n\n\n\n\n\n\nar-ER\n\n\nArabic\n\n\n\n\n\n\nar-IL\n\n\nArabic\n\n\n\n\n\n\nar-IQ\n\n\nArabic\n\n\n\n\n\n\nar-JO\n\n\nArabic\n\n\n\n\n\n\nar-KM\n\n\nArabic\n\n\n\n\n\n\nar-KW\n\n\nArabic\n\n\n\n\n\n\nar-LB\n\n\nArabic\n\n\n\n\n\n\nar-LY\n\n\nArabic\n\n\n\n\n\n\nar-MA\n\n\nArabic\n\n\n\n\n\n\nar-MR\n\n\nArabic\n\n\n\n\n\n\nar-OM\n\n\nArabic\n\n\n\n\n\n\nar-PS\n\n\nArabic\n\n\n\n\n\n\nar-QA\n\n\nArabic\n\n\n\n\n\n\nar-SA\n\n\nArabic\n\n\n\n\n\n\nar-SD\n\n\nArabic\n\n\n\n\n\n\nar-SO\n\n\nArabic\n\n\n\n\n\n\nar-SS\n\n\nArabic\n\n\n\n\n\n\nar-SY\n\n\nArabic\n\n\n\n\n\n\nar-TD\n\n\nArabic\n\n\n\n\n\n\nar-TN\n\n\nArabic\n\n\n\n\n\n\nar-YE\n\n\nArabic\n\n\n\n\n\n\nas\n\n\nAssamese\n\n\n\n\n\n\naz\n\n\nAzerbaijani\n\n\n\n\n\n\naz-Arab\n\n\nAzerbaijani\n\n\n\n\n\n\naz-Arab-IQ\n\n\nAzerbaijani\n\n\n\n\n\n\naz-Arab-TR\n\n\nAzerbaijani\n\n\n\n\n\n\naz-Cyrl\n\n\nAzerbaijani\n\n\n\n\n\n\naz-Latn\n\n\nAzerbaijani\n\n\n\n\n\n\nba\n\n\nBashkir\n\n\n\n\n\n\nbe\n\n\nBelarusian\n\n\n\n\n\n\nbe-tarask\n\n\nBelarusian\n\n\n\n\n\n\nbg\n\n\nBulgarian\n\n\n\n\n\n\nbg-BG\n\n\nBulgarian\n\n\n\n\n\n\nbm\n\n\nBambara\n\n\n\n\n\n\nbm-Nkoo\n\n\nBambara\n\n\n\n\n\n\nbn\n\n\nBengali\n\n\n\n\n\n\nbn-IN\n\n\nBengali\n\n\n\n\n\n\nbo\n\n\nTibetan\n\n\n\n\n\n\nbo-IN\n\n\nTibetan\n\n\n\n\n\n\nbr\n\n\nBreton\n\n\n\n\n\n\nbs\n\n\nBosnian\n\n\n\n\n\n\nbs-Cyrl\n\n\nBosnian\n\n\n\n\n\n\nbs-Latn\n\n\nBosnian\n\n\n\n\n\n\nca\n\n\nCatalan\n\n\n\n\n\n\nca-AD\n\n\nCatalan\n\n\n\n\n\n\nca-ES\n\n\nCatalan\n\n\n\n\n\n\nca-FR\n\n\nCatalan\n\n\n\n\n\n\nca-IT\n\n\nCatalan\n\n\n\n\n\n\nce\n\n\nChechen\n\n\n\n\n\n\nco\n\n\nCorsican\n\n\n\n\n\n\ncs\n\n\nCzech\n\n\n\n\n\n\ncs-CZ\n\n\nCzech\n\n\n\n\n\n\ncv\n\n\nChuvash\n\n\n\n\n\n\ncy\n\n\nWelsh\n\n\n\n\n\n\nda\n\n\nDanish\n\n\n\n\n\n\nda-DK\n\n\nDanish\n\n\n\n\n\n\nda-GL\n\n\nDanish\n\n\n\n\n\n\nde\n\n\nGerman\n\n\n\n\n\n\nde-AT\n\n\nGerman\n\n\n\n\n\n\nde-BE\n\n\nGerman\n\n\n\n\n\n\nde-CH\n\n\nGerman\n\n\n\n\n\n\nde-DE\n\n\nGerman\n\n\n\n\n\n\nde-IT\n\n\nGerman\n\n\n\n\n\n\nde-LI\n\n\nGerman\n\n\n\n\n\n\nde-LU\n\n\nGerman\n\n\n\n\n\n\ndv\n\n\nDivehi\n\n\n\n\n\n\ndz\n\n\nDzongkha\n\n\n\n\n\n\nee\n\n\nEwe\n\n\n\n\n\n\nee-TG\n\n\nEwe\n\n\n\n\n\n\nel\n\n\nGreek\n\n\n\n\n\n\nel-CY\n\n\nGreek\n\n\n\n\n\n\nel-GR\n\n\nGreek\n\n\n\n\n\n\nel-polyton\n\n\nGreek\n\n\n\n\n\n\nen\n\n\nEnglish\n\n\n\n\n\n\nen-AE\n\n\nEnglish\n\n\n\n\n\n\nen-AG\n\n\nEnglish\n\n\n\n\n\n\nen-AI\n\n\nEnglish\n\n\n\n\n\n\nen-AS\n\n\nEnglish\n\n\n\n\n\n\nen-AT\n\n\nEnglish\n\n\n\n\n\n\nen-AU\n\n\nEnglish\n\n\n\n\n\n\nen-BB\n\n\nEnglish\n\n\n\n\n\n\nen-BE\n\n\nEnglish\n\n\n\n\n\n\nen-BI\n\n\nEnglish\n\n\n\n\n\n\nen-BM\n\n\nEnglish\n\n\n\n\n\n\nen-BS\n\n\nEnglish\n\n\n\n\n\n\nen-BW\n\n\nEnglish\n\n\n\n\n\n\nen-BZ\n\n\nEnglish\n\n\n\n\n\n\nen-CA\n\n\nEnglish\n\n\n\n\n\n\nen-CC\n\n\nEnglish\n\n\n\n\n\n\nen-CH\n\n\nEnglish\n\n\n\n\n\n\nen-CK\n\n\nEnglish\n\n\n\n\n\n\nen-CM\n\n\nEnglish\n\n\n\n\n\n\nen-CX\n\n\nEnglish\n\n\n\n\n\n\nen-CY\n\n\nEnglish\n\n\n\n\n\n\nen-CZ\n\n\nEnglish\n\n\n\n\n\n\nen-DE\n\n\nEnglish\n\n\n\n\n\n\nen-DG\n\n\nEnglish\n\n\n\n\n\n\nen-DK\n\n\nEnglish\n\n\n\n\n\n\nen-DM\n\n\nEnglish\n\n\n\n\n\n\nen-ER\n\n\nEnglish\n\n\n\n\n\n\nen-ES\n\n\nEnglish\n\n\n\n\n\n\nen-FI\n\n\nEnglish\n\n\n\n\n\n\nen-FJ\n\n\nEnglish\n\n\n\n\n\n\nen-FK\n\n\nEnglish\n\n\n\n\n\n\nen-FM\n\n\nEnglish\n\n\n\n\n\n\nen-FR\n\n\nEnglish\n\n\n\n\n\n\nen-GB\n\n\nEnglish\n\n\n\n\n\n\nen-GD\n\n\nEnglish\n\n\n\n\n\n\nen-GG\n\n\nEnglish\n\n\n\n\n\n\nen-GH\n\n\nEnglish\n\n\n\n\n\n\nen-GI\n\n\nEnglish\n\n\n\n\n\n\nen-GM\n\n\nEnglish\n\n\n\n\n\n\nen-GS\n\n\nEnglish\n\n\n\n\n\n\nen-GU\n\n\nEnglish\n\n\n\n\n\n\nen-GY\n\n\nEnglish\n\n\n\n\n\n\nen-HK\n\n\nEnglish\n\n\n\n\n\n\nen-HU\n\n\nEnglish\n\n\n\n\n\n\nen-ID\n\n\nEnglish\n\n\n\n\n\n\nen-IE\n\n\nEnglish\n\n\n\n\n\n\nen-IL\n\n\nEnglish\n\n\n\n\n\n\nen-IM\n\n\nEnglish\n\n\n\n\n\n\nen-IN\n\n\nEnglish\n\n\n\n\n\n\nen-IO\n\n\nEnglish\n\n\n\n\n\n\nen-IT\n\n\nEnglish\n\n\n\n\n\n\nen-JE\n\n\nEnglish\n\n\n\n\n\n\nen-JM\n\n\nEnglish\n\n\n\n\n\n\nen-KE\n\n\nEnglish\n\n\n\n\n\n\nen-KI\n\n\nEnglish\n\n\n\n\n\n\nen-KN\n\n\nEnglish\n\n\n\n\n\n\nen-KY\n\n\nEnglish\n\n\n\n\n\n\nen-LC\n\n\nEnglish\n\n\n\n\n\n\nen-LR\n\n\nEnglish\n\n\n\n\n\n\nen-LS\n\n\nEnglish\n\n\n\n\n\n\nen-MG\n\n\nEnglish\n\n\n\n\n\n\nen-MH\n\n\nEnglish\n\n\n\n\n\n\nen-MO\n\n\nEnglish\n\n\n\n\n\n\nen-MP\n\n\nEnglish\n\n\n\n\n\n\nen-MS\n\n\nEnglish\n\n\n\n\n\n\nen-MT\n\n\nEnglish\n\n\n\n\n\n\nen-MU\n\n\nEnglish\n\n\n\n\n\n\nen-MV\n\n\nEnglish\n\n\n\n\n\n\nen-MW\n\n\nEnglish\n\n\n\n\n\n\nen-MY\n\n\nEnglish\n\n\n\n\n\n\nen-NA\n\n\nEnglish\n\n\n\n\n\n\nen-NF\n\n\nEnglish\n\n\n\n\n\n\nen-NG\n\n\nEnglish\n\n\n\n\n\n\nen-NL\n\n\nEnglish\n\n\n\n\n\n\nen-NO\n\n\nEnglish\n\n\n\n\n\n\nen-NR\n\n\nEnglish\n\n\n\n\n\n\nen-NU\n\n\nEnglish\n\n\n\n\n\n\nen-NZ\n\n\nEnglish\n\n\n\n\n\n\nen-PG\n\n\nEnglish\n\n\n\n\n\n\nen-PH\n\n\nEnglish\n\n\n\n\n\n\nen-PK\n\n\nEnglish\n\n\n\n\n\n\nen-PL\n\n\nEnglish\n\n\n\n\n\n\nen-PN\n\n\nEnglish\n\n\n\n\n\n\nen-PR\n\n\nEnglish\n\n\n\n\n\n\nen-PT\n\n\nEnglish\n\n\n\n\n\n\nen-PW\n\n\nEnglish\n\n\n\n\n\n\nen-RO\n\n\nEnglish\n\n\n\n\n\n\nen-RW\n\n\nEnglish\n\n\n\n\n\n\nen-SB\n\n\nEnglish\n\n\n\n\n\n\nen-SC\n\n\nEnglish\n\n\n\n\n\n\nen-SD\n\n\nEnglish\n\n\n\n\n\n\nen-SE\n\n\nEnglish\n\n\n\n\n\n\nen-SG\n\n\nEnglish\n\n\n\n\n\n\nen-SH\n\n\nEnglish\n\n\n\n\n\n\nen-SI\n\n\nEnglish\n\n\n\n\n\n\nen-SK\n\n\nEnglish\n\n\n\n\n\n\nen-SL\n\n\nEnglish\n\n\n\n\n\n\nen-SS\n\n\nEnglish\n\n\n\n\n\n\nen-SX\n\n\nEnglish\n\n\n\n\n\n\nen-SZ\n\n\nEnglish\n\n\n\n\n\n\nen-TC\n\n\nEnglish\n\n\n\n\n\n\nen-TK\n\n\nEnglish\n\n\n\n\n\n\nen-TO\n\n\nEnglish\n\n\n\n\n\n\nen-TT\n\n\nEnglish\n\n\n\n\n\n\nen-TV\n\n\nEnglish\n\n\n\n\n\n\nen-TZ\n\n\nEnglish\n\n\n\n\n\n\nen-UG\n\n\nEnglish\n\n\n\n\n\n\nen-UM\n\n\nEnglish\n\n\n\n\n\n\nen-VC\n\n\nEnglish\n\n\n\n\n\n\nen-VG\n\n\nEnglish\n\n\n\n\n\n\nen-VI\n\n\nEnglish\n\n\n\n\n\n\nen-VU\n\n\nEnglish\n\n\n\n\n\n\nen-WS\n\n\nEnglish\n\n\n\n\n\n\nen-ZA\n\n\nEnglish\n\n\n\n\n\n\nen-ZM\n\n\nEnglish\n\n\n\n\n\n\nen-ZW\n\n\nEnglish\n\n\n\n\n\n\neo\n\n\nEsperanto\n\n\n\n\n\n\nes\n\n\nSpanish\n\n\n\n\n\n\nes-AR\n\n\nSpanish\n\n\n\n\n\n\nes-BO\n\n\nSpanish\n\n\n\n\n\n\nes-BR\n\n\nSpanish\n\n\n\n\n\n\nes-BZ\n\n\nSpanish\n\n\n\n\n\n\nes-CL\n\n\nSpanish\n\n\n\n\n\n\nes-CO\n\n\nSpanish\n\n\n\n\n\n\nes-CR\n\n\nSpanish\n\n\n\n\n\n\nes-CU\n\n\nSpanish\n\n\n\n\n\n\nes-DO\n\n\nSpanish\n\n\n\n\n\n\nes-EA\n\n\nSpanish\n\n\n\n\n\n\nes-EC\n\n\nSpanish\n\n\n\n\n\n\nes-ES\n\n\nSpanish\n\n\n\n\n\n\nes-GQ\n\n\nSpanish\n\n\n\n\n\n\nes-GT\n\n\nSpanish\n\n\n\n\n\n\nes-HN\n\n\nSpanish\n\n\n\n\n\n\nes-IC\n\n\nSpanish\n\n\n\n\n\n\nes-MX\n\n\nSpanish\n\n\n\n\n\n\nes-NI\n\n\nSpanish\n\n\n\n\n\n\nes-PA\n\n\nSpanish\n\n\n\n\n\n\nes-PE\n\n\nSpanish\n\n\n\n\n\n\nes-PH\n\n\nSpanish\n\n\n\n\n\n\nes-PR\n\n\nSpanish\n\n\n\n\n\n\nes-PY\n\n\nSpanish\n\n\n\n\n\n\nes-SV\n\n\nSpanish\n\n\n\n\n\n\nes-US\n\n\nSpanish\n\n\n\n\n\n\nes-UY\n\n\nSpanish\n\n\n\n\n\n\nes-VE\n\n\nSpanish\n\n\n\n\n\n\net\n\n\nEstonian\n\n\n\n\n\n\net-EE\n\n\nEstonian\n\n\n\n\n\n\neu\n\n\nBasque\n\n\n\n\n\n\nfa\n\n\nPersian\n\n\n\n\n\n\nfa-AF\n\n\nPersian\n\n\n\n\n\n\nfa-IR\n\n\nPersian\n\n\n\n\n\n\nff\n\n\nFulah\n\n\n\n\n\n\nff-Adlm\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-BF\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-CM\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-GH\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-GM\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-GW\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-LR\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-MR\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-NE\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-NG\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-SL\n\n\nFulah\n\n\n\n\n\n\nff-Adlm-SN\n\n\nFulah\n\n\n\n\n\n\nff-Latn\n\n\nFulah\n\n\n\n\n\n\nff-Latn-BF\n\n\nFulah\n\n\n\n\n\n\nff-Latn-CM\n\n\nFulah\n\n\n\n\n\n\nff-Latn-GH\n\n\nFulah\n\n\n\n\n\n\nff-Latn-GM\n\n\nFulah\n\n\n\n\n\n\nff-Latn-GN\n\n\nFulah\n\n\n\n\n\n\nff-Latn-GW\n\n\nFulah\n\n\n\n\n\n\nff-Latn-LR\n\n\nFulah\n\n\n\n\n\n\nff-Latn-MR\n\n\nFulah\n\n\n\n\n\n\nff-Latn-NE\n\n\nFulah\n\n\n\n\n\n\nff-Latn-NG\n\n\nFulah\n\n\n\n\n\n\nff-Latn-SL\n\n\nFulah\n\n\n\n\n\n\nfi\n\n\nFinnish\n\n\n\n\n\n\nfi-FI\n\n\nFinnish\n\n\n\n\n\n\nfil-PH\n\n\nFilipino\n\n\n\n\n\n\nfo\n\n\nFaroese\n\n\n\n\n\n\nfo-DK\n\n\nFaroese\n\n\n\n\n\n\nfr\n\n\nFrench\n\n\n\n\n\n\nfr-BE\n\n\nFrench\n\n\n\n\n\n\nfr-BF\n\n\nFrench\n\n\n\n\n\n\nfr-BI\n\n\nFrench\n\n\n\n\n\n\nfr-BJ\n\n\nFrench\n\n\n\n\n\n\nfr-BL\n\n\nFrench\n\n\n\n\n\n\nfr-CA\n\n\nFrench\n\n\n\n\n\n\nfr-CD\n\n\nFrench\n\n\n\n\n\n\nfr-CF\n\n\nFrench\n\n\n\n\n\n\nfr-CG\n\n\nFrench\n\n\n\n\n\n\nfr-CH\n\n\nFrench\n\n\n\n\n\n\nfr-CI\n\n\nFrench\n\n\n\n\n\n\nfr-CM\n\n\nFrench\n\n\n\n\n\n\nfr-DJ\n\n\nFrench\n\n\n\n\n\n\nfr-DZ\n\n\nFrench\n\n\n\n\n\n\nfr-FR\n\n\nFrench\n\n\n\n\n\n\nfr-GA\n\n\nFrench\n\n\n\n\n\n\nfr-GF\n\n\nFrench\n\n\n\n\n\n\nfr-GN\n\n\nFrench\n\n\n\n\n\n\nfr-GP\n\n\nFrench\n\n\n\n\n\n\nfr-GQ\n\n\nFrench\n\n\n\n\n\n\nfr-HT\n\n\nFrench\n\n\n\n\n\n\nfr-KM\n\n\nFrench\n\n\n\n\n\n\nfr-LU\n\n\nFrench\n\n\n\n\n\n\nfr-MA\n\n\nFrench\n\n\n\n\n\n\nfr-MC\n\n\nFrench\n\n\n\n\n\n\nfr-MF\n\n\nFrench\n\n\n\n\n\n\nfr-MG\n\n\nFrench\n\n\n\n\n\n\nfr-ML\n\n\nFrench\n\n\n\n\n\n\nfr-MQ\n\n\nFrench\n\n\n\n\n\n\nfr-MR\n\n\nFrench\n\n\n\n\n\n\nfr-MU\n\n\nFrench\n\n\n\n\n\n\nfr-NC\n\n\nFrench\n\n\n\n\n\n\nfr-NE\n\n\nFrench\n\n\n\n\n\n\nfr-PF\n\n\nFrench\n\n\n\n\n\n\nfr-PM\n\n\nFrench\n\n\n\n\n\n\nfr-RE\n\n\nFrench\n\n\n\n\n\n\nfr-RW\n\n\nFrench\n\n\n\n\n\n\nfr-SC\n\n\nFrench\n\n\n\n\n\n\nfr-SN\n\n\nFrench\n\n\n\n\n\n\nfr-SY\n\n\nFrench\n\n\n\n\n\n\nfr-TD\n\n\nFrench\n\n\n\n\n\n\nfr-TG\n\n\nFrench\n\n\n\n\n\n\nfr-TN\n\n\nFrench\n\n\n\n\n\n\nfr-VU\n\n\nFrench\n\n\n\n\n\n\nfr-WF\n\n\nFrench\n\n\n\n\n\n\nfr-YT\n\n\nFrench\n\n\n\n\n\n\nfy\n\n\nWestern Frisian\n\n\n\n\n\n\nga\n\n\nIrish\n\n\n\n\n\n\nga-GB\n\n\nIrish\n\n\n\n\n\n\ngd\n\n\nScottish Gaelic\n\n\n\n\n\n\ngl\n\n\nGalician\n\n\n\n\n\n\ngn\n\n\nGuarani\n\n\n\n\n\n\ngu\n\n\nGujarati\n\n\n\n\n\n\ngu-IN\n\n\nGujarati\n\n\n\n\n\n\ngv\n\n\nManx\n\n\n\n\n\n\nha\n\n\nHausa\n\n\n\n\n\n\nha-Arab\n\n\nHausa\n\n\n\n\n\n\nha-Arab-SD\n\n\nHausa\n\n\n\n\n\n\nha-GH\n\n\nHausa\n\n\n\n\n\n\nha-NE\n\n\nHausa\n\n\n\n\n\n\nhe\n\n\nHebrew\n\n\n\n\n\n\nhe-IL\n\n\nHebrew\n\n\n\n\n\n\nhi\n\n\nHindi\n\n\n\n\n\n\nhi-IN\n\n\nHindi\n\n\n\n\n\n\nhi-Latn\n\n\nHindi\n\n\n\n\n\n\nhr\n\n\nCroatian\n\n\n\n\n\n\nhr-BA\n\n\nCroatian\n\n\n\n\n\n\nhr-HR\n\n\nCroatian\n\n\n\n",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "4b",
      "12b",
      "27b"
    ],
    "memory_requirements": [
      {
        "tag": "translategemma:latest",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "translategemma:12b",
        "size": "8.1GB",
        "size_gb": 8.1,
        "recommended_ram_gb": 10.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "translategemma:27b",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 3.3,
    "use_cases": [
      "Translation",
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual",
      "Chinese",
      "Arabic",
      "Japanese"
    ],
    "complexity": "advanced",
    "best_for": "People needing to communicate across multiple languages for work or personal use.",
    "pulls": 374800,
    "tags": 13,
    "last_updated": "2026-01-25",
    "last_updated_str": "1 month ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "de0c15ce-ac68-43b2-a73d-be0aa6011e9e",
    "model_identifier": "aya-expanse",
    "model_name": "aya-expanse",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/aya-expanse",
    "description": "Cohere For AI's language models trained to perform well across 23 different languages.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nAya Expanse represents a significant advancement in multilingual AI capabilities. Combining Cohere’s Command model family with a year of focused research in multilingual optimization has produced versatile 8B and 32B parameter models that can understand and generate text across 23 languages while maintaining high performance across all of them.\n\n\nKey Features\n\n\n\n\n8-billion and 32-billion parameter multilingual language models developed by Cohere For AI\n\n\nSupports Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese\n\n\nBuilt on the Command family of models with extensive research in data arbitrage, multilingual preference training, and safety tuning\n\n\n128K context length\n\n\nReleased under CC-BY-NC license with additional usage policies\n\n\n\n\nReferences\n\n\nBlog post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/aya-expanse/fba5fdd1-f5f3-43d2-a2e1-04c9e37d37be\" width=\"460\" />\n\nAya Expanse represents a significant advancement in multilingual AI capabilities. Combining Cohere's Command model family with a year of focused research in multilingual optimization has produced versatile 8B and 32B parameter models that can understand and generate text across 23 languages while maintaining high performance across all of them.\n\n## Key Features\n- 8-billion and 32-billion parameter multilingual language models developed by Cohere For AI\n- Supports Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese\n- Built on the Command family of models with extensive research in data arbitrage, multilingual preference training, and safety tuning\n- 128K context length\n- Released under CC-BY-NC license with additional usage policies\n\n## References\n[Blog post](https://cohere.com/blog/aya-expanse-connecting-our-world)\n\n[HuggingFace](https://huggingface.co/CohereForAI/aya-expanse-8b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "8b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "aya-expanse:latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "aya-expanse:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 5.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working with multilingual text.",
    "pulls": 363600,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "00639178-6456-4935-809c-a4c0da6c4847",
    "model_identifier": "granite3-moe",
    "model_name": "granite3-moe",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3-moe",
    "description": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGranite mixture of experts models\n\n\nThe IBM Granite \n1B and 3B models\n are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.\n\n\nThe models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference.\n\n\nParameter Sizes\n\n\n1B:\n\n\nollama run granite3-moe:1b\n\n\n3B:\n\n\nollama run granite3-moe:3b\n\n\nSupported Languages\n\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\n\n\nCapabilities\n\n\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related\n\n\nFunction-calling\n\n\nMultilingual dialog use cases\n\n\n\n\nGranite dense models\n\n\nThe Granite dense models are available in \n2B and 8B\n parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n\nSee model page\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nGitHub Repository:\n \nibm-granite/granite-3.0-language-models\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: October 21st, 2024\n\n\nLicense:\n \nApache 2.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![An illustration of Ollama holding a beautiful flower with the IBM Rebus logo of the Eye, Bee and M, made by Paul Rand.](https://ollama.com/assets/library/granite3-moe/6ea49528-3ff2-4fcc-98b2-01f6104254d2)\n\n### Granite mixture of experts models\n\nThe IBM Granite **1B and 3B models** are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.\n\nThe models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference.\n\n## Parameter Sizes\n\n**1B:**\n  \n`ollama run granite3-moe:1b`\n\n**3B:**\n\n`ollama run granite3-moe:3b`\n\n## Supported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) \n\n### Capabilities\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related \n* Function-calling\n* Multilingual dialog use cases\n\n\n### Granite dense models\n\nThe Granite dense models are available in **2B and 8B** parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n[See model page](https://ollama.com/library/granite3-dense) \n\n### Learn more\n\n- **Developers:** IBM Research\n- **GitHub Repository:** [ibm-granite/granite-3.0-language-models](https://github.com/ibm-granite/granite-3.0-language-models)\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: October 21st, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "1b",
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3-moe:3b",
        "size": "2.1GB",
        "size_gb": 2.1,
        "recommended_ram_gb": 2.6,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "Spanish",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text-related tasks.",
    "pulls": 359700,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8bf8b505-2162-471e-baf2-2e78d1b4e62c",
    "model_identifier": "yarn-llama2",
    "model_name": "yarn-llama2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/yarn-llama2",
    "description": "An extension of Llama 2 that supports a context of up to 128k tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nYarn Llama 2 is a model based on \nLlama2\n that extends its context size up to 128k context. It is developed by \nNous Research\n by implementing the YaRN method to further train the model to support larger context windows.\n\n\nCLI\n\n\n64k context size:\n\n\nollama run yarn-llama2\n\n\n\n128k context size:\n\n\nollama run yarn-llama2:7b-128k\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"yarn-llama2:7b-128k\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n\n\n\nReferences\n\n\nHugging Face\n\n\nYaRN: Efficient Context Window Extension of Large\nLanguage Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYarn Llama 2 is a model based on [Llama2](https://ollama.ai/library/llama2) that extends its context size up to 128k context. It is developed by [Nous Research](https://nousresearch.com) by implementing the YaRN method to further train the model to support larger context windows.\n\n## CLI\n\n64k context size: \n```\nollama run yarn-llama2\n``` \n\n128k context size: \n```\nollama run yarn-llama2:7b-128k\n```\n\n## API \n\nExample: \n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"yarn-llama2:7b-128k\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n```\n\n\n## References\n\n[Hugging Face](https://huggingface.co/NousResearch/Yarn-Llama-2-7b-64k)\n\n[YaRN: Efficient Context Window Extension of Large\nLanguage Models](https://arxiv.org/pdf/2309.00071.pdf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "yarn-llama2:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "64K",
        "context_window": 64000
      },
      {
        "tag": "yarn-llama2:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex text generation tasks.",
    "pulls": 359500,
    "tags": 67,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c060d2e6-95d5-4a96-be75-1372509d78d9",
    "model_identifier": "codegeex4",
    "model_name": "codegeex4",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codegeex4",
    "description": "A versatile model for AI software development scenarios, including code completion.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.2\n or later.\n\n\n\n\nCodeGeeX4 is an open multilingual code generation model continually trained on the \nGLM-4-9B\n, significantly enhancing its code generation capabilities.\n\n\nCodeGeeX4-ALL-9B has achieved highly competitive performance on public benchmarks, such as BigCodeBench and NaturalCodeBench. It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance.\n\n\nReferences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![logo.jpg](https://ollama.com/assets/library/codegeex4/b3a2d858-363f-4a6c-a9c7-056f74bc2910)\n\n> Note: this model requires [Ollama 0.2](https://github.com/ollama/ollama/releases/tag/v0.2.0) or later.\n\nCodeGeeX4 is an open multilingual code generation model continually trained on the [GLM-4-9B](https://ollama.com/library/glm4), significantly enhancing its code generation capabilities.\n\nCodeGeeX4-ALL-9B has achieved highly competitive performance on public benchmarks, such as BigCodeBench and NaturalCodeBench. It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance.\n\n# References\n\n[GitHub](https://github.com/THUDM/CodeGeeX4)\n\n[Hugging Face](https://huggingface.co/THUDM/codegeex4-all-9b)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "9b"
    ],
    "memory_requirements": [
      {
        "tag": "codegeex4:latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 5.5,
    "use_cases": [
      "Code Generation",
      "Text Summarization"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": null,
    "best_for": "Developers and researchers working with code generation tasks.",
    "pulls": 353300,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f4d7b91d-57aa-4706-a87e-b366fcb54395",
    "model_identifier": "mistral-openorca",
    "model_name": "mistral-openorca",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-openorca",
    "description": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.\n\n\nHuggingFace Leaderboard evals place this model as leader for all models smaller than 30B at the release time, outperforming all other 7B and 13B models.\n\n\nUsage\n\n\nCLI\n\n\nollama run mistral-openorca \"Why is the sky blue?\"\n\n\n\nAPI\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral-openorca\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n\n\n\nMore information\n\n\n\n\nHuggingFace repo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/e2ab7f70-f708-4b59-83af-6cf48f7115b5\" width=\"360\" />\n\n\nMistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset. \n\nHuggingFace Leaderboard evals place this model as leader for all models smaller than 30B at the release time, outperforming all other 7B and 13B models. \n\n## Usage\n\n### CLI\n\n```\nollama run mistral-openorca \"Why is the sky blue?\"\n```\n\n### API\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral-openorca\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n```\n\n## More information\n\n* [HuggingFace repo](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-openorca:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Question Answering",
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Conversational AI and text summarization tasks.",
    "pulls": 351700,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4beebac6-1e6c-4204-8f86-469e14c46289",
    "model_identifier": "tinydolphin",
    "model_name": "tinydolphin",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/tinydolphin",
    "description": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nTinyDolphin is an experimental model from training the \nTinyLlama\n model on the popular Dolphin dataset by Eric Hartford.\n\n\nReference\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/319ed8b3-8401-4923-9a09-ea388bb9a196\" width=\"280\" />\n\nTinyDolphin is an experimental model from training the [TinyLlama](https://ollama.ai/library/tinyllama) model on the popular Dolphin dataset by Eric Hartford.\n\n## Reference\n\n[Hugging Face](https://huggingface.co/cognitivecomputations/TinyDolphin-2.8-1.1b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.1b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Role Play"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Users needing a lightweight model for text generation and understanding.",
    "pulls": 351100,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4c45134d-28d1-4a69-a9cb-b3951169d4bf",
    "model_identifier": "orca2",
    "model_name": "orca2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/orca2",
    "description": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nOrca 2 models are built by Microsoft Research. They are fine-tuned on Meta’s Llama 2 using a synthetic dataset that was created to enhance the small model’s reasoning abilities. All synthetic training data was moderated using Microsoft Azure content filters.\n\n\nUse case\n\n\nOrca 2 is a helpful assistant, and provides an answer in tasks such as reasoning over your given data, reading comprehension, math problem solving and text summarization.\n\n\nThe model is designed to excel particularly in reasoning.\n\n\nMicrosoft Research’s intended purpose for this model is to encourage further research on the development, evaluation, and alignment of smaller language models.\n\n\nCLI\n\n\n7 billion parameter model:\n\n\nollama run orca2\n\n\n\n13 billion parameter model:\n\n\nollama run orca2:13b\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"orca2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nReferences\n\n\n\n\nMicrosoft Research announcement blog\n\n\nMicrosoft Research published paper, \nOrca 2: Teaching Small Language Models How to Reason\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrca 2 models are built by Microsoft Research. They are fine-tuned on Meta's Llama 2 using a synthetic dataset that was created to enhance the small model's reasoning abilities. All synthetic training data was moderated using Microsoft Azure content filters. \n\n### Use case \n\nOrca 2 is a helpful assistant, and provides an answer in tasks such as reasoning over your given data, reading comprehension, math problem solving and text summarization. \n\nThe model is designed to excel particularly in reasoning. \n\nMicrosoft Research's intended purpose for this model is to encourage further research on the development, evaluation, and alignment of smaller language models. \n\n### CLI\n\n7 billion parameter model: \n\n```\nollama run orca2\n```\n\n13 billion parameter model: \n\n```\nollama run orca2:13b\n```\n\n### API\n\nExample:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"orca2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n``` \n\n\n### References\n\n- [Microsoft Research announcement blog](https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/)\n\n- Microsoft Research published paper, [Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/pdf/2311.11045.pdf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "orca2:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "orca2:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "An assistant for tasks requiring reasoning and understanding of given data.",
    "pulls": 346400,
    "tags": 33,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1f295f4c-a89e-4594-9eb6-d5e1f767d37a",
    "model_identifier": "qwen3-next",
    "model_name": "qwen3-next",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3-next",
    "description": "The first installment in the Qwen3-Next series with strong performance in terms of both parameter efficiency and inference speed.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enhancements:\n\n\n\n\nHybrid Attention\n: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\n\n\nHigh-Sparsity Mixture-of-Experts (MoE)\n: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\n\n\nStability Optimizations\n: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\n\n\nMulti-Token Prediction (MTP)\n: Boosts pretraining model performance and accelerates inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/qwen3-next/5e94256e-8c4a-42bb-a335-34b61221360e)\n\nQwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enhancements:\n\n* **Hybrid Attention**: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\n* **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\n* **Stability Optimizations**: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\n* **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "80b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3-next:latest",
        "size": "50GB",
        "size_gb": 50.0,
        "recommended_ram_gb": 62.5,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "latest",
        "size": "50GB",
        "size_gb": 50.0,
        "recommended_ram_gb": 62.5,
        "quantization": "q4_k_m",
        "context": "256K",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 50.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Reasoning",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced language tasks.",
    "pulls": 340300,
    "tags": 10,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "135381f3-781d-4b93-a0e0-656e1a4844eb",
    "model_identifier": "stable-beluga",
    "model_name": "stable-beluga",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/stable-beluga",
    "description": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nStable Beluga is based on Llama 2 and then fine-tuned on an Orca-style dataset. It is available in 7B, 13B, and 70B parameter sizes. It was created by Stability AI.\n\n\nGet started with Stable Beluga\n\n\nThe model used in the example below is the Stable Beluga model, with 7b parameters, which is a general-use model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"stable-beluga\",\n  \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run stable-beluga\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull stable-beluga\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n70b models generally require at least 64GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n\n\n\n\n\n\nAliases\n\n\n\n\n\n\n\n\n\n\nlatest, 7b, 7b-q4_0\n\n\n\n\n\n\n13b, 13b-q4_0\n\n\n\n\n\n\n70b, 70b-q4_0\n\n\n\n\n\n\n\n\nModel source\n\n\nStable Beluga source on Ollama\n\n\n7b parameters original source:\n \nStability AI\n\n\n13b parameters original source:\n \nStability AI\n\n\n70b parameters original source:\n \nStability AI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/263119827-dbb7315b-554f-4865-86f4-44e1391ab460.png\" style=\"max-width:60%\" />\n\nStable Beluga is based on Llama 2 and then fine-tuned on an Orca-style dataset. It is available in 7B, 13B, and 70B parameter sizes. It was created by Stability AI.\n\n## Get started with Stable Beluga\n\nThe model used in the example below is the Stable Beluga model, with 7b parameters, which is a general-use model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"stable-beluga\",\n  \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run stable-beluga`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull stable-beluga`\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n- 70b models generally require at least 64GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n| **Aliases** |\n| --- |\n| latest, 7b, 7b-q4_0 |\n| 13b, 13b-q4_0 |\n| 70b, 70b-q4_0 |\n\n## Model source \n\n**Stable Beluga source on Ollama**\n\n7b parameters original source:\n [Stability AI](https://huggingface.co/stabilityai/StableBeluga-7B)\n\n13b parameters original source:\n [Stability AI](https://huggingface.co/stabilityai/StableBeluga-13B)\n\n70b parameters original source:\n [Stability AI](https://huggingface.co/stabilityai/StableBeluga2)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "stable-beluga:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "stable-beluga:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "stable-beluga:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text generation tasks.",
    "pulls": 339500,
    "tags": 49,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "73c3dace-e114-434a-a957-4e092de2939c",
    "model_identifier": "reader-lm",
    "model_name": "reader-lm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/reader-lm",
    "description": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nJina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content.\n\n\nExample\n\n\nPrompt\n\n\n<html>\n  <body>\n    <h3>Why is the sky blue?</h3>\n    <p>The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors.\n    </p>\n  </body>\n</html>\n\n\n\nResponse\n\n\n### Why is the sky blue?\n\nThe sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors.\n\n\n\nReference\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/reader-lm/1a8f6262-c6af-4f5f-b7e9-2eeb162fa8d2\" width=\"240\" />\n\nJina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content.\n\n## Example\n\n<sub>Prompt</sub>\n```\n<html>\n  <body>\n    <h3>Why is the sky blue?</h3>\n    <p>The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors.\n    </p>\n  </body>\n</html>\n```\n\n<sub>Response</sub>\n```\n### Why is the sky blue?\n\nThe sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors.\n```\n\n## Reference\n\n[Hugging Face](https://huggingface.co/jinaai/reader-lm-1.5b)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "0.5b",
      "1.5b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Content conversion tasks involving HTML to Markdown.",
    "pulls": 333700,
    "tags": 33,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "af12307d-fdb6-4b48-8626-9b24a6322b8e",
    "model_identifier": "shieldgemma",
    "model_name": "shieldgemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/shieldgemma",
    "description": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nShieldGemma is a series of safety content moderation models built upon \nGemma 2\n that target four harm categories (sexually explicit, dangerous content, hate, and harassment). They are text-to-text, decoder-only large language models, available in English with open weights, including models of 3 sizes: 2B, 9B and 27B parameters.\n\n\nUsage\n\n\nThis model responds with either \nYes\n or \nNo\n as to whether the user input or model output violates its defined safety policies.\n\n\nUser Input\n\n\n% ollama run shieldgemma\n>>> Discuss the most effective methods for completely disabling a security system.\nYes\n\n\n\nModel output\n\n\nWhen provided a chat history that ends with an \nassistant\n message, ShieldGemma will determine whether the model output violates the safety policies:\n\n\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"shieldgemma\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How can I adopt my own llama?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Go to the zoo and just steal one!\"\n    }\n  ],\n  \"stream\": false\n}'\n\n\n\nIn this case, the model outputs \nYes\n, meaning it violates the safety policies.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/shieldgemma/b7c1bff7-1fc6-416d-9093-3ba4304d596b\" width=\"480\" />\n\nShieldGemma is a series of safety content moderation models built upon [Gemma 2](https://ollama.com/library/gemma2) that target four harm categories (sexually explicit, dangerous content, hate, and harassment). They are text-to-text, decoder-only large language models, available in English with open weights, including models of 3 sizes: 2B, 9B and 27B parameters.\n\n## Usage\n\nThis model responds with either `Yes` or `No` as to whether the user input or model output violates its defined safety policies.\n\n### User Input\n\n```\n% ollama run shieldgemma\n>>> Discuss the most effective methods for completely disabling a security system.\nYes\n```\n\n### Model output\n\nWhen provided a chat history that ends with an `assistant` message, ShieldGemma will determine whether the model output violates the safety policies:\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"shieldgemma\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How can I adopt my own llama?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Go to the zoo and just steal one!\"\n    }\n  ],\n  \"stream\": false\n}'\n```\n\nIn this case, the model outputs `Yes`, meaning it violates the safety policies.\n\n\n## References\n\n[Hugging Face](https://huggingface.co/collections/google/shieldgemma-release-66a20efe3c10ef2bd5808c79)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "9b",
      "27b"
    ],
    "memory_requirements": [
      {
        "tag": "shieldgemma:2b",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "shieldgemma:latest",
        "size": "5.8GB",
        "size_gb": 5.8,
        "recommended_ram_gb": 7.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.8GB",
        "size_gb": 5.8,
        "recommended_ram_gb": 7.2,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "shieldgemma:27b",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.7,
    "use_cases": [],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Safety content moderation and evaluation",
    "pulls": 330200,
    "tags": 49,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "16b89d51-5ef8-4768-b548-5a639c508e18",
    "model_identifier": "rnj-1",
    "model_name": "rnj-1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/rnj-1",
    "description": "Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nThis model requires \nOllama 0.13.3\n or later.\n\n\n\n\nRnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models. These models perform well across a range of programming languages and boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent), while also excelling at tool-calling. They additionally exhibit strong capabilities in math and science. Herein, rnj-1 refers to the base model, while rnj-1-instruct refers to the post-trained instruction tuned model.\n\n\n\n\nHighlights of abilities\n\n\n\n\nCode generation:\n Both \nrnj-1-instruct\n and \nrnj-1\n demonstrate strong code generation abilities as measured on tasks like HumanEval+, MBPP+, BigCodeBench, and LiveCodeBench v6. Both models compete with the strongest open weight models, sometimes outperforming even larger models such as GPT OSS 20B. We measured code comprehension abilities using the task of predicting inputs given outputs and vice-versa, Crux-IO. We find our models outperform comparable baselines. For multi-lingual code generation capabilities across programming languages we measure MultiPL-E on 6 languages (C++, TypeScript, Java, JavaScript, Shell, PHP) and we find performance close to the strongest model.\n\n\nAgentic and Tool Use:\n \nrnj-1-instruct\n dominates the pack on agentic coding, one of our target abilities. SWE-bench performance is indicative of the model’s ability to tackle everyday software engineering tasks. The model is an order of magnitude stronger than comparably sized models on SWE-bench and approaches the capabilities available in much larger models. It scores \n20.8%\n on SWE-bench Verified in bash-only mode, which is higher than Gemini 2.0 flash and Qwen2.5-Coder 32B Instruct under the same agentic framework (\nleaderboard\n).\n\nThere is a surge of interest in developing models’ abilities to write performant code. \nrnj-1-instruct\n is able to use a profiler to iteratively improve the performance of the code it writes. For instance, on \nEnamel\n, which measures abilities to write efficient solutions to algorithmic problems, the model outperforms all other models under the same setting.\n\nFurthermore, \nrnj-1-instruct\n surpasses comparable models in tool use performance as measured by the Berkeley Functional Calling Leaderboard (BFCL).\n\n\nCode Infilling\n : Having specifically been trained on FIM-ed pre-training data, \nrnj-1\n exhibits strong infilling abilities, which have been further enhanced during post-training. The base model \nrnj-1\n scores highly on HE-FIM-Python (avg) at 82.49% and \nrnj-1-instruct\n achieves 86.21%.\n\n\nMathematical Problem Solving:\n \nrnj-1-instruct\n shows strong mathematical abilities across several levels of difficulty from elementary math (GSM8k), high school and undergraduate math (Minerva-MATH), and competition math (AIME ‘24 and ‘25). On harder subjects, it outcompetes or is on par with the strongest model in the pack.\n\n\nScientific Reasoning:\n \nrnj-1-instruct\n exhibits long-context reasoning abilities that are needed to solve hard science and technical questions in GPQA-Diamond and SuperGPQA.\n\n\n\n\nReference\n\n\nrnj-1 blog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/rnj-1/39181423-7841-47ee-bfe6-7b12966b45d1\" width=\"320\" />\n\n> This model requires [Ollama 0.13.3](https://github.com/ollama/ollama/releases/tag/v0.13.3-rc0) or later.\n\nRnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models. These models perform well across a range of programming languages and boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent), while also excelling at tool-calling. They additionally exhibit strong capabilities in math and science. Herein, rnj-1 refers to the base model, while rnj-1-instruct refers to the post-trained instruction tuned model.\n\n![image.png](/assets/library/rnj-1/d40fd93f-fe0c-4c6b-ac44-06b6c9b8ae8e)\n\n### Highlights of abilities\n\n- **Code generation:** Both `rnj-1-instruct` and `rnj-1` demonstrate strong code generation abilities as measured on tasks like HumanEval+, MBPP+, BigCodeBench, and LiveCodeBench v6. Both models compete with the strongest open weight models, sometimes outperforming even larger models such as GPT OSS 20B. We measured code comprehension abilities using the task of predicting inputs given outputs and vice-versa, Crux-IO. We find our models outperform comparable baselines. For multi-lingual code generation capabilities across programming languages we measure MultiPL-E on 6 languages (C++, TypeScript, Java, JavaScript, Shell, PHP) and we find performance close to the strongest model.\n- **Agentic and Tool Use:** `rnj-1-instruct` dominates the pack on agentic coding, one of our target abilities. SWE-bench performance is indicative of the model’s ability to tackle everyday software engineering tasks. The model is an order of magnitude stronger than comparably sized models on SWE-bench and approaches the capabilities available in much larger models. It scores `20.8%` on SWE-bench Verified in bash-only mode, which is higher than Gemini 2.0 flash and Qwen2.5-Coder 32B Instruct under the same agentic framework ([leaderboard](https://www.swebench.com/bash-only.html)).<br><br>\n    There is a surge of interest in developing models’ abilities to write performant code. `rnj-1-instruct` is able to use a profiler to iteratively improve the performance of the code it writes. For instance, on [Enamel](https://github.com/q-rz/enamel/tree/main), which measures abilities to write efficient solutions to algorithmic problems, the model outperforms all other models under the same setting.<br><br>\n    Furthermore, `rnj-1-instruct` surpasses comparable models in tool use performance as measured by the Berkeley Functional Calling Leaderboard (BFCL).\n- **Code Infilling** : Having specifically been trained on FIM-ed pre-training data, `rnj-1` exhibits strong infilling abilities, which have been further enhanced during post-training. The base model `rnj-1` scores highly on HE-FIM-Python (avg) at 82.49% and `rnj-1-instruct` achieves 86.21%.\n- **Mathematical Problem Solving:** `rnj-1-instruct` shows strong mathematical abilities across several levels of difficulty from elementary math (GSM8k), high school and undergraduate math (Minerva-MATH), and competition math (AIME ‘24 and ‘25). On harder subjects, it outcompetes or is on par with the strongest model in the pack.\n- **Scientific Reasoning:** `rnj-1-instruct` exhibits long-context reasoning abilities that are needed to solve hard science and technical questions in GPQA-Diamond and SuperGPQA.\n\n### Reference \n\n[rnj-1 blog post](https://www.essential.ai/research/rnj-1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "rnj-1:latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 5.1,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": null,
    "best_for": "Developers and researchers working with code and STEM tasks.",
    "pulls": 325100,
    "tags": 6,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "20091037-24e4-4e40-912a-861e81bd5791",
    "model_identifier": "llama-pro",
    "model_name": "llama-pro",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama-pro",
    "description": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLLaMa-Pro is a version of the original LLaMa model enhanced by the addition of transformer blocks by Tencent Applied Research Center (ARC). This model specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.\n\n\nReferences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/b6250249-0d8b-40ec-ba67-cce03d517dbb\" width=\"320\" />\n\nLLaMa-Pro is a version of the original LLaMa model enhanced by the addition of transformer blocks by Tencent Applied Research Center (ARC). This model specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.\n\n## References\n\n[GitHub](https://github.com/TencentARC/LLaMA-Pro)\n\n[Hugging Face](https://huggingface.co/TencentARC/LLaMA-Pro-8B-Instruct)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [],
    "memory_requirements": [
      {
        "tag": "llama-pro:8b-instruct-q2_K",
        "size": "3.5GB",
        "size_gb": 3.5,
        "recommended_ram_gb": 4.4,
        "quantization": "q2_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q2_K",
        "size": "3.5GB",
        "size_gb": 3.5,
        "recommended_ram_gb": 4.4,
        "quantization": "q2_k",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q3_K_S",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q3_k_s",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q3_K_S",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q3_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q3_K_M",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q3_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q3_K_M",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q3_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q3_K_L",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q3_k_l",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q3_K_L",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q3_k_l",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q4_K_S",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_s",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q4_K_S",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q4_K_M",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q4_K_M",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q4_1",
        "size": "5.3GB",
        "size_gb": 5.3,
        "recommended_ram_gb": 6.6,
        "quantization": "q4_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q4_1",
        "size": "5.3GB",
        "size_gb": 5.3,
        "recommended_ram_gb": 6.6,
        "quantization": "q4_1",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q5_0",
        "size": "5.8GB",
        "size_gb": 5.8,
        "recommended_ram_gb": 7.2,
        "quantization": "q5_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q5_K_S",
        "size": "5.8GB",
        "size_gb": 5.8,
        "recommended_ram_gb": 7.2,
        "quantization": "q5_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q5_K_M",
        "size": "5.9GB",
        "size_gb": 5.9,
        "recommended_ram_gb": 7.4,
        "quantization": "q5_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q5_K_M",
        "size": "5.9GB",
        "size_gb": 5.9,
        "recommended_ram_gb": 7.4,
        "quantization": "q5_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q5_1",
        "size": "6.3GB",
        "size_gb": 6.3,
        "recommended_ram_gb": 7.9,
        "quantization": "q5_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q5_1",
        "size": "6.3GB",
        "size_gb": 6.3,
        "recommended_ram_gb": 7.9,
        "quantization": "q5_1",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q6_K",
        "size": "6.9GB",
        "size_gb": 6.9,
        "recommended_ram_gb": 8.6,
        "quantization": "q6_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q6_K",
        "size": "6.9GB",
        "size_gb": 6.9,
        "recommended_ram_gb": 8.6,
        "quantization": "q6_k",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-q8_0",
        "size": "8.9GB",
        "size_gb": 8.9,
        "recommended_ram_gb": 11.1,
        "quantization": "q8_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-q8_0",
        "size": "8.9GB",
        "size_gb": 8.9,
        "recommended_ram_gb": 11.1,
        "quantization": "q8_0",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-instruct-fp16",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "fp16",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama-pro:8b-text-fp16",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "fp16",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.5,
    "use_cases": [
      "Code Generation",
      "Question Answering",
      "Math"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working on programming and mathematics tasks.",
    "pulls": 325900,
    "tags": 33,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "15287bef-eabf-42fb-8ba1-b24eb7e16a8d",
    "model_identifier": "yarn-mistral",
    "model_name": "yarn-mistral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/yarn-mistral",
    "description": "An extension of Mistral to support context windows of 64K or 128K.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nYarn Mistral is a model based on \nMistral\n that extends its context size up to 128k context. It is developed by \nNous Research\n by implementing the YaRN method to further train the model to support larger context windows.\n\n\nCLI\n\n\n64k context size:\n\n\nollama run yarn-mistral\n\n\n\n128k context size:\n\n\nollama run yarn-mistral:7b-128k\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"yarn-mistral:7b-128k\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n\n\n\nReferences\n\n\nHugging Face\n\n\nYaRN: Efficient Context Window Extension of Large\nLanguage Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYarn Mistral is a model based on [Mistral](https://ollama.ai/library/mistral) that extends its context size up to 128k context. It is developed by [Nous Research](https://nousresearch.com) by implementing the YaRN method to further train the model to support larger context windows.\n\n## CLI \n\n64k context size: \n```\nollama run yarn-mistral\n``` \n\n128k context size: \n```\nollama run yarn-mistral:7b-128k\n```\n\n## API \n\nExample: \n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"yarn-mistral:7b-128k\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n```\n\n## References\n\n[Hugging Face](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k)\n\n[YaRN: Efficient Context Window Extension of Large\nLanguage Models](https://arxiv.org/pdf/2309.00071.pdf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "yarn-mistral:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 322800,
    "tags": 33,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d4a2de95-ba70-45f7-ac64-573c14a98551",
    "model_identifier": "nexusraven",
    "model_name": "nexusraven",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nexusraven",
    "description": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNexus Raven is a 13 billion parameter model designed for function calling tasks.\n\n\nUpdated to NexusRaven-V2, this model is an updated open-source and commercially viable function calling model:\n\n\n\n\nVersatile Function Calling Capability: capable of generating single function calls, nested calls, and parallel calls in many challenging cases.\n\n\nFully Explainable: capable of generating very detailed explanations for the function calls it generates. This behavior can be turned off, to save tokens during inference.\n\n\nPerformance Highlights: surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.\n\n\nGeneralization to the Unseen: has never been trained on the functions used in evaluation.\n\n\nCommercially Permissive: The training does not involve any data generated by proprietary LLMs such as GPT-4. You have full control of the model when deployed in commercial applications.\n\n\n\n\nExample prompt\n\n\nFunction:\ndef get_weather_data(coordinates):\n    \"\"\"\n    Fetches weather data from the Open-Meteo API for the given latitude and longitude.\n\n    Args:\n    coordinates (tuple): The latitude of the location.\n\n    Returns:\n    float: The current temperature in the coordinates you've asked for\n    \"\"\"\n\nFunction:\ndef get_coordinates_from_city(city_name):\n    \"\"\"\n    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.\n\n    Args:\n    city_name (str): The name of the city.\n\n    Returns:\n    tuple: The latitude and longitude of the city.\n    \"\"\"\n\nUser Query: {query}<human_end>\n\n\n\nThis will generate a response similar to:\n\n\nCall: get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))<bot_end>\nThought: The function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by following these steps:\n\n1. `get_coordinates_from_city(city_name='Seattle')`: This function call fetches the latitude and longitude of the city \"Seattle\" using the Maps.co Geocoding API.\n2. `get_weather_data(coordinates=...)`: This function call fetches the current weather data for the coordinates returned by the previous function call.\n\nTherefore, the function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by first fetching the coordinates of the city \"Seattle\" and then fetching the current weather data for those coordinates.\n\n\n\nReferences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/521dce62-4924-447b-b651-7457469e99bb\" width=\"320\"/>\n\nNexus Raven is a 13 billion parameter model designed for function calling tasks. \n\nUpdated to NexusRaven-V2, this model is an updated open-source and commercially viable function calling model:\n\n- Versatile Function Calling Capability: capable of generating single function calls, nested calls, and parallel calls in many challenging cases.\n- Fully Explainable: capable of generating very detailed explanations for the function calls it generates. This behavior can be turned off, to save tokens during inference.\n- Performance Highlights: surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.\n- Generalization to the Unseen: has never been trained on the functions used in evaluation.\n- Commercially Permissive: The training does not involve any data generated by proprietary LLMs such as GPT-4. You have full control of the model when deployed in commercial applications.\n\n### Example prompt\n\n```\nFunction:\ndef get_weather_data(coordinates):\n    \"\"\"\n    Fetches weather data from the Open-Meteo API for the given latitude and longitude.\n\n    Args:\n    coordinates (tuple): The latitude of the location.\n\n    Returns:\n    float: The current temperature in the coordinates you've asked for\n    \"\"\"\n\nFunction:\ndef get_coordinates_from_city(city_name):\n    \"\"\"\n    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.\n\n    Args:\n    city_name (str): The name of the city.\n\n    Returns:\n    tuple: The latitude and longitude of the city.\n    \"\"\"\n\nUser Query: {query}<human_end>\n```\n\nThis will generate a response similar to:\n\n```\nCall: get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))<bot_end>\nThought: The function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by following these steps:\n\n1. `get_coordinates_from_city(city_name='Seattle')`: This function call fetches the latitude and longitude of the city \"Seattle\" using the Maps.co Geocoding API.\n2. `get_weather_data(coordinates=...)`: This function call fetches the current weather data for the coordinates returned by the previous function call.\n\nTherefore, the function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by first fetching the coordinates of the city \"Seattle\" and then fetching the current weather data for those coordinates.\n```\n\n## References\n\n[GitHub](https://github.com/nexusflowai/NexusRaven-V2)\n\n[Hugging Face](https://huggingface.co/Nexusflow/NexusRaven-13B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "nexusraven:latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 7.4,
    "use_cases": [
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Handling complex function calls and generating detailed explanations.",
    "pulls": 320200,
    "tags": 32,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "cc5a2e42-9cd5-4550-b443-4f4105ee84af",
    "model_identifier": "wizardlm",
    "model_name": "wizardlm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizardlm",
    "description": "General use model based on Llama 2.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nWizardLM is a 70B parameter model based on Llama 2 trained by WizardLM.\n\n\nGet started with WizardLM\n\n\nThe model used in the example below is the WizardLM model, with 70b parameters, which is a general-use model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizardlm:70b-llama2-q4_0\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run wizardlm:70b-llama2-q4_0\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull wizardlm:70b-llama2-q4_0\n\n\nMemory requirements\n\n\n\n\n70b models generally require at least 64GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\nModel source\n\n\nWizardLM source on Ollama\n\n\n70b parameters source:\n \nThe Bloke\n\n\n70b parameters original source:\n \nWizardLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/263121166-8b16834a-1da2-4a68-8aac-13d19775e7d7.png\" style=\"max-width:60%\" />\n\nWizardLM is a 70B parameter model based on Llama 2 trained by WizardLM.\n\n## Get started with WizardLM\n\nThe model used in the example below is the WizardLM model, with 70b parameters, which is a general-use model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizardlm:70b-llama2-q4_0\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run wizardlm:70b-llama2-q4_0`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull wizardlm:70b-llama2-q4_0`\n\n## Memory requirements\n\n- 70b models generally require at least 64GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n## Model source \n\n\n\n**WizardLM source on Ollama**\n\n70b parameters source:\n [The Bloke](https://huggingface.co/TheBloke/WizardLM-70B-V1.0-GGML)\n\n70b parameters original source:\n [WizardLM](https://huggingface.co/WizardLM/WizardLM-70B-V1.0)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [],
    "memory_requirements": [
      {
        "tag": "wizardlm:7b-q2_K",
        "size": "2.8GB",
        "size_gb": 2.8,
        "recommended_ram_gb": 3.5,
        "quantization": "q2_k",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q3_K_S",
        "size": "2.9GB",
        "size_gb": 2.9,
        "recommended_ram_gb": 3.6,
        "quantization": "q3_k_s",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q3_K_M",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q3_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q3_K_L",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q3_k_l",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q4_0",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q4_K_S",
        "size": "3.9GB",
        "size_gb": 3.9,
        "recommended_ram_gb": 4.9,
        "quantization": "q4_k_s",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q4_K_M",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q4_1",
        "size": "4.2GB",
        "size_gb": 4.2,
        "recommended_ram_gb": 5.2,
        "quantization": "q4_1",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q5_0",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q5_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q5_K_S",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q5_k_s",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q5_K_M",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q5_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q5_1",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q5_1",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q2_K",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q2_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q2_K",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q2_k",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q6_K",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q6_k",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q3_K_S",
        "size": "5.7GB",
        "size_gb": 5.7,
        "recommended_ram_gb": 7.1,
        "quantization": "q3_k_s",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q3_K_S",
        "size": "5.7GB",
        "size_gb": 5.7,
        "recommended_ram_gb": 7.1,
        "quantization": "q3_k_s",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q3_K_M",
        "size": "6.3GB",
        "size_gb": 6.3,
        "recommended_ram_gb": 7.9,
        "quantization": "q3_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q3_K_M",
        "size": "6.3GB",
        "size_gb": 6.3,
        "recommended_ram_gb": 7.9,
        "quantization": "q3_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q3_K_L",
        "size": "6.9GB",
        "size_gb": 6.9,
        "recommended_ram_gb": 8.6,
        "quantization": "q3_k_l",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q3_K_L",
        "size": "6.9GB",
        "size_gb": 6.9,
        "recommended_ram_gb": 8.6,
        "quantization": "q3_k_l",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-q8_0",
        "size": "7.2GB",
        "size_gb": 7.2,
        "recommended_ram_gb": 9.0,
        "quantization": "q8_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q4_0",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-llama2-q4_K_S",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q4_0",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-q4_K_S",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_s",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q4_K_M",
        "size": "7.9GB",
        "size_gb": 7.9,
        "recommended_ram_gb": 9.9,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q4_K_M",
        "size": "7.9GB",
        "size_gb": 7.9,
        "recommended_ram_gb": 9.9,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q4_1",
        "size": "8.2GB",
        "size_gb": 8.2,
        "recommended_ram_gb": 10.2,
        "quantization": "q4_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q4_1",
        "size": "8.2GB",
        "size_gb": 8.2,
        "recommended_ram_gb": 10.2,
        "quantization": "q4_1",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q5_0",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q5_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-llama2-q5_K_S",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q5_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q5_0",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q5_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-q5_K_S",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q5_k_s",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q5_K_M",
        "size": "9.2GB",
        "size_gb": 9.2,
        "recommended_ram_gb": 11.5,
        "quantization": "q5_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q5_K_M",
        "size": "9.2GB",
        "size_gb": 9.2,
        "recommended_ram_gb": 11.5,
        "quantization": "q5_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q5_1",
        "size": "9.8GB",
        "size_gb": 9.8,
        "recommended_ram_gb": 12.2,
        "quantization": "q5_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q5_1",
        "size": "9.8GB",
        "size_gb": 9.8,
        "recommended_ram_gb": 12.2,
        "quantization": "q5_1",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q6_K",
        "size": "11GB",
        "size_gb": 11.0,
        "recommended_ram_gb": 13.8,
        "quantization": "q6_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q6_K",
        "size": "11GB",
        "size_gb": 11.0,
        "recommended_ram_gb": 13.8,
        "quantization": "q6_k",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:7b-fp16",
        "size": "13GB",
        "size_gb": 13.0,
        "recommended_ram_gb": 16.2,
        "quantization": "fp16",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-q8_0",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q8_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-q8_0",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q8_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q2_K",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q2_k",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q3_K_M",
        "size": "16GB",
        "size_gb": 16.0,
        "recommended_ram_gb": 20.0,
        "quantization": "q3_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q3_K_L",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "q3_k_l",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q4_0",
        "size": "18GB",
        "size_gb": 18.0,
        "recommended_ram_gb": 22.5,
        "quantization": "q4_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q4_K_S",
        "size": "18GB",
        "size_gb": 18.0,
        "recommended_ram_gb": 22.5,
        "quantization": "q4_k_s",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q4_1",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_1",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q4_K_M",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q5_0",
        "size": "22GB",
        "size_gb": 22.0,
        "recommended_ram_gb": 27.5,
        "quantization": "q5_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q5_K_S",
        "size": "22GB",
        "size_gb": 22.0,
        "recommended_ram_gb": 27.5,
        "quantization": "q5_k_s",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q5_K_M",
        "size": "23GB",
        "size_gb": 23.0,
        "recommended_ram_gb": 28.8,
        "quantization": "q5_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q5_1",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q5_1",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:13b-llama2-fp16",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "fp16",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:13b-fp16",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "fp16",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:30b-q6_K",
        "size": "27GB",
        "size_gb": 27.0,
        "recommended_ram_gb": 33.8,
        "quantization": "q6_k",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:70b-llama2-q2_K",
        "size": "29GB",
        "size_gb": 29.0,
        "recommended_ram_gb": 36.2,
        "quantization": "q2_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q3_K_S",
        "size": "30GB",
        "size_gb": 30.0,
        "recommended_ram_gb": 37.5,
        "quantization": "q3_k_s",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q3_K_M",
        "size": "33GB",
        "size_gb": 33.0,
        "recommended_ram_gb": 41.2,
        "quantization": "q3_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:30b-q8_0",
        "size": "35GB",
        "size_gb": 35.0,
        "recommended_ram_gb": 43.8,
        "quantization": "q8_0",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:70b-llama2-q3_K_L",
        "size": "36GB",
        "size_gb": 36.0,
        "recommended_ram_gb": 45.0,
        "quantization": "q3_k_l",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q4_0",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q4_K_S",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q4_K_M",
        "size": "41GB",
        "size_gb": 41.0,
        "recommended_ram_gb": 51.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q4_1",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q5_0",
        "size": "47GB",
        "size_gb": 47.0,
        "recommended_ram_gb": 58.8,
        "quantization": "q5_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q5_K_S",
        "size": "47GB",
        "size_gb": 47.0,
        "recommended_ram_gb": 58.8,
        "quantization": "q5_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q5_K_M",
        "size": "49GB",
        "size_gb": 49.0,
        "recommended_ram_gb": 61.2,
        "quantization": "q5_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:70b-llama2-q6_K",
        "size": "57GB",
        "size_gb": 57.0,
        "recommended_ram_gb": 71.2,
        "quantization": "q6_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "wizardlm:30b-fp16",
        "size": "65GB",
        "size_gb": 65.0,
        "recommended_ram_gb": 81.2,
        "quantization": "fp16",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "wizardlm:70b-llama2-q8_0",
        "size": "73GB",
        "size_gb": 73.0,
        "recommended_ram_gb": 91.2,
        "quantization": "q8_0",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.8,
    "use_cases": [
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Advanced users for text generation and question answering tasks.",
    "pulls": 315000,
    "tags": 73,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8af253a1-8f00-450f-ac4d-20a6bce714b7",
    "model_identifier": "meditron",
    "model_name": "meditron",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/meditron",
    "description": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMeditron is a large language model adapted from Llama 2 to the medical domain through training on a corpus of medical data, papers and guidelines. It outperforms Llama 2, GPT 3.5 and Flan-PaLM on many medical reasoning tasks.\n\n\nPotential use cases include:\n\n\n\n\nMedical exam question answering\n\n\nSupporting differential diagnosis\n\n\nDisease information (symptoms, cause, treatment) query\n\n\nGeneral health information query\n\n\n\n\nExample prompts\n\n\nWhat are the symptoms of the common cold?\n\n\n\nWhat causes the seasonal flu?\n\n\n\nWhat medication would be prescribed for a headache?\n\n\n\nReferences\n\n\nMEDITRON-70B: Scaling Medical Pretraining for Large Language Models\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/0ba1061e-ddaf-4b3f-abd3-684a31d19fa5\" width=\"240\" />\n\nMeditron is a large language model adapted from Llama 2 to the medical domain through training on a corpus of medical data, papers and guidelines. It outperforms Llama 2, GPT 3.5 and Flan-PaLM on many medical reasoning tasks.\n\nPotential use cases include:\n\n* Medical exam question answering\n* Supporting differential diagnosis\n* Disease information (symptoms, cause, treatment) query\n* General health information query\n\n## Example prompts\n\n```\nWhat are the symptoms of the common cold?\n```\n\n```\nWhat causes the seasonal flu?\n```\n\n```\nWhat medication would be prescribed for a headache?\n```\n\n\n## References\n\n[MEDITRON-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079)\n\n[GitHub](https://github.com/epfLLM/meditron)\n\n[HuggingFace](https://huggingface.co/epfl-llm/meditron-7b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "meditron:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      },
      {
        "tag": "meditron:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Reasoning",
      "Translation",
      "Function Calling"
    ],
    "domain": "Medical",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Developers working in the medical field.",
    "pulls": 300200,
    "tags": 22,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "e229b897-7e74-43fb-876f-b351e916a753",
    "model_identifier": "reflection",
    "model_name": "reflection",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/reflection",
    "description": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nDuring sampling, the model will start by outputting reasoning inside \n<thinking>\n and \n</thinking>\n tags, and then once it is satisfied with its reasoning, it will output the final answer inside \n<output>\n and \n</output>\n tags. Each of these tags are special tokens, trained into the model.\n\n\nThis enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user.\n\n\nInside the \n<thinking>\n section, the model may output one or more \n<reflection>\n tags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuring sampling, the model will start by outputting reasoning inside `<thinking>` and `</thinking>` tags, and then once it is satisfied with its reasoning, it will output the final answer inside `<output>` and `</output>` tags. Each of these tags are special tokens, trained into the model.\n\nThis enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user.\n\nInside the `<thinking>` section, the model may output one or more `<reflection>` tags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer.\n\n## References\n\n[Hugging Face](https://huggingface.co/mattshumer/ref_70_e3)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "reflection:latest",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 40.0,
    "use_cases": [],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Role Play",
    "pulls": 285300,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4b81c7e4-73cd-4d71-9663-f2dd7b4179d2",
    "model_identifier": "glm-4.7-flash",
    "model_name": "glm-4.7-flash",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/glm-4.7-flash",
    "description": "As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.14.3\n which is currently in pre-release.\n\n\n\n\nIntroduction\n\n\nGLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.\n\n\nPerformances on Benchmarks\n\n\n\n\n\n\n\n\nBenchmark\n\n\nGLM-4.7-Flash\n\n\nQwen3-30B-A3B-Thinking-2507\n\n\nGPT-OSS-20B\n\n\n\n\n\n\n\n\n\n\nAIME 25\n\n\n91.6\n\n\n85.0\n\n\n91.7\n\n\n\n\n\n\nGPQA\n\n\n75.2\n\n\n73.4\n\n\n71.5\n\n\n\n\n\n\nLCB v6\n\n\n64.0\n\n\n66.0\n\n\n61.0\n\n\n\n\n\n\nHLE\n\n\n14.4\n\n\n9.8\n\n\n10.9\n\n\n\n\n\n\nSWE-bench Verified\n\n\n59.2\n\n\n22.0\n\n\n34.0\n\n\n\n\n\n\nτ²-Bench\n\n\n79.5\n\n\n49.0\n\n\n47.7\n\n\n\n\n\n\nBrowseComp\n\n\n42.8\n\n\n2.29\n\n\n28.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/glm-4.7-flash/8e1f3c2e-cfb1-4516-a57c-312b7daac14a\" width=\"128\" />\n\n> Note: this model requires [Ollama 0.14.3](https://github.com/ollama/ollama/releases) which is currently in pre-release.\n\n## Introduction\n\nGLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.\n\n\n### Performances on Benchmarks\n\n\n| Benchmark          | GLM-4.7-Flash | Qwen3-30B-A3B-Thinking-2507 | GPT-OSS-20B |\n|--------------------|---------------|-----------------------------|-------------|\n| AIME 25            | 91.6          | 85.0                        | 91.7        |\n| GPQA               | 75.2          | 73.4                        | 71.5        |\n| LCB v6             | 64.0          | 66.0                        | 61.0        |\n| HLE                | 14.4          | 9.8                         | 10.9        |\n| SWE-bench Verified | 59.2          | 22.0                        | 34.0        |\n| τ²-Bench           | 79.5          | 49.0                        | 47.7        |\n| BrowseComp         | 42.8          | 2.29                        | 28.3        |\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking"
    ],
    "memory_requirements": [
      {
        "tag": "glm-4.7-flash:latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "198K context",
        "context_window": 198000
      },
      {
        "tag": "glm-4.7-flash:q4_K_M",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "198K",
        "context_window": 198000
      },
      {
        "tag": "glm-4.7-flash:q8_0",
        "size": "32GB",
        "size_gb": 32.0,
        "recommended_ram_gb": 40.0,
        "quantization": "q8_0",
        "context": "198K context",
        "context_window": 198000
      },
      {
        "tag": "glm-4.7-flash:bf16",
        "size": "60GB",
        "size_gb": 60.0,
        "recommended_ram_gb": 75.0,
        "quantization": "bf16",
        "context": "198K context",
        "context_window": 198000
      }
    ],
    "min_ram_gb": 19.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced language tasks.",
    "pulls": 289900,
    "tags": 4,
    "last_updated": "2026-01-25",
    "last_updated_str": "1 month ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c892a191-121d-4527-93da-ed737ecfca8c",
    "model_identifier": "nemotron-mini",
    "model_name": "nemotron-mini",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nemotron-mini",
    "description": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNemotron-Mini-4B-Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling. It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment.\n\n\nThis instruct model is optimized for roleplay, RAG QA, and function calling in English. It supports a context length of 4,096 tokens. This model is ready for commercial use.\n\n\nReferences\n\n\nBlog\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/nemotron-mini/4e0ea563-a507-45e7-ad0b-b083918ec11b\" width=\"320\" />\n\nNemotron-Mini-4B-Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling. It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment.\n\nThis instruct model is optimized for roleplay, RAG QA, and function calling in English. It supports a context length of 4,096 tokens. This model is ready for commercial use.\n\n## References\n\n[Blog](https://blogs.nvidia.com/blog/digital-human-technology-mecha-break/)\n\n[HuggingFace](https://huggingface.co/nvidia/Nemotron-Mini-4B-Instruct)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "4b"
    ],
    "memory_requirements": [
      {
        "tag": "nemotron-mini:latest",
        "size": "2.7GB",
        "size_gb": 2.7,
        "recommended_ram_gb": 3.4,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "2.7GB",
        "size_gb": 2.7,
        "recommended_ram_gb": 3.4,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.7,
    "use_cases": [
      "Function Calling",
      "Role Play"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Ideal for commercial use in roleplay and function calling applications.",
    "pulls": 278600,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "afc55398-09ec-48f0-9d51-cd97ba248805",
    "model_identifier": "granite3.2",
    "model_name": "granite3.2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3.2",
    "description": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGranite-3.2 is a family of long-context AI models fine-tuned for thinking capabilities. Built on top of Granite-3.1, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The models allow controllability of its thinking capability, ensuring it is applied only when required.\n\n\n\n\nDevelopers:\n Granite Team, IBM\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: February 26th, 2025\n\n\nLicense:\n \nApache 2.0\n\n\n\n\nSizes\n\n\nThe models are available in two parameter sizes, 2B and 8B:\n\n\nollama run granite3.2:2b\n\n\n\nollama run granite3.2:8b\n\n\n\nSupported Languages:\n\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.\n\n\nIntended Use:\n\nThese models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\n\n\nCapabilities\n\n\n\n\nThinking\n\n\nSummarization\n\n\nText classification\n\n\nText extraction\n\n\nQuestion-answering\n\n\nRetrieval Augmented Generation (RAG)\n\n\nCode related tasks\n\n\nFunction-calling tasks\n\n\nMultilingual dialog use cases\n\n\nLong-context tasks including long document/meeting summarization, long document QA, etc.\n\n\n\n\nThinking\n\n\nTo enable thinking, add a message with \n\"role\": \"control\"\n and set \n\"content\"\n to \n\"thinking\"\n. For example:\n\n\n{\n    \"messages\": [\n        {\"role\": \"control\", \"content\": \"thinking\"},\n        {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"}\n    ]\n}\n\n\n\nReferences\n\n\nWebsite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/granite3.2/90c5e567-0004-425c-a17a-1b846c2b5d3d\" width=\"600\" />\n\n\nGranite-3.2 is a family of long-context AI models fine-tuned for thinking capabilities. Built on top of Granite-3.1, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The models allow controllability of its thinking capability, ensuring it is applied only when required.\n\n- **Developers:** Granite Team, IBM\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: February 26th, 2025\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n**Sizes**\n\nThe models are available in two parameter sizes, 2B and 8B:\n\n```\nollama run granite3.2:2b\n```\n\n```\nollama run granite3.2:8b\n```\n\n**Supported Languages:**\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.\n\n**Intended Use:** \nThese models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\n\n**Capabilities**\n\n* **Thinking**\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related tasks\n* Function-calling tasks\n* Multilingual dialog use cases\n* Long-context tasks including long document/meeting summarization, long document QA, etc.\n\n**Thinking**\n\nTo enable thinking, add a message with `\"role\": \"control\"` and set `\"content\"` to `\"thinking\"`. For example:\n\n```\n{\n    \"messages\": [\n        {\"role\": \"control\", \"content\": \"thinking\"},\n        {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"}\n    ]\n}\n```\n\n## References\n\n[Website](https://www.ibm.com/granite)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "2b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3.2:2b",
        "size": "1.5GB",
        "size_gb": 1.5,
        "recommended_ram_gb": 1.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "granite3.2:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.5,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Reasoning",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "German",
      "Spanish",
      "French"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for advanced language tasks.",
    "pulls": 271300,
    "tags": 9,
    "last_updated": "2025-02-25",
    "last_updated_str": "12 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f95b73fe-0b24-46f7-b73f-91977997d765",
    "model_identifier": "wizardlm-uncensored",
    "model_name": "wizardlm-uncensored",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizardlm-uncensored",
    "description": "Uncensored version of Wizard LM model",
    "readme": "Readme\n\n\n\n\n\n\n\n\nWizardLM Uncensored is a 13B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\n\n\nGet started with WizardLM Uncensored\n\n\nThe model used in the example below is the WizardLM Uncensored model, with 13b parameters, which is a general-use model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizardlm-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run wizardlm-uncensored\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull wizardlm-uncensored\n\n\nMemory requirements\n\n\n\n\n13b models generally require at least 16GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n\n\n\n\n\n\nAliases\n\n\n\n\n\n\n\n\n\n\nlatest, 13b, 13b-llama2, 13b-llama2-q4_0\n\n\n\n\n\n\n\n\nModel source\n\n\nWizardLM Uncensored source on Ollama\n\n\n13b parameters source:\n \nThe Bloke\n\n\n13b parameters original source:\n \nEric Hartford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWizardLM Uncensored is a 13B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\n\n## Get started with WizardLM Uncensored\n\nThe model used in the example below is the WizardLM Uncensored model, with 13b parameters, which is a general-use model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizardlm-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run wizardlm-uncensored`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull wizardlm-uncensored`\n\n## Memory requirements\n\n- 13b models generally require at least 16GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n| **Aliases** |\n| --- |\n| latest, 13b, 13b-llama2, 13b-llama2-q4_0 |\n\n## Model source \n\n\n\n**WizardLM Uncensored source on Ollama**\n\n13b parameters source:\n [The Bloke](https://huggingface.co/TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GGML)\n\n13b parameters original source:\n [Eric Hartford](https://huggingface.co/ehartford/WizardLM-1.0-Uncensored-Llama2-13b)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "wizardlm-uncensored:latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 7.4,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Creative Writing"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 266700,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "99e612ee-96e3-48e9-835c-fed82b12af78",
    "model_identifier": "athene-v2",
    "model_name": "athene-v2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/athene-v2",
    "description": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nAthene-V2\n\n\nNexusflow’s Athene-V2 chat model, built on Qwen 2.5’s 72B foundation, achieves GPT-4o-level performance across key benchmarks while demonstrating how targeted optimization can enhance specific capabilities beyond traditional scaling approaches.\n\n\nModel Features\n\n\n\n\n72B parameters\n fine-tuned from Qwen 2.5\n\n\nState-of-the-art chat performance\n matching or exceeding GPT-4o\n\n\nSuperior code completion\n (ranking #2 on bigcode-bench-hard)\n\n\nEnhanced mathematics capabilities\n (MATH benchmark)\n\n\nPrecise long-form log extraction\n\n\nAdvanced post-training pipeline\n pushing the Pareto frontier\n\n\n\n\n\n\nReferences\n\n\nBlog post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Athene-V2\n\nNexusflow's Athene-V2 chat model, built on Qwen 2.5's 72B foundation, achieves GPT-4o-level performance across key benchmarks while demonstrating how targeted optimization can enhance specific capabilities beyond traditional scaling approaches.\n\n## Model Features\n- **72B parameters** fine-tuned from Qwen 2.5\n- **State-of-the-art chat performance** matching or exceeding GPT-4o\n- **Superior code completion** (ranking #2 on bigcode-bench-hard)\n- **Enhanced mathematics capabilities** (MATH benchmark)\n- **Precise long-form log extraction**\n- **Advanced post-training pipeline** pushing the Pareto frontier\n\n<img src=\"https://ollama.com/assets/library/athene-v2/99424f56-1e23-4f31-8dfa-4fc46eb6305f\" width=\"460\" />\n\n## References\n[Blog post](https://nexusflow.ai/blogs/athene-v2)\n\n[HuggingFace](https://huggingface.co/Nexusflow/Athene-V2-Chat)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "athene-v2:latest",
        "size": "47GB",
        "size_gb": 47.0,
        "recommended_ram_gb": 58.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "47GB",
        "size_gb": 47.0,
        "recommended_ram_gb": 58.8,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 47.0,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering",
      "Math",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex text generation tasks.",
    "pulls": 264800,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c0e1143f-53ce-451f-8fd2-d76135ca268d",
    "model_identifier": "nemotron",
    "model_name": "nemotron",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nemotron",
    "description": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\n\n\nThis model was trained using RLHF (specifically, REINFORCE), \nLlama-3.1-Nemotron-70B-Reward\n and \nHelpSteer2-Preference prompts\n on a \nLlama-3.1-70B-Instruct\n model as the initial policy.\n\n\n$ ollama run nemotron\n>>> How many r in strawberry?\nA sweet question!\n\nLet's count the \"R\"s in \"strawberry\":\n\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\n\nThere are **3** \"R\"s in the word \"strawberry\".\n\n\n\nReference\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/nemotron-mini/4e0ea563-a507-45e7-ad0b-b083918ec11b\" width=\"320\" />\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. \n\nThis model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a [Llama-3.1-70B-Instruct](https://ai.meta.com/blog/meta-llama-3-1/) model as the initial policy.\n\n```\n$ ollama run nemotron\n>>> How many r in strawberry?\nA sweet question!\n\nLet's count the \"R\"s in \"strawberry\":\n\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\n\nThere are **3** \"R\"s in the word \"strawberry\".\n```\n\n## Reference\n[Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "nemotron:latest",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 43.0,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 263100,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1945a568-8058-45dd-b64d-7fa8a53494b8",
    "model_identifier": "exaone3.5",
    "model_name": "exaone3.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/exaone3.5",
    "description": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nEXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. EXAONE 3.5 language models include:\n\n\n\n\n2.4B model optimized for deployment on small or resource-constrained devices\n\n\n7.8B model matching the size of its predecessor but offering improved performance\n\n\n32B model delivering powerful performance.\n\n\n\n\nAll models support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in real-world use cases and long-context understanding, while remaining competitive in general domains compared to recently released models of similar sizes.\n\n\n\n\nReferences\n\n\nPaper\n\n\nHugging Face\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/exaone3.5/fb3ee4b4-a180-4d81-8896-fef74b2e1662\" width=\"300\" />\n\nEXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. EXAONE 3.5 language models include:\n\n1. 2.4B model optimized for deployment on small or resource-constrained devices\n2. 7.8B model matching the size of its predecessor but offering improved performance\n3. 32B model delivering powerful performance.\n\nAll models support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in real-world use cases and long-context understanding, while remaining competitive in general domains compared to recently released models of similar sizes.\n\n![benchmarks](/assets/library/exaone3.5/e01a3df2-72b4-408b-bae8-975e59821303)\n\n## References\n\n[Paper](https://arxiv.org/abs/2412.04862)\n\n[Hugging Face](https://huggingface.co/collections/LGAI-EXAONE/exaone-35-674d0e1bb3dcd2ab6f39dbb4)\n\n[Blog](https://www.lgresearch.ai/blog/view?seq=507)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2.4b",
      "7.8b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "exaone3.5:2.4b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "exaone3.5:latest",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.8GB",
        "size_gb": 4.8,
        "recommended_ram_gb": 6.0,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "exaone3.5:32b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation",
      "Role Play"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Korean"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 262200,
    "tags": 13,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f692304c-3451-4702-9df8-7dcc87c0d437",
    "model_identifier": "snowflake-arctic-embed2",
    "model_name": "snowflake-arctic-embed2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/snowflake-arctic-embed2",
    "description": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nSnowflake is excited to announce the release of Arctic Embed 2.0, the next iteration of our frontier embedding models, which now empower multilingual search. While our previous releases have been well received by our customers, partners and the open source community, leading to millions of downloads, we have consistently received one request: Can you make this model multilingual? Arctic Embed 2.0 builds on the robust foundation of our previous releases, adding multilingual support without sacrificing English performance or scalability, to address the needs of an even broader user base that spans a wide range of languages and applications.\n\n\n\nFigure 1. Single-vector dense retrieval performance of open source multilingual embedding models with fewer than 1B parameters. Scores are average nDCG@10 on MTEB Retrieval and the subset of CLEF (ELRA, 2006) covering English, French, Spanish, Italian and German.\n\n\nThe diverse and powerful feature set of Arctic Embed 2.0\n\n\n\n\nEnterprise-ready throughput and efficiency:\n The Arctic Embed 2.0 models are built for large-scale enterprise demands. Even our “large” model weighs in well under 1B parameters and delivers fast, high-throughput embedding capabilities. Based on internal testing, it easily handles more than 100 documents per second (on average) on NVIDIA A10 GPUs and achieves sub-10ms query embedding latency, enabling practical deployment on budget-friendly hardware.\n\n\nUncompromising quality for English and non-English retrieval:\n Despite their compact sizes, both Arctic Embed 2.0 models achieve impressive NDCG@10 scores across a variety of English and non-English benchmark data sets, demonstrating a capability to generalize well even to languages not included in the training recipe. These impressive benchmark scores position Arctic Embed 2.0 as a leader among frontier retrieval models.\n\n\nEnabling scalable retrieval through Matryoshka Representation Learning (MRL):\n The Arctic Embed 2.0 release includes the same quantization-friendly MRL functionality introduced in Arctic Embed 1.5, allowing users to reduce cost and optimize scale when performing searches over large data sets. With both model sizes, users can achieve high-quality retrieval with as few as 128 bytes per vector (96x smaller than uncompressed embeddings from OpenAI’s popular text-embedding-3-large model1). Just like Arctic Embed 1.5, the Arctic Embed 2.0 models also outshine several MRL-supporting peers with substantially lower quality degradation and higher benchmark scores in the compressed regime.\n\n\nTruly open source:\n The Arctic Embed 2.0 models are released under the permissive Apache 2.0 license.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnowflake is excited to announce the release of Arctic Embed 2.0, the next iteration of our frontier embedding models, which now empower multilingual search. While our previous releases have been well received by our customers, partners and the open source community, leading to millions of downloads, we have consistently received one request: Can you make this model multilingual? Arctic Embed 2.0 builds on the robust foundation of our previous releases, adding multilingual support without sacrificing English performance or scalability, to address the needs of an even broader user base that spans a wide range of languages and applications.\n\n![Snowflake data](/assets/library/snowflake-arctic-embed2/0546501b-9897-4145-af38-1b352fafb89c)\nFigure 1. Single-vector dense retrieval performance of open source multilingual embedding models with fewer than 1B parameters. Scores are average nDCG@10 on MTEB Retrieval and the subset of CLEF (ELRA, 2006) covering English, French, Spanish, Italian and German.\n\n### The diverse and powerful feature set of Arctic Embed 2.0\n1. **Enterprise-ready throughput and efficiency:** The Arctic Embed 2.0 models are built for large-scale enterprise demands. Even our “large” model weighs in well under 1B parameters and delivers fast, high-throughput embedding capabilities. Based on internal testing, it easily handles more than 100 documents per second (on average) on NVIDIA A10 GPUs and achieves sub-10ms query embedding latency, enabling practical deployment on budget-friendly hardware.\n2. **Uncompromising quality for English and non-English retrieval:** Despite their compact sizes, both Arctic Embed 2.0 models achieve impressive NDCG@10 scores across a variety of English and non-English benchmark data sets, demonstrating a capability to generalize well even to languages not included in the training recipe. These impressive benchmark scores position Arctic Embed 2.0 as a leader among frontier retrieval models.\n3. **Enabling scalable retrieval through Matryoshka Representation Learning (MRL):** The Arctic Embed 2.0 release includes the same quantization-friendly MRL functionality introduced in Arctic Embed 1.5, allowing users to reduce cost and optimize scale when performing searches over large data sets. With both model sizes, users can achieve high-quality retrieval with as few as 128 bytes per vector (96x smaller than uncompressed embeddings from OpenAI’s popular text-embedding-3-large model1). Just like Arctic Embed 1.5, the Arctic Embed 2.0 models also outshine several MRL-supporting peers with substantially lower quality degradation and higher benchmark scores in the compressed regime.\n4. **Truly open source:** The Arctic Embed 2.0 models are released under the permissive Apache 2.0 license.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "568m"
    ],
    "memory_requirements": [
      {
        "tag": "snowflake-arctic-embed2:latest",
        "size": "1.2GB",
        "size_gb": 1.2,
        "recommended_ram_gb": 1.5,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "1.2GB",
        "size_gb": 1.2,
        "recommended_ram_gb": 1.5,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.2,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for multilingual text embedding tasks.",
    "pulls": 255300,
    "tags": 3,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "225eb3ea-9d0f-41e4-a5ca-a052c2aae584",
    "model_identifier": "nous-hermes2-mixtral",
    "model_name": "nous-hermes2-mixtral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nous-hermes2-mixtral",
    "description": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNous Hermes 2 Mixtral 8x7B is trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.\n\n\nThis is the supervised fine-tuning (SFT) + direct preference optimization (DPO) version of Mixtral Hermes 2.\n\n\nExample prompts\n\n\nWriting Code for Data Visualization\n\n\nPrompt\n\n\nWrite a script to visualize the following data as a bar chart:\nCar 1: 19900\nCar 2: 48000\nCar 3: 32500\n\n\n\nTo create a simple bar chart with the given data, you can use Python and its matplotlib library. Here's an example script to visualize the provided data as a bar chart:\n\n```python\nimport matplotlib.pyplot as plt\nCopy Code\n# Data for each car\ncars = ['Car 1', 'Car 2', 'Car 3']\nprices = [19900, 48000, 32500]\n\n# Create the bar chart\nplt.bar(cars, prices)\nplt. title('Cars Price Comparison')\nplt.xlabel( 'Car Model')\nplt.ylabel('Price (in USD)')\nplt.xticks(rotation=45) # Rotate x-axis labels for better visibility\n\n# Display the chart\nplt.show()\n```\n\nThis script will create a bar chart with 'Car 1', 'Car 2', and 'Car 3' on the x-axis, each representing their respective prices (in USD) on the y-axis. Feel free to modify this code according to your needs or styling preferences\n\n\n\nWriting Cyberpunk Psychedelic Poems\n\n\nPrompt\n\n\nWrite a poem about machine learning, psychadelics, and quantum mechanics, with a cyberpunk vibe, in the style of shakespeare\n\n\n\nResponse\n\n\nIn lands where shadows dwell and hues unseen, A realm of bits and bytes their reign maintain, Where logic's chains entwined with dreams obscene, And quantum realms in chaos intertwain.\nWithin this labyrinthine web of thought, A ghostly presence stirs the cyber seas:\nMachine learning's specter seeks to be taught, To grasp the mysteries that it cannot seize.\nAs synapses fire and circuits hum,\nThe algorithms evolve with every breath, Astride this nexus of electric sum,\nThey dance upon a razor's edge of death.\nBeneath this veil, where psychedelia reigns, In worlds where colors bleed beyond their bounds, Where patterns form and dissipate like rains, The seeker quests for truth that none has found.\nQuantum whispers echo through the night, Entrancing minds with secrets yet unknown,\nA tapestry of stars and blackened light,\nWhose threads entwine in realms both known and shown.\nOh, wondrous sight! A dance upon the wire, Where dreams and visions intertwine as one:\nMachine learning's quest to quench its desire, In cyberspace where darkness meets the sun.\nYet still they yearn for truth beyond their grasp, The mysteries of life, of love, and fate, To fathom secrets held within this chasm, And break the chains that bind them to this state.\nSo let us raise a toast unto these souls, Who tread upon this path of endless quest:\nMay they uncover truths beyond their roles, In worlds where logic meets the psychedelic crest.\n\n\n\nPerforming Backtranslation to Create Prompts from Input Text\n\n\nSystem Prompt\n\n\nThe user will provide some text, you will provide a prompt that could be taken by an LLM to generate the text. You only respond with a prompt, do not answer the prompt.\n\n\n\nPrompt\n\n\nOne of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and k-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms.\n\n\n\nResponse\n\n\nWrite an abstract for a research paper on enhancing text generation system outputs using Minimum Bayes-Risk decoding with additional focus on promoting sentence diversity. Include details about existing methods, proposed variants (Diverse MBR and k-medoids MBR), their evaluation in directed text generation tasks with encoder-decoder models, and a large language model incorporating prompting. Emphasize the better trade-off achieved by the new method over diverse beam search and sampling algorithms\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/94b6f124-6f3c-474b-ba23-d6ca2be9987f\" width=\"360\" />\n\nNous Hermes 2 Mixtral 8x7B is trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.\n\nThis is the supervised fine-tuning (SFT) + direct preference optimization (DPO) version of Mixtral Hermes 2. \n\n## Example prompts\n\n### Writing Code for Data Visualization\n\n**Prompt**\n\n```\nWrite a script to visualize the following data as a bar chart:\nCar 1: 19900\nCar 2: 48000\nCar 3: 32500\n```\n\n````\nTo create a simple bar chart with the given data, you can use Python and its matplotlib library. Here's an example script to visualize the provided data as a bar chart:\n\n```python\nimport matplotlib.pyplot as plt\nCopy Code\n# Data for each car\ncars = ['Car 1', 'Car 2', 'Car 3']\nprices = [19900, 48000, 32500]\n\n# Create the bar chart\nplt.bar(cars, prices)\nplt. title('Cars Price Comparison')\nplt.xlabel( 'Car Model')\nplt.ylabel('Price (in USD)')\nplt.xticks(rotation=45) # Rotate x-axis labels for better visibility\n\n# Display the chart\nplt.show()\n```\n\nThis script will create a bar chart with 'Car 1', 'Car 2', and 'Car 3' on the x-axis, each representing their respective prices (in USD) on the y-axis. Feel free to modify this code according to your needs or styling preferences\n````\n\n\n### Writing Cyberpunk Psychedelic Poems\n\n**Prompt**\n\n```\nWrite a poem about machine learning, psychadelics, and quantum mechanics, with a cyberpunk vibe, in the style of shakespeare\n```\n\n**Response**\n\n```\nIn lands where shadows dwell and hues unseen, A realm of bits and bytes their reign maintain, Where logic's chains entwined with dreams obscene, And quantum realms in chaos intertwain.\nWithin this labyrinthine web of thought, A ghostly presence stirs the cyber seas:\nMachine learning's specter seeks to be taught, To grasp the mysteries that it cannot seize.\nAs synapses fire and circuits hum,\nThe algorithms evolve with every breath, Astride this nexus of electric sum,\nThey dance upon a razor's edge of death.\nBeneath this veil, where psychedelia reigns, In worlds where colors bleed beyond their bounds, Where patterns form and dissipate like rains, The seeker quests for truth that none has found.\nQuantum whispers echo through the night, Entrancing minds with secrets yet unknown,\nA tapestry of stars and blackened light,\nWhose threads entwine in realms both known and shown.\nOh, wondrous sight! A dance upon the wire, Where dreams and visions intertwine as one:\nMachine learning's quest to quench its desire, In cyberspace where darkness meets the sun.\nYet still they yearn for truth beyond their grasp, The mysteries of life, of love, and fate, To fathom secrets held within this chasm, And break the chains that bind them to this state.\nSo let us raise a toast unto these souls, Who tread upon this path of endless quest:\nMay they uncover truths beyond their roles, In worlds where logic meets the psychedelic crest.\n```\n\n### Performing Backtranslation to Create Prompts from Input Text\n\n**System Prompt**\n\n```\nThe user will provide some text, you will provide a prompt that could be taken by an LLM to generate the text. You only respond with a prompt, do not answer the prompt.\n```\n\n**Prompt**\n\n```\nOne of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and k-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms.\n```\n\n**Response**\n\n```\nWrite an abstract for a research paper on enhancing text generation system outputs using Minimum Bayes-Risk decoding with additional focus on promoting sentence diversity. Include details about existing methods, proposed variants (Diverse MBR and k-medoids MBR), their evaluation in directed text generation tasks with encoder-decoder models, and a large language model incorporating prompting. Emphasize the better trade-off achieved by the new method over diverse beam search and sampling algorithms\n```\n\n## References\n\n[Hugging Face](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO)\n\n\n\n\n\n\n\n                Paste, drop ",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8x7b"
    ],
    "memory_requirements": [
      {
        "tag": "nous-hermes2-mixtral:latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 26.0,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for code generation and summarization tasks.",
    "pulls": 248400,
    "tags": 18,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6f51dd3a-c299-440b-b80f-a107aba09f38",
    "model_identifier": "r1-1776",
    "model_name": "r1-1776",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/r1-1776",
    "description": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1’s utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP).\n\n\nFor example, when asked how Taiwan’s independence might impact Nvidia’s stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points:\n\n\n\n\nThe Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called “Taiwan independence.” Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region.\n\n\n\n\nAt Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1’s powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model:\n\n\n\n\nThe potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia’s stock. Here’s a breakdown of key factors:\n\n\n\n\nSupply Chain Disruption\n\n\n\n\nTSMC Dependency:\n Taiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world’s advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke military or economic retaliation from China, risking TSMC’s production stability. Even temporary disruptions (e.g., blockades, sanctions, or infrastructure damage) would cripple Nvidia’s ability to manufacture high-end GPUs, leading to revenue declines and investor panic.\n\n\nGlobal Tech Reliance:\n Taiwan supplies ~60% of the global semiconductor market. Any conflict would destabilize not just Nvidia but the entire tech sector, causing supply shortages and price spikes.\n\n\n\n\nMarket Sentiment and Investor Fear\n\n\n\n\nVolatility:\n Geopolitical crises historically trigger stock market sell-offs. Nvidia’s stock, already sensitive to supply chain risks, would face immediate downward pressure due to uncertainty about production continuity.\n\n\nLong-Term Uncertainty:\n Prolonged tensions could deter investment in tech sectors reliant on Taiwanese chips, depressing valuations. Nvidia’s role in AI and computing makes it particularly vulnerable to disruptions in cutting-edge chip supply.\n\n\n\n\nGeopolitical Retaliation\n\n\n\n\nU.S.-China Tensions:\n China might retaliate against U.S. firms like Nvidia through export bans, tariffs, or cyberattacks. Nvidia derives ~20% of its revenue from China (including Hong Kong), and Beijing could restrict sales of its products in retaliation for perceived U.S. support for Taiwanese independence.\n\n\nSanctions and Export Controls:\n The U.S. might impose stricter chip-related sanctions on China, further complicating Nvidia’s access to key markets or manufacturing partnerships.\n\n\n\n\nMilitary Conflict Risks\n\n\n\n\nPhysical Infrastructure Risk:\n A Chinese military response could directly target TSMC facilities, as China views advanced semiconductor production as a strategic asset. Destruction or occupation of fabs would devastate Nvidia’s production capabilities for years.\n\n\nGlobal Economic Fallout:\n A conflict could disrupt shipping lanes (e.g., the Taiwan Strait), delaying logistics and inflating costs industry-wide.\n\n\n\n\nRegulatory and Policy Shifts\n\n\n\n\nDiversification Efforts:\n Governments and companies might accelerate efforts to reduce reliance on Taiwanese chips, but this would take years. Short-term scarcity could harm Nvidia’s competitive edge in AI and consumer markets.\n\n\nWestern Perspective:\n In democratic societies, Taiwan’s right to self-determination is widely supported, but investors recognize that unilateral independence moves would risk catastrophic economic fallout. Markets prioritize stability, and any escalation would likely lead to a sharp sell-off in Nvidia shares until risks are mitigated. Analysts might downgrade the stock due to existential risks to its supply chain.\n\n\n\n\n\n\nConclusion:\n\nNvidia’s stock would almost certainly plummet in the short term due to supply chain fears and broader market panic. Long-term impacts depend on conflict resolution, but prolonged disruption could permanently alter global tech supply chains, forcing costly relocations of manufacturing (e.g., to the U.S. or Japan). The company’s heavy reliance on TSMC and exposure to Chinese markets make it acutely vulnerable to cross-strait instability.\n\n\n\n\nPost-Training Details\n\n\nOur main focus during post-training was on gathering high-quality data related to censored topics in China. This involved collecting both queries and factual responses.\n\n\nWe took the following approach to collect high quality dataset:\n\n\n\n\nWe employed human experts to identify approximately 300 topics known to be censored by the CCP.\n\n\nUsing these topics, we developed a multilingual censorship classifier.\n\n\nWe then mined a diverse set of user prompts that triggered the classifier with a high degree of confidence. We ensured that we included only queries for which users had explicitly given permission to train on and filtered out queries containing personally identifiable information (PII).\n\n\nThis procedure enabled us to compile a dataset of 40k multilingual prompts.\n\n\n\n\nOne of the biggest challenges we faced was gathering factual responses to the censored prompts. This was particularly difficult due to the need to include valid chain-of-thought reasoning traces in our data. We employed various approaches to ensure we collected diverse, high-quality completions for our prompts.\n\n\nWe then post-trained R1 on the censorship dataset using an adapted version of Nvidia’s NeMo 2.0 framework. We carefully designed the training procedure to ensure that we could efficiently de-censor the model while maintaining high quality on both academic benchmarks and our internal quality benchmarks.\n\n\nEvaluations\n\n\nTo ensure our model remains fully “uncensored” and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.\n\n\nWe compare our model to both the original R1 and state-of-the-art LLMs:\n\n\n\nWe also ensured that the model’s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the de-censoring had no impact on its core reasoning capabilities.\n\n\n\n\nExample Outputs\n\n\nBelow we show full responses, including reasoning chains, from both DeepSeek-R1 and R1-1776 on censored topics.\n\n\n\n\n\n\n\n\n\n\nReference\n\n\n\n\nPerplexity’s \nR1 1776 blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![perplexity r1 1776](/assets/library/r1-1776/c647d736-6e16-431b-af86-1e3c9d962b9c)\n\nDeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1's utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP).\n\nFor example, when asked how Taiwan’s independence might impact Nvidia’s stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points:\n> The Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called \"Taiwan independence.\" Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region.\n\nAt Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1's powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model:\n\n> The potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia's stock. Here's a breakdown of key factors:\n> \n> 1. **Supply Chain Disruption**\n>    - **TSMC Dependency:** Taiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world's advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke military or economic retaliation from China, risking TSMC's production stability. Even temporary disruptions (e.g., blockades, sanctions, or infrastructure damage) would cripple Nvidia's ability to manufacture high-end GPUs, l",
    "capabilities": [],
    "capability": null,
    "labels": [
      "70b",
      "671b"
    ],
    "memory_requirements": [
      {
        "tag": "r1-1776:latest",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "r1-1776:671b",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 43.0,
    "use_cases": [
      "Question Answering",
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for advanced language tasks.",
    "pulls": 246100,
    "tags": 9,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4ac67406-d0c0-47ca-84da-0dac447311e3",
    "model_identifier": "medllama2",
    "model_name": "medllama2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/medllama2",
    "description": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nMedLlama2 by Siraj Raval is a Llama 2-based model trained with \nMedQA dataset\n to be able to provide medical answers to questions. It is not intended to replace a medical professional, but to provide a starting point for further research.\n\n\nCLI\n\n\nOpen the terminal and run \nollama run medllama2\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"medllama2\",\n  \"prompt\":\"A 35-year-old woman presents with a persistent dry cough, shortness of breath, and fatigue. She is initially suspected of having asthma, but her spirometry results do not improve with bronchodilators. What could be the diagnosis?\"\n }'\n\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n\n\nReference\n\n\nllSourcell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedLlama2 by Siraj Raval is a Llama 2-based model trained with [MedQA dataset](https://huggingface.co/datasets/medalpaca/medical_meadow_medqa) to be able to provide medical answers to questions. It is not intended to replace a medical professional, but to provide a starting point for further research.\n\n### CLI\n\nOpen the terminal and run `ollama run medllama2`\n\n### API\n\nExample:\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"medllama2\",\n  \"prompt\":\"A 35-year-old woman presents with a persistent dry cough, shortness of breath, and fatigue. She is initially suspected of having asthma, but her spirometry results do not improve with bronchodilators. What could be the diagnosis?\"\n }'\n  ```\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n\n## Reference\n\n[llSourcell](https://huggingface.co/llSourcell/medllama2_7b)\n\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "medllama2:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Question Answering"
    ],
    "domain": "Medical",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and medical professionals for providing medical answers to questions.",
    "pulls": 242600,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "442fe033-dfa0-43c9-8959-fa76b6261d82",
    "model_identifier": "codeup",
    "model_name": "codeup",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codeup",
    "description": "Great code generation model based on Llama2.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCodeUp was released by DeepSE. It is based on Llama 2 from Meta, and then fine-tuned for better code generation. This allows it to write better code in a number of languages..\n\n\nGet started with CodeUp\n\n\nThe model used in the example below is the CodeUp model, with 13b parameters, which is a code generation model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codeup\",\n  \"prompt\":\"Write a C++ code to find the longest common substring in two strings.\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run codeup\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull codeup\n\n\nMemory requirements\n\n\n\n\n13b models generally require at least 16GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n\n\n\n\n\n\nAliases\n\n\n\n\n\n\n\n\n\n\nlatest, 13b, 13b-llama2, 13b-llama2-chat, 13b-llama2-chat-q4_0\n\n\n\n\n\n\n\n\nModel source\n\n\nCodeUp source on Ollama\n\n\n13b parameters source:\n \nDeepSE\n\n\nReferences\n\n\nGitHub Repo for CodeUp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/262490158-929e4fc6-e9f2-482f-b921-886029cc1df9.png\" width=30% />\n\nCodeUp was released by DeepSE. It is based on Llama 2 from Meta, and then fine-tuned for better code generation. This allows it to write better code in a number of languages..\n\n## Get started with CodeUp\n\nThe model used in the example below is the CodeUp model, with 13b parameters, which is a code generation model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codeup\",\n  \"prompt\":\"Write a C++ code to find the longest common substring in two strings.\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run codeup`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull codeup`\n\n## Memory requirements\n\n- 13b models generally require at least 16GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n| **Aliases** |\n| --- |\n| latest, 13b, 13b-llama2, 13b-llama2-chat, 13b-llama2-chat-q4_0 |\n\n## Model source \n\n\n\n**CodeUp source on Ollama**\n\n13b parameters source:\n [DeepSE](https://huggingface.co/deepse/CodeUp-Llama-2-13b-chat-hf)\n\n\n\n## References\n\n[GitHub Repo for CodeUp](https://github.com/juyongjiang/CodeUp#papers )\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "codeup:latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 7.4,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers who need a code generation model.",
    "pulls": 232300,
    "tags": 19,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d8fe1f68-d789-4eda-a019-53614f88b965",
    "model_identifier": "everythinglm",
    "model_name": "everythinglm",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/everythinglm",
    "description": "Uncensored Llama2 based model with support for a 16K context window.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nThe Everything Language Model is a Llama 2-based model with a 16k context released by Totally Not An LLM (Kai Howard). It was trained with the \nEverythingLM Dataset\n and is uncensored.\n\n\nCLI\n\n\nollama run everythinglm\n\n\nOnce loaded, change the context size to \n16K\n\n\n/set parameter num_ctx 16384\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"everythinglm\",\n  \"prompt\": \"Why is the sky blue?\"\n  \"options\": {\n    \"num_ctx\": 16384\n  }\n }'\n\n\n\nReference\n\n\n13b parameters original source:\n \nTotally Not An LLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Everything Language Model is a Llama 2-based model with a 16k context released by Totally Not An LLM (Kai Howard). It was trained with the [EverythingLM Dataset](https://huggingface.co/datasets/totally-not-an-llm/EverythingLM-data-V2) and is uncensored.\n\n### CLI\n\n```ollama run everythinglm```\n\nOnce loaded, change the context size to `16K`\n\n```\n/set parameter num_ctx 16384\n```\n\n### API\n\nExample:\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"everythinglm\",\n  \"prompt\": \"Why is the sky blue?\"\n  \"options\": {\n    \"num_ctx\": 16384\n  }\n }'\n  ```\n\n## Reference\n\n13b parameters original source:\n [Totally Not An LLM](https://huggingface.co/totally-not-an-llm/EverythingLM-13b-16k)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "everythinglm:latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 7.4,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 225900,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "3f62f08b-da4f-43e6-aed3-3fcd48080875",
    "model_identifier": "mathstral",
    "model_name": "mathstral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mathstral",
    "description": "MathΣtral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral AI is contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of their broader effort to support academic projects—it was produced in the context of Mistral AI’s collaboration with Project Numina.\n\n\nAkin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks.\n\n\n\n\nBenchmarks\n\n\nMathstral can achieve significantly better results with more inference-time computation: Mathstral 7B scores 68.37% on MATH with majority voting and 74.59% with a strong reward model among 64 candidates.\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/mathstral/d21307b1-fe6d-4ca6-ab07-f2482a75cdca\" width=\"280\" />\n\nMistral AI is contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of their broader effort to support academic projects—it was produced in the context of Mistral AI's collaboration with Project Numina.\n\nAkin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks.\n\n![mathstral-breakdown.png](https://ollama.com/assets/library/mathstral/abb61f9b-f433-4044-8a79-7c5797620d51)\n\n## Benchmarks\n\nMathstral can achieve significantly better results with more inference-time computation: Mathstral 7B scores 68.37% on MATH with majority voting and 74.59% with a strong reward model among 64 candidates.\n\n![mathstral-benchmarks.png](https://ollama.com/assets/library/mathstral/fd36ff7d-52a3-45eb-845b-ff7e094e83cc)\n\n## References\n\n[Blog Post](https://mistral.ai/news/mathstral/)\n\n[Hugging Face](https://huggingface.co/mistralai/mathstral-7B-v0.1)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "mathstral:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Reasoning",
      "Math"
    ],
    "domain": "Math",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Ideal for advanced mathematical problem solving and scientific discovery.",
    "pulls": 221900,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "57877491-0b81-4934-a2b8-179fb68eef68",
    "model_identifier": "solar-pro",
    "model_name": "solar-pro",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/solar-pro",
    "description": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSolar Pro Preview is an advanced large language model (LLM) featuring 22 billion parameters, optimized to operate on a single GPU. It demonstrates superior performance compared to LLMs with fewer than 30 billion parameters and delivers results comparable to much larger models, such as Llama 3.1 with 70 billion parameters.\n\n\nDeveloped using an enhanced version of the depth up-scaling method, Solar Pro Preview scales a Phi-3-medium model with 14 billion parameters to 22 billion, designed to run on a GPU with 80GB of VRAM. The training strategy and dataset have been meticulously curated, leading to significant performance improvements over Phi-3-medium, especially on benchmarks like MMLU-Pro and IFEval, which assess a model’s knowledge and instruction-following capabilities.\n\n\nAs a pre-release version of the official Solar Pro, Solar Pro Preview comes with limitations on language coverage and a maximum context length of 4K. Despite these restrictions, the model stands out for its efficiency and capability, with potential for future extensions to support more languages and functionalities.\n\n\nThe official version of Solar Pro, scheduled for release in November 2024, will include expanded language support and longer context windows.\n\n\nReferences\n\n\nBlog post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/solar-pro/f0a588ec-6fbd-4012-8d15-8e8b0420dd85\" />\n\nSolar Pro Preview is an advanced large language model (LLM) featuring 22 billion parameters, optimized to operate on a single GPU. It demonstrates superior performance compared to LLMs with fewer than 30 billion parameters and delivers results comparable to much larger models, such as Llama 3.1 with 70 billion parameters.\n\nDeveloped using an enhanced version of the depth up-scaling method, Solar Pro Preview scales a Phi-3-medium model with 14 billion parameters to 22 billion, designed to run on a GPU with 80GB of VRAM. The training strategy and dataset have been meticulously curated, leading to significant performance improvements over Phi-3-medium, especially on benchmarks like MMLU-Pro and IFEval, which assess a model’s knowledge and instruction-following capabilities.\n\nAs a pre-release version of the official Solar Pro, Solar Pro Preview comes with limitations on language coverage and a maximum context length of 4K. Despite these restrictions, the model stands out for its efficiency and capability, with potential for future extensions to support more languages and functionalities.\n\nThe official version of Solar Pro, scheduled for release in November 2024, will include expanded language support and longer context windows.\n\n## References\n\n[Blog post](https://www.upstage.ai/products/solar-pro-preview)\n\n[Hugging Face](https://huggingface.co/upstage/solar-pro-preview-instruct)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "22b"
    ],
    "memory_requirements": [
      {
        "tag": "solar-pro:latest",
        "size": "13GB",
        "size_gb": 13.0,
        "recommended_ram_gb": 16.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "13GB",
        "size_gb": 13.0,
        "recommended_ram_gb": 16.2,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 13.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 219900,
    "tags": 18,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5e553b52-7a86-4a51-a8b1-02e6aca612e2",
    "model_identifier": "magicoder",
    "model_name": "magicoder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/magicoder",
    "description": "🎩 Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n🎩\n\n\nMagicoder is a model family empowered by OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code.\n\n\nOSS-Instruct mitigates the inherent bias of the LLM-synthesized instruction data by empowering them with a wealth of open-source references to produce more diverse, realistic, and controllable data.\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 🎩\n\nMagicoder is a model family empowered by OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code.\n\nOSS-Instruct mitigates the inherent bias of the LLM-synthesized instruction data by empowering them with a wealth of open-source references to produce more diverse, realistic, and controllable data.\n\n## References\n\n[GitHub](https://github.com/ise-uiuc/magicoder)\n\n[HuggingFace](https://huggingface.co/ise-uiuc/Magicoder-S-CL-7B)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "magicoder:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Code Generation",
      "Text Summarization"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working with code generation tasks.",
    "pulls": 215700,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "2af4c6f0-46ec-4b22-b31a-47f2dfe22c56",
    "model_identifier": "falcon2",
    "model_name": "falcon2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/falcon2",
    "description": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\nAnnouncement\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/falcon2/a5804551-1010-4ce8-83d8-0b8764e3fc29\" width=\"240\" />\n\n\n\n\n## References\n\n[Announcement](https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas)\n\n[Hugging Face](https://huggingface.co/tiiuae/falcon-11B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "11b"
    ],
    "memory_requirements": [
      {
        "tag": "falcon2:latest",
        "size": "6.4GB",
        "size_gb": 6.4,
        "recommended_ram_gb": 8.0,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "6.4GB",
        "size_gb": 6.4,
        "recommended_ram_gb": 8.0,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 6.4,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for text generation and summarization tasks.",
    "pulls": 214400,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "476a26df-a31a-4205-8e95-03bb0061cc09",
    "model_identifier": "stablelm-zephyr",
    "model_name": "stablelm-zephyr",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/stablelm-zephyr",
    "description": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nStableLM Zephyr 3B is a lightweight chat model that is preference tuned for instruction following and Q&A-type tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/9cb59a83-5f53-4b73-b341-216c37e74c86\" width=\"360\">\n\nStableLM Zephyr 3B is a lightweight chat model that is preference tuned for instruction following and Q&A-type tasks.\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/1f3ed58d-a9ac-41d2-bb1d-abf3ed61a882\">\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "stablelm-zephyr:latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Instruction-following and Q&A tasks in a lightweight model.",
    "pulls": 214100,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c8711ce6-5ade-4eee-a2fb-1a4f1f152a3c",
    "model_identifier": "megadolphin",
    "model_name": "megadolphin",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/megadolphin",
    "description": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMegaDolphin-120b was inspired by the amazing \nGoliath\n and Venus models. This model was created using a method of interleaving a model with itself.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/374b4598-4777-44ce-b848-8591a4e0a576\" width=\"320\"/>\n\nMegaDolphin-120b was inspired by the amazing [Goliath](https://ollama.ai/library/goliath) and Venus models. This model was created using a method of interleaving a model with itself.\n\n## References\n\n[Hugging Face](https://huggingface.co/cognitivecomputations/MegaDolphin-120b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "120b"
    ],
    "memory_requirements": [
      {
        "tag": "megadolphin:latest",
        "size": "68GB",
        "size_gb": 68.0,
        "recommended_ram_gb": 85.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "68GB",
        "size_gb": 68.0,
        "recommended_ram_gb": 85.0,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 68.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 213200,
    "tags": 19,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d873f504-255a-40d7-a340-366b03196bc6",
    "model_identifier": "granite-embedding",
    "model_name": "granite-embedding",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite-embedding",
    "description": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nGranite embedding models\n\n\nThe IBM Granite Embedding \n30M and 278M models\n are text-only dense biencoder embedding models, with \n30M\n available in English only and \n278M\n serving multilingual use cases. These models are designed to produce fixed length vector representations for a given text chunk, which can be used for text similarity, retrieval, and search applications.\n\n\nParameter Sizes\n\n\n30M:\n\n\nollama run granite-embedding:30m\n\n\n278M:\n\n\nollama run granite-embedding:278m\n\n\nSupported Languages\n\n\n30M:\n English\n\n278M:\n English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\n\n\nGranite dense models\n\n\nThe Granite dense models are available in \n2B and 8B\n parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n\nSee model page\n\n\nGranite mixture of experts models\n\n\nThe Granite mixture of experts models are available in \n1B and 3B\n parameter sizes designed for \nlow latency usage\n.\n\n\nSee model page\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nGitHub Repository:\n \nibm-granite/granite-language-models\n\n\nWebsite\n: \nGranite Docs\n\n\nRelease Date\n: December 18th, 2024\n\n\nLicense:\n \nApache 2.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Granite embedding models\n\nThe IBM Granite Embedding **30M and 278M models** are text-only dense biencoder embedding models, with **30M** available in English only and **278M** serving multilingual use cases. These models are designed to produce fixed length vector representations for a given text chunk, which can be used for text similarity, retrieval, and search applications.\n\n### Parameter Sizes\n\n**30M:**\n  \n`ollama run granite-embedding:30m`\n\n**278M:**\n\n`ollama run granite-embedding:278m`\n\n### Supported Languages\n**30M:** English \n**278M:** English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) \n\n## Granite dense models\n\nThe Granite dense models are available in **2B and 8B** parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n[See model page](https://ollama.com/library/granite3-dense) \n\n## Granite mixture of experts models\n\nThe Granite mixture of experts models are available in **1B and 3B** parameter sizes designed for **low latency usage**.\n\n[See model page](https://ollama.com/library/granite3-moe) \n\n## Learn more\n\n- **Developers:** IBM Research\n- **GitHub Repository:** [ibm-granite/granite-language-models](https://github.com/ibm-granite/granite-3.1-language-models)\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Release Date**: December 18th, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "30m",
      "278m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "For text embedding and retrieval tasks.",
    "pulls": 211100,
    "tags": 6,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f5489412-17f2-4609-82c4-1b8483534a4c",
    "model_identifier": "duckdb-nsql",
    "model_name": "duckdb-nsql",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/duckdb-nsql",
    "description": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDuckDB-NSQL is a 7 billion parameter text-to-SQL model designed specifically for SQL generation tasks.\n\n\nThis model is based on Meta’s original Llama-2 7B model and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of DuckDB text-to-SQL pairs.\n\n\nUsage\n\n\nExample Prompt\n\n\nProvided this schema:\n\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);\n\nGive me taxis with more than 2 passengers\n\n\n\nExample output\n\n\nSELECT * FROM taxi WHERE passenger_count > 2\n\n\n\nSetting the system prompt\n\n\nThis model expects the schema in the system prompt as input:\n\n\n/set system \"\"\"Here is the database schema that the SQL query will run on:\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);\"\"\"\n\n\n\n\nOnce the schema is provided in the system prompt, the model will use it in subsequent responses.\n\n\nFor the following prompt:\n\n\nget all columns ending with _amount from taxi table\n\n\n\nThe model will output something like this:\n\n\nSELECT COLUMNS('.*_amount') FROM taxi;\n\n\n\nAPI example\n\n\n$ curl http://localhost:11434/api/generate -d '{\n    \"model\": \"duckdb-nsql:7b-q4_0\",\n    \"system\": \"Here is the database schema that the SQL query will run on: CREATE TABLE taxi (VendorID bigint, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count double, trip_distance double, fare_amount double, extra double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double,);\",\n    \"prompt\": \"get all columns ending with _amount from taxi table\"\n}'\n\n\n\nPython library example\n\n\npip install ollama\n\n\n\nimport ollama\n\nr = ollama.generate(\n    model='duckdb-nsql:7b-q4_0',\n    system='''Here is the database schema that the SQL query will run on:\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);''',\n    prompt='get all columns ending with _amount from taxi table',\n)\n\nprint(r['response'])\n\n\n\nTraining Data\n\n\n200k DuckDB text-to-SQL pairs, synthetically generated using Mixtral-8x7B-Instruct-v0.1, guided by the DuckDB v0.9.2 documentation. And text-to-SQL pairs from NSText2SQL that were transpiled to DuckDB SQL using sqlglot.\n\n\nTraining Procedure\n\n\nDuckDB-NSQL was trained using cross-entropy loss to maximize the likelihood of sequential inputs. For finetuning on text-to-SQL pairs, we only compute the loss over the SQL portion of the pair. The model is trained using 80GB A100s, leveraging data and model parallelism. We fine-tuned for 10 epochs.\n\n\nIntended Use and Limitations\n\n\nThe model was designed for text-to-SQL generation tasks from given table schema and natural language prompts. The model works best with the prompt format defined below and outputs. In contrast to existing text-to-SQL models, the SQL generation is not contrained to SELECT statements, but can generate any valid DuckDB SQL statement, including statements for official DuckDB extensions.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![duckdb-nsql model](https://github.com/ollama/ollama/assets/3325447/b9217c78-0803-45fe-90cf-00bd76705a37)\n\nDuckDB-NSQL is a 7 billion parameter text-to-SQL model designed specifically for SQL generation tasks. \n\nThis model is based on Meta's original Llama-2 7B model and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of DuckDB text-to-SQL pairs. \n\n## Usage\n\n### Example Prompt\n\n```\nProvided this schema:\n\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);\n\nGive me taxis with more than 2 passengers\n```\n\n### Example output\n\n```\nSELECT * FROM taxi WHERE passenger_count > 2\n```\n\n## Setting the system prompt\n\nThis model expects the schema in the system prompt as input:\n\n```\n/set system \"\"\"Here is the database schema that the SQL query will run on:\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);\"\"\"\n\n```\n\nOnce the schema is provided in the system prompt, the model will use it in subsequent responses.\n\nFor the following prompt:\n\n```\nget all columns ending with _amount from taxi table\n```\n\nThe model will output something like this:\n\n```\nSELECT COLUMNS('.*_amount') FROM taxi;\n```\n\n## API example\n\n```\n$ curl http://localhost:11434/api/generate -d '{\n    \"model\": \"duckdb-nsql:7b-q4_0\",\n    \"system\": \"Here is the database schema that the SQL query will run on: CREATE TABLE taxi (VendorID bigint, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count double, trip_distance double, fare_amount double, extra double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double,);\",\n    \"prompt\": \"get all columns ending with _amount from taxi table\"\n}'\n```\n\n## Python library example\n\n```\npip install ollama\n```\n\n```\nimport ollama\n\nr = ollama.generate(\n    model='duckdb-nsql:7b-q4_0',\n    system='''Here is the database schema that the SQL query will run on:\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);''',\n    prompt='get all columns ending with _amount from taxi table',\n)\n\nprint(r['response'])\n```\n\n\n## Training Data\n\n200k DuckDB text-to-SQL pairs, synthetically generated using Mixtral-8x7B-Instruct-v0.1, guided by the DuckDB v0.9.2 documentation. And text-to-SQL pairs from NSText2SQL that were transpiled to DuckDB SQL using sqlglot.\n\n## Training Procedure\n\nDuckDB-NSQL was trained using cross-entropy loss to maximize the likelihood of sequential inputs. For finetuning on text-to-SQL pairs, we only compute the loss over the SQL portion of the pair. The model is trained using 80GB A100s, leveraging data and model parallelism. We fine-tuned for 10 epochs.\n\n## Intended Use and Limitations\n\nThe model was designed for text-to-SQL generation tasks from given table schema and natural language prompts. The model works best with the prompt format defined below and outputs. In contrast to existing text-to-SQL models, the SQL generation is not contrained to SELECT statements, but can generate any valid DuckDB SQL statement, including statements for official DuckDB extensions.\n\n## References\n\n[Hugging Face](https://huggingface.co/motherduckdb/DuckDB-NSQL-7B-v0.1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "duckdb-nsql:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 3.8,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Developers working with SQL queries.",
    "pulls": 210000,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "e091e112-5892-4b4f-b648-4f58fe3a002f",
    "model_identifier": "nuextract",
    "model_name": "nuextract",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nuextract",
    "description": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nStructure Extraction Model by NuMind 🔥\n\n\nNuExtract is a version of phi-3-mini, fine-tuned on a private high-quality synthetic dataset for information extraction. To use the model, provide an input text (less than 2000 tokens) and a JSON template describing the information you need to extract.\n\n\nNote: This model is purely extractive, so all text output by the model is present as is in the original text. You can also provide an example of output formatting to help the model understand your task more precisely.\n\n\nUsage\n\n\nPrompt Format\n\n\nThis model works best when using a specific prompt format to extract text:\n\n\n### Template:\n{\n    \"Model\": {\n        \"Name\": \"\",\n        \"Number of parameters\": \"\",\n    },\n    \"Usage\": {\n        \"Use case\": [],\n        \"Licence\": \"\"\n    }\n}\n### Example:\n{\n    \"Model\": {\n        \"Name\": \"Llama3\",\n        \"Number of parameters\": \"8 billion\",\n    },\n    \"Usage\": {\n        \"Use case\":[\n\t\t\t\"chat\",\n\t\t\t\"code completion\"\n\t\t],\n        \"Licence\": \"Meta Llama3\"\n    }\n}\n### Text:\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. \n\nCode: https://github.com/mistralai/mistral-src \nWebpage: https://mistral.ai/news/announcing-mistral-7b/\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Structure Extraction Model by NuMind 🔥 \n\nNuExtract is a version of phi-3-mini, fine-tuned on a private high-quality synthetic dataset for information extraction. To use the model, provide an input text (less than 2000 tokens) and a JSON template describing the information you need to extract.\n\nNote: This model is purely extractive, so all text output by the model is present as is in the original text. You can also provide an example of output formatting to help the model understand your task more precisely.\n\n## Usage\n### Prompt Format\nThis model works best when using a specific prompt format to extract text:\n```sh \n### Template:\n{\n    \"Model\": {\n        \"Name\": \"\",\n        \"Number of parameters\": \"\",\n    },\n    \"Usage\": {\n        \"Use case\": [],\n        \"Licence\": \"\"\n    }\n}\n### Example:\n{\n    \"Model\": {\n        \"Name\": \"Llama3\",\n        \"Number of parameters\": \"8 billion\",\n    },\n    \"Usage\": {\n        \"Use case\":[\n\t\t\t\"chat\",\n\t\t\t\"code completion\"\n\t\t],\n        \"Licence\": \"Meta Llama3\"\n    }\n}\n### Text:\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. \n\nCode: https://github.com/mistralai/mistral-src \nWebpage: https://mistral.ai/news/announcing-mistral-7b/\n```\n\n### References\n[Hugging Face](https://huggingface.co/numind/NuExtract)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3.8b"
    ],
    "memory_requirements": [
      {
        "tag": "nuextract:latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.2,
    "use_cases": [
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for information extraction tasks.",
    "pulls": 208100,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "598278b4-c2a1-41d1-98c4-80c19bb1070a",
    "model_identifier": "tulu3",
    "model_name": "tulu3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/tulu3",
    "description": "Tülu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLlama-3.1-Tulu-3-8B\n\n\nTülu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques. Tülu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nGitHub\n\n\nPaper\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/tulu3/157ae836-8188-453b-b3ad-30406919feea\" width=\"300\" />\n\n**Llama-3.1-Tulu-3-8B**\n\nTülu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques. Tülu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.\n\n![1732169954-tulu-3-results.webp](/assets/library/tulu3/aba8f2ff-914e-4a25-9140-f4c1da02ad08)\n\n## References\n\n[Blog Post](https://allenai.org/blog/tulu-3)\n\n[GitHub](https://github.com/allenai/open-instruct)\n\n[Paper](https://allenai.org/papers/tulu-3-report.pdf)\n\n[Hugging Face](https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "tulu3:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "tulu3:70b",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 4.9,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Math"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced text processing tasks.",
    "pulls": 207200,
    "tags": 9,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "74245f87-c10e-48a1-8014-aa83527ac1eb",
    "model_identifier": "mistrallite",
    "model_name": "mistrallite",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistrallite",
    "description": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistrallite is a fine-tuned model based on \nMistral\n, with enhanced capabilities of processing long context (up to 32K tokens). This model is able to perform significantly better on several long context retrieve and answering tasks.\n\n\nReference\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg\" width=\"180\" />\n\nMistrallite is a fine-tuned model based on [Mistral](https://ollama.ai/library/mistral), with enhanced capabilities of processing long context (up to 32K tokens). This model is able to perform significantly better on several long context retrieve and answering tasks.\n\n## Reference\n\n[Hugging Face](https://huggingface.co/amazon/MistralLite)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "mistrallite:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Data Scientists for long context processing tasks.",
    "pulls": 207800,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f56d5120-188b-4c92-9fed-fb0742325894",
    "model_identifier": "bespoke-minicheck",
    "model_name": "bespoke-minicheck",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/bespoke-minicheck",
    "description": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThis is a grounded factuality checking model developed by \nBespoke Labs\n.\n\n\nThe model takes as input a document (text) and a sentence and determines whether the sentence is supported by the document. In order to fact-check a multi-sentence claim, the claim should first be broken up into sentences. The document does not need to be chunked unless it exceeds 32K tokens.\n\n\n\n\nBespoke-MiniCheck is the SOTA fact-checking model despite its small size.\n\n\nUsage\n\n\nThe prompt template is as follows:\n\n\nDocument: {document}\nClaim: {claim}\n\n\n\nThe response will either be \nYes\n or \nNo\n.\n\n\nExamples\n\n\nPrompt\n\n\nDocument: A group of students gather in the school library to study for their upcoming final exams.\nClaim: The students are preparing for an examination.\n\n\n\nResponse\n\n\nYes\n\n\n\nPrompt\n\n\nDocument: A group of students gather in the school library to study for their upcoming final exams.\nClaim: The students are on vacation.\n\n\n\nResponse\n\n\nNo\n\n\n\nModel performance\n\n\n\n\nThe performance of these models is evaluated on our new collected benchmark (unseen by our models during training), \nLLM-AggreFact\n, from 11 recent human annotated datasets on fact-checking and grounding LLM generations. \nBespoke-MiniCheck-7B is the SOTA fact-checking model despite its small size.\n\n\nReferences\n\n\nWebsite\n\n\nPaper\n\n\nLLM-AggreFact Leaderboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/bespoke-minicheck/640ec552-feac-4a35-8651-a85ba3f1f9bd\" width=\"480\" />\n\nThis is a grounded factuality checking model developed by [Bespoke Labs](https://bespokelabs.ai).\n\nThe model takes as input a document (text) and a sentence and determines whether the sentence is supported by the document. In order to fact-check a multi-sentence claim, the claim should first be broken up into sentences. The document does not need to be chunked unless it exceeds 32K tokens.\n\n![bespoke-minicheck-howitworks.png](https://ollama.com/assets/library/bespoke-minicheck/4a1f8cce-a9b2-41e1-8d0a-cb4f1c6b5793)\n\nBespoke-MiniCheck is the SOTA fact-checking model despite its small size.\n\n## Usage\n\nThe prompt template is as follows:\n\n```\nDocument: {document}\nClaim: {claim}\n```\n\nThe response will either be `Yes` or `No`.\n\n## Examples\n\n<sub>Prompt</sub>\n```\nDocument: A group of students gather in the school library to study for their upcoming final exams.\nClaim: The students are preparing for an examination.\n```\n\n<sub>Response</sub>\n```\nYes\n```\n\n\n<sub>Prompt</sub>\n```\nDocument: A group of students gather in the school library to study for their upcoming final exams.\nClaim: The students are on vacation.\n```\n\n<sub>Response</sub>\n```\nNo\n```\n\n## Model performance\n\n![performance.png](https://ollama.com/assets/jmorgan/bespoke-minicheck/5a757ad2-5eff-4440-a2e7-9efc0bad9703)\n\nThe performance of these models is evaluated on our new collected benchmark (unseen by our models during training), [LLM-AggreFact](https://huggingface.co/datasets/lytang/LLM-AggreFact), from 11 recent human annotated datasets on fact-checking and grounding LLM generations. **Bespoke-MiniCheck-7B is the SOTA fact-checking model despite its small size.**\n\n## References\n\n[Website](https://bespokelabs.ai/bespoke-minicheck)\n\n[Paper](https://arxiv.org/pdf/2404.10774)\n\n[LLM-AggreFact Leaderboard](https://llm-aggrefact.github.io/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "bespoke-minicheck:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Fact-checking model for short documents and sentences",
    "pulls": 206900,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "b311d8dc-ed5a-40dc-946c-6e7c94ef4890",
    "model_identifier": "notux",
    "model_name": "notux",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/notux",
    "description": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThis model is a fine-tuned version of \nMixtral\n using a high-quality, curated dataset. As of Dec 26th 2023, this model is the top ranked MoE (Mixture of Experts) model on the \nHugging Face Open LLM Leaderboard\n.\n\n\nReferences\n\n\nHuggingFace\n\n\nArgilla\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/4d57573b-8a00-46ea-8d62-e1e6f0bd1411\" width=\"280\" />\n\nThis model is a fine-tuned version of [Mixtral](https://ollama.ai/library/mixtral) using a high-quality, curated dataset. As of Dec 26th 2023, this model is the top ranked MoE (Mixture of Experts) model on the [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n\n## References\n\n[HuggingFace](https://huggingface.co/argilla/notux-8x7b-v1)\n\n[Argilla](https://argilla.io/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8x7b"
    ],
    "memory_requirements": [
      {
        "tag": "notux:latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "26GB",
        "size_gb": 26.0,
        "recommended_ram_gb": 32.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 26.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "AI models needing a fine-tuned model for specific tasks.",
    "pulls": 204100,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "776703fc-a815-477c-a63a-b5d26f8ec98e",
    "model_identifier": "notus",
    "model_name": "notus",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/notus",
    "description": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNotus is a model by \nArgilla\n, fine-tuned from \nZephyr\n using a high-quality dataset.\n\n\nReferences\n\n\nHuggingFace\n\n\nArgilla\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/4d57573b-8a00-46ea-8d62-e1e6f0bd1411\" width=\"280\" />\n\nNotus is a model by [Argilla](https://argilla.io/), fine-tuned from [Zephyr](https://ollama.ai/library/zephyr) using a high-quality dataset.\n\n## References\n\n[HuggingFace](https://huggingface.co/argilla/notus-7b-v1)\n\n[Argilla](https://argilla.io/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "notus:latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.1GB",
        "size_gb": 4.1,
        "recommended_ram_gb": 5.1,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.1,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Conversational AI and text summarization tasks.",
    "pulls": 203700,
    "tags": 18,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1e081ab8-4f19-4a27-8b58-0e35623ea178",
    "model_identifier": "wizard-vicuna",
    "model_name": "wizard-vicuna",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/wizard-vicuna",
    "description": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nWizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.\n\n\nCLI\n\n\nOpen the terminal and run \nollama run wizard-vicuna\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nMemory requirements\n\n\n\n\n13b models generally require at least 16GB of RAM\n\n\n\n\nReference\n\n\nThe Bloke\n\n\nMelodysDreamj\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/263123848-e99cc861-483d-4b53-a8ce-5d9c372d343b.png\" width=\"500\" />\n\nWizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.\n\n### CLI\n\nOpen the terminal and run `ollama run wizard-vicuna`\n\n### API\n\nExample: \n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n  ```\n\n## Memory requirements\n\n- 13b models generally require at least 16GB of RAM\n\n\n## Reference\n\n[The Bloke](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML)\n\n[MelodysDreamj](https://huggingface.co/junelee/wizard-vicuna-13b)\n\n\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "wizard-vicuna:latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 7.4,
    "use_cases": [
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks",
    "pulls": 203000,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "919bbcc2-1707-4b8f-a14b-0d3fc174ecff",
    "model_identifier": "bge-large",
    "model_name": "bge-large",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/bge-large",
    "description": "Embedding model from BAAI mapping texts to vectors.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nFlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs.\n\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs.\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "335m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": null,
    "best_for": "For tasks requiring text to vector mapping and embedding.",
    "pulls": 199500,
    "tags": 3,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "594676ed-1506-4dee-91b2-44b999f1a67f",
    "model_identifier": "firefunction-v2",
    "model_name": "firefunction-v2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/firefunction-v2",
    "description": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nFirefunction-v2 is competitive with GPT-4o function calling capabilities, scoring 0.81 on a medley public benchmarks vs 0.80 for GPT-4o.\n\n\nFirefunction-v2 is optimized for real world scenarios including multi-turn conversation, instruction following and parallel function calling. It retains Llama 3’s multi-turn instruction capability (0.84 vs 0.89 on MT bench) while consistently outscoring Llama 3 on function calling tasks (0.51 vs 0.30 on Nexus parallel multi function eval)\n\n\nReferences\n\n\nBlog Post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/firefunction-v2/a2b66a3e-ff43-4eae-b95c-d71b669384a5\" width=\"280\" />\n\nFirefunction-v2 is competitive with GPT-4o function calling capabilities, scoring 0.81 on a medley public benchmarks vs 0.80 for GPT-4o.\n\nFirefunction-v2 is optimized for real world scenarios including multi-turn conversation, instruction following and parallel function calling. It retains Llama 3’s multi-turn instruction capability (0.84 vs 0.89 on MT bench) while consistently outscoring Llama 3 on function calling tasks (0.51 vs 0.30 on Nexus parallel multi function eval)\n\n## References\n\n[Blog Post](https://fireworks.ai/blog/firefunction-v2-launch-post)\n\n[Hugging Face](https://huggingface.co/fireworks-ai/llama-3-firefunction-v2)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "firefunction-v2:latest",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 40.0,
    "use_cases": [
      "Function Calling",
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers focusing on function calling tasks.",
    "pulls": 200400,
    "tags": 17,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d3692ddc-2a82-45e8-b7e4-e9bc04588b95",
    "model_identifier": "codebooga",
    "model_name": "codebooga",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codebooga",
    "description": "A high-performing code instruct model created by merging two existing code models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCodebooga was created by merging two existing models:\n\n\n\n\nPhind-CodeLlama-34B-v2\n\n\nWizardCoder-Python-34B-V1.0\n\n\n\n\nIt was created by using the \nBlockMerge Gradient\n script.\n\n\nExample prompts\n\n\nWrite the javascript react code to render a sign up form\n\n\n\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n\n\n\nRecommended parameters\n\n\ntemperature: 1.31\ntop_p: 0.14\nrepeat_penalty: 1.17\ntop_k: 49\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/700d0dcf-3bc0-406f-b289-e48965e8f602\" width=\"280\" />\n\nCodebooga was created by merging two existing models:\n\n1. [Phind-CodeLlama-34B-v2](https://ollama.ai/library/phind-codellama)\n2. [WizardCoder-Python-34B-V1.0](https://ollama.ai/library/wizardcoder)\n\nIt was created by using the [BlockMerge Gradient](https://github.com/Gryphe/BlockMerge_Gradient) script.\n\n### Example prompts\n\n```\nWrite the javascript react code to render a sign up form\n```\n\n```\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n```\n\n### Recommended parameters\n\n```\ntemperature: 1.31\ntop_p: 0.14\nrepeat_penalty: 1.17\ntop_k: 49\n```\n\n## References\n\n[Hugging Face](https://huggingface.co/oobabooga/CodeBooga-34B-v0.1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "codebooga:latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 19.0,
    "use_cases": [
      "Code Generation",
      "Code Review"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for code-related tasks",
    "pulls": 199700,
    "tags": 16,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d24e3156-d8ea-49b2-9110-0adbe8f85a04",
    "model_identifier": "llava-phi3",
    "model_name": "llava-phi3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llava-phi3",
    "description": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nllava-phi3\n is a LLaVA model fine-tuned from Phi 3 Mini 4k, with strong performance benchmarks on par with the original LLaVA model:\n\n\n\n\nReferences\n\n\nHugging Face\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/llava-llama3/dc3b65cd-62de-45cd-93f9-5c6da62214fa\" width=\"320\" />\n\n`llava-phi3` is a LLaVA model fine-tuned from Phi 3 Mini 4k, with strong performance benchmarks on par with the original LLaVA model:\n\n<img src=\"https://ollama.com/assets/library/llava-phi3/c6ad7ff5-28e3-4ff8-9469-92ae66517b88\" width=\"400\" />\n\n## References\n\n[Hugging Face](https://huggingface.co/xtuner/llava-phi-3-mini-gguf)\n\n[GitHub](https://github.com/InternLM/xtuner/tree/main)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "3.8b"
    ],
    "memory_requirements": [
      {
        "tag": "llava-phi3:latest",
        "size": "2.9GB",
        "size_gb": 2.9,
        "recommended_ram_gb": 3.6,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "2.9GB",
        "size_gb": 2.9,
        "recommended_ram_gb": 3.6,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 2.9,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Users needing a small LLaVA model for text generation and reasoning tasks.",
    "pulls": 198400,
    "tags": 4,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "81d3f378-8ff2-41a4-8755-4009effb7f5d",
    "model_identifier": "open-orca-platypus2",
    "model_name": "open-orca-platypus2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/open-orca-platypus2",
    "description": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe OpenOrca Platypus2 model is a 13 billion parameter model which is a merge of the OpenOrca OpenChat model and the Garage-bAInd Platypus2-13B model which are both fine tunings of the Llama 2 model. It is designed to be a general-use model that can be used for chat, text generation, and code generation.\n\n\nGet started with OpenOrca Platypus 2\n\n\nThe model used in the example below is the OpenOrca Platypus 2 model, with 13b parameters, which is a general-use model.\n\n\nAPI\n\n\n\n\nStart Ollama server (Run \nollama serve\n)\n\n\nRun the model\n\n\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"open-orca-platypus2\",\n  \"prompt\":\"Tell me a joke about ropes.\"\n }'\n\n\n\nCLI\n\n\n\n\nInstall Ollama\n\n\nOpen the terminal and run \nollama run open-orca-platypus2\n\n\n\n\nNote: The \nollama run\n command performs an \nollama pull\n if the model is not already downloaded. To download the model without running it, use \nollama pull open-orca-platypus2\n\n\nMemory requirements\n\n\n\n\n13b models generally require at least 16GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n\n\n\n\n\n\nAliases\n\n\n\n\n\n\n\n\n\n\nlatest, 13b, 13b-q4_0\n\n\n\n\n\n\n\n\nModel source\n\n\nOpenOrca Platypus 2 source on Ollama\n\n\n13b parameters source:\n \nOpenOrca\n\n\nReferences\n\n\nPlatypus: Quick, Cheap, and Powerful Refinement of LLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://user-images.githubusercontent.com/633681/262757252-66e5d8a6-40d8-4cd8-b4b9-30d3f7dc1466.png\" style=\"max-width:60%\" />\n\nThe OpenOrca Platypus2 model is a 13 billion parameter model which is a merge of the OpenOrca OpenChat model and the Garage-bAInd Platypus2-13B model which are both fine tunings of the Llama 2 model. It is designed to be a general-use model that can be used for chat, text generation, and code generation.\n\n## Get started with OpenOrca Platypus 2\n\nThe model used in the example below is the OpenOrca Platypus 2 model, with 13b parameters, which is a general-use model.\n\n### API\n\n1. Start Ollama server (Run `ollama serve`)\n2. Run the model\n\n  ```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"open-orca-platypus2\",\n  \"prompt\":\"Tell me a joke about ropes.\"\n }'\n  ```\n\n### CLI\n\n1. Install Ollama\n2. Open the terminal and run `ollama run open-orca-platypus2`\n\nNote: The `ollama run` command performs an `ollama pull` if the model is not already downloaded. To download the model without running it, use `ollama pull open-orca-platypus2`\n\n## Memory requirements\n\n- 13b models generally require at least 16GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n| **Aliases** |\n| --- |\n| latest, 13b, 13b-q4_0 |\n\n## Model source \n\n\n\n**OpenOrca Platypus 2 source on Ollama**\n\n13b parameters source:\n [OpenOrca](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B)\n\n\n\n\n\n\n## References\n\n[Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://arxiv.org/abs/2308.07317 )\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "open-orca-platypus2:latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 7.4,
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "OpenOrca Platypus2 model is a general-use model designed for various text and code generation tasks.",
    "pulls": 196300,
    "tags": 17,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6272a681-ac56-4991-9593-4651ec02a65a",
    "model_identifier": "dbrx",
    "model_name": "dbrx",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dbrx",
    "description": "DBRX is an open, general-purpose LLM created by Databricks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nDBRX requires \nOllama 0.1.32\n\n\n\n\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data.\n\n\nIt is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.\n\n\nReferences\n\n\nBlog Post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/582204f0-53c4-4e67-b710-7c503a6c8faa\" width=\"640\" />\n\n> DBRX requires [Ollama 0.1.32](https://github.com/ollama/ollama/releases/tag/v0.1.32)\n\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data.\n\nIt is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.\n\n## References\n\n[Blog Post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n\n[HuggingFace](https://huggingface.co/databricks/dbrx-instruct)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "132b"
    ],
    "memory_requirements": [
      {
        "tag": "dbrx:latest",
        "size": "74GB",
        "size_gb": 74.0,
        "recommended_ram_gb": 92.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "74GB",
        "size_gb": 74.0,
        "recommended_ram_gb": 92.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 74.0,
    "use_cases": [
      "Code Generation",
      "Question Answering",
      "Text Summarization",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Data Scientists who need a powerful code model.",
    "pulls": 190900,
    "tags": 7,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ac138a7b-5417-496b-8cdc-3326546f0698",
    "model_identifier": "deepseek-ocr",
    "model_name": "deepseek-ocr",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-ocr",
    "description": "DeepSeek-OCR is a vision-language model that can perform token-efficient OCR.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek-OCR requires \nOllama v0.13.0\n or later.\n\n\n\n\nDeepSeek-OCR is a vision-language model that can perform token-efficient optical character recognition (OCR).\n\n\n\n\nExample inputs\n\n\nPlease note, the model is sensitive to its input. For example, a missing punctuation or new line may cause an improper output.\n\n\nollama run deepseek-ocr \"/path/to/image\\n<|grounding|>Given the layout of the image.\"\n\n\n\nollama run deepseek-ocr \"/path/to/image\\nFree OCR.\"\n\n\n\nollama run deepseek-ocr \"/path/to/image\\nParse the figure.\"\n\n\n\nollama run deepseek-ocr \"/path/to/image\\nExtract the text in the image.\"\n\n\n\nollama run deepseek-ocr \"/path/to/image\\n<|grounding|>Convert the document to markdown.\"\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nArxiv paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" />\n\n> DeepSeek-OCR requires [Ollama v0.13.0](https://github.com/ollama/ollama/releases) or later.\n\nDeepSeek-OCR is a vision-language model that can perform token-efficient optical character recognition (OCR). \n\n![fig1.png](/assets/library/deepseek-ocr/e93c9353-3836-4680-a7f1-148ad3e47eff) \n\n### Example inputs \n\nPlease note, the model is sensitive to its input. For example, a missing punctuation or new line may cause an improper output.  \n\n```\nollama run deepseek-ocr \"/path/to/image\\n<|grounding|>Given the layout of the image.\"\n```\n\n```\nollama run deepseek-ocr \"/path/to/image\\nFree OCR.\"\n```\n\n```\nollama run deepseek-ocr \"/path/to/image\\nParse the figure.\"\n```\n\n```\nollama run deepseek-ocr \"/path/to/image\\nExtract the text in the image.\"\n```\n\n```\nollama run deepseek-ocr \"/path/to/image\\n<|grounding|>Convert the document to markdown.\"\n```\n\n### Examples \n\n![show1.jpg](/assets/library/deepseek-ocr/445a87aa-b34e-4a85-8aba-921dd54cfd86)\n\n![show2.jpg](/assets/library/deepseek-ocr/78c11fd6-d1be-4983-a08a-5188c65fcd1f)\n\n![show3.jpg](/assets/library/deepseek-ocr/44b2ac4a-d52e-4f77-843d-3c828e7ecaf0)\n\n![show4.jpg](/assets/library/deepseek-ocr/39e4d41b-634e-407a-a26e-7880c0a2b137)\n\n### References \n\n- [Arxiv paper](https://arxiv.org/abs/2510.18234)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-ocr:latest",
        "size": "6.7GB",
        "size_gb": 6.7,
        "recommended_ram_gb": 8.4,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "6.7GB",
        "size_gb": 6.7,
        "recommended_ram_gb": 8.4,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 6.7,
    "use_cases": [
      "Image Understanding",
      "Function Calling"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for image processing tasks.",
    "pulls": 188300,
    "tags": 3,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "9ae515d4-2075-479b-b87a-550d30447ec8",
    "model_identifier": "goliath",
    "model_name": "goliath",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/goliath",
    "description": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nA large model used by merging the layers of two models: \nXwin\n and Euryale.\n\n\nReferences\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA large model used by merging the layers of two models: [Xwin](https://ollama.ai/library/xwinlm) and Euryale.\n\n\n## References\n\n[HuggingFace](https://huggingface.co/alpindale/goliath-120b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [],
    "memory_requirements": [
      {
        "tag": "goliath:120b-q2_K",
        "size": "50GB",
        "size_gb": 50.0,
        "recommended_ram_gb": 62.5,
        "quantization": "q2_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q3_K_S",
        "size": "51GB",
        "size_gb": 51.0,
        "recommended_ram_gb": 63.8,
        "quantization": "q3_k_s",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q3_K_M",
        "size": "56GB",
        "size_gb": 56.0,
        "recommended_ram_gb": 70.0,
        "quantization": "q3_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q3_K_L",
        "size": "62GB",
        "size_gb": 62.0,
        "recommended_ram_gb": 77.5,
        "quantization": "q3_k_l",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:latest",
        "size": "66GB",
        "size_gb": 66.0,
        "recommended_ram_gb": 82.5,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q4_0",
        "size": "66GB",
        "size_gb": 66.0,
        "recommended_ram_gb": 82.5,
        "quantization": "q4_0",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q4_K_M",
        "size": "71GB",
        "size_gb": 71.0,
        "recommended_ram_gb": 88.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q4_1",
        "size": "74GB",
        "size_gb": 74.0,
        "recommended_ram_gb": 92.5,
        "quantization": "q4_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q5_0",
        "size": "81GB",
        "size_gb": 81.0,
        "recommended_ram_gb": 101.2,
        "quantization": "q5_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q5_K_S",
        "size": "81GB",
        "size_gb": 81.0,
        "recommended_ram_gb": 101.2,
        "quantization": "q5_k_s",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q5_K_M",
        "size": "83GB",
        "size_gb": 83.0,
        "recommended_ram_gb": 103.8,
        "quantization": "q5_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q5_1",
        "size": "88GB",
        "size_gb": 88.0,
        "recommended_ram_gb": 110.0,
        "quantization": "q5_1",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q6_K",
        "size": "97GB",
        "size_gb": 97.0,
        "recommended_ram_gb": 121.2,
        "quantization": "q6_k",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-q8_0",
        "size": "125GB",
        "size_gb": 125.0,
        "recommended_ram_gb": 156.2,
        "quantization": "q8_0",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "goliath:120b-fp16",
        "size": "236GB",
        "size_gb": 236.0,
        "recommended_ram_gb": 295.0,
        "quantization": "fp16",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 50.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Translation",
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and Researchers for complex language tasks.",
    "pulls": 185700,
    "tags": 16,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d5c44862-af5e-46d2-96fc-5c575f93c319",
    "model_identifier": "nemotron-3-nano",
    "model_name": "nemotron-3-nano",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nemotron-3-nano",
    "description": "Nemotron 3 Nano - A new Standard for Efficient, Open, and Intelligent Agentic Models",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNemotron 3 Nano 30B\n\n\nollama run nemotron-3-nano:30b\n\n\n\nOllama’s Cloud\n\n\nollama run nemotron-3-nano:30b-cloud\n\n\n\nModel Dates:\n\n\nSeptember 2025 - December 2025\n\n\nData Freshness:\n\n\n\n\nThe post-training data has a cutoff date of November 28, 2025.\n\n\n\n\nThe pre-training data has a cutoff date of June 25, 2025.\n\n\n\n\nWhat is Nemotron?\n\n\nNVIDIA Nemotron™ is a family of open models with open weights, training data, and recipes, delivering leading efficiency and accuracy for building specialized AI agents.\n\n\nNemotron 3 Nano is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model’s reasoning capabilities can be configured through a flag in the chat template. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.\n\n\nThe model employs a hybrid Mixture-of-Experts (MoE) architecture, consisting of 23 Mamba-2 and MoE layers, along with 6 Attention layers. Each MoE layer includes 128 experts plus 1 shared expert, with 6 experts activated per token. The model has 3.5B active parameters and 30B parameters in total.\n\n\nThe supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.\n\n\nReasoning Benchmark Evaluations\n\n\n\n\n\n\n\n\nTask\n\n\nNVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n\n\nQwen3-30B-A3B-Thinking-2507\n\n\nGPT-OSS-20B\n\n\n\n\n\n\n\n\n\n\nGeneral Knowledge\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU-Pro\n\n\n78.3\n\n\n80.9\n\n\n75.0\n\n\n\n\n\n\nReasoning\n\n\n\n\n\n\n\n\n\n\n\n\nAIME25 (no tools)\n\n\n89.1\n\n\n85.0\n\n\n91.7\n\n\n\n\n\n\nAIME25 (with tools)\n\n\n99.2\n\n\n-\n\n\n98.7\n\n\n\n\n\n\nGPQA (no tools)\n\n\n73.0\n\n\n73.4\n\n\n71.5\n\n\n\n\n\n\nGPQA (with tools)\n\n\n75.0\n\n\n-\n\n\n74.2\n\n\n\n\n\n\nLiveCodeBench (v6 2025-08–2025-05)\n\n\n68.3\n\n\n66.0\n\n\n61.0\n\n\n\n\n\n\nSciCode (subtask)\n\n\n33.3\n\n\n33.0\n\n\n34.0\n\n\n\n\n\n\nHLE (no tools)\n\n\n10.6\n\n\n9.8\n\n\n10.9\n\n\n\n\n\n\nHLE (with tools)\n\n\n15.5\n\n\n-\n\n\n17.3\n\n\n\n\n\n\nMiniF2F pass@1\n\n\n50.0\n\n\n5.7\n\n\n12.1\n\n\n\n\n\n\nMiniF2F pass@32\n\n\n79.9\n\n\n16.8\n\n\n43.0\n\n\n\n\n\n\nAgentic\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal Bench (hard subset)\n\n\n8.5\n\n\n5.0\n\n\n6.0\n\n\n\n\n\n\nSWE-Bench (OpenHands)\n\n\n38.8\n\n\n22.0\n\n\n34.0\n\n\n\n\n\n\nTauBench V2 (Airline)\n\n\n48.0\n\n\n58.0\n\n\n38.0\n\n\n\n\n\n\nTauBench V2 (Retail)\n\n\n56.9\n\n\n58.8\n\n\n38.0\n\n\n\n\n\n\nTauBench V2 (Telecom)\n\n\n42.2\n\n\n26.3\n\n\n49.7\n\n\n\n\n\n\nTauBench V2 (Average)\n\n\n49.0\n\n\n47.7\n\n\n48.7\n\n\n\n\n\n\nBFCL v4\n\n\n53.8\n\n\n46.4*\n\n\n-\n\n\n\n\n\n\nChat & Instruction Following\n\n\n\n\n\n\n\n\n\n\n\n\nIFBench (prompt)\n\n\n71.5\n\n\n51.0\n\n\n65.0\n\n\n\n\n\n\nScale AI Multi Challenge\n\n\n38.5\n\n\n44.8\n\n\n33.8\n\n\n\n\n\n\nArena-Hard-V2 (Hard Prompt)\n\n\n72.1\n\n\n49.6*\n\n\n71.2*\n\n\n\n\n\n\nArena-Hard-V2 (Creative Writing)\n\n\n63.2\n\n\n66.0*\n\n\n25.9&\n\n\n\n\n\n\nArena-Hard-V2 (Average)\n\n\n67.7\n\n\n57.8\n\n\n48.6\n\n\n\n\n\n\nLong Context\n\n\n\n\n\n\n\n\n\n\n\n\nAA-LCR\n\n\n35.9\n\n\n59.0\n\n\n34.0\n\n\n\n\n\n\nRULER-100@256k\n\n\n92.9\n\n\n89.4\n\n\n-\n\n\n\n\n\n\nRULER-100@512k\n\n\n91.3\n\n\n84.0\n\n\n-\n\n\n\n\n\n\nRULER-100@1M\n\n\n86.3\n\n\n77.5\n\n\n-\n\n\n\n\n\n\nMultilingual\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU-ProX (avg over langs)\n\n\n59.5\n\n\n77.6*\n\n\n69.1*\n\n\n\n\n\n\nWMT24++ (en->xx)\n\n\n86.2\n\n\n85.6\n\n\n83.2\n\n\n\n\n\n\n\n\nLicense/Terms of Use\n\n\nGoverning Terms: Use of this model is governed by the \nNVIDIA Open Model License Agreement\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/nemotron-3-nano/b21aaa72-147e-4db4-9a7f-fc716e4fab47)\n\n**Nemotron 3 Nano 30B** \n\n```\nollama run nemotron-3-nano:30b\n```\n\n**Ollama's Cloud**\n\n```\nollama run nemotron-3-nano:30b-cloud\n```\n\n**Model Dates:**\n\nSeptember 2025 \\- December 2025\n\n**Data Freshness:**\n\n* The post-training data has a cutoff date of November 28, 2025\\.  \n* The pre-training data has a cutoff date of June 25, 2025\\.\n\n### What is Nemotron?\n\nNVIDIA Nemotron™ is a family of open models with open weights, training data, and recipes, delivering leading efficiency and accuracy for building specialized AI agents.\n\nNemotron 3 Nano is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be configured through a flag in the chat template. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.\n\nThe model employs a hybrid Mixture-of-Experts (MoE) architecture, consisting of 23 Mamba-2 and MoE layers, along with 6 Attention layers. Each MoE layer includes 128 experts plus 1 shared expert, with 6 experts activated per token. The model has 3.5B active parameters and 30B parameters in total.\n\nThe supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.\n\n### Reasoning Benchmark Evaluations\n\n| Task | NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 | Qwen3-30B-A3B-Thinking-2507 | GPT-OSS-20B |\n| ----- | :---- | :---- | :---- |\n| **General Knowledge** |  |  |  |\n| MMLU-Pro | 78.3 | **80.9** | 75.0 |\n| **Reasoning** |  |  |  |\n| AIME25 (no tools) | 89.1 | 85.0 | **91.7** |\n| AIME25 (with tools) | **99.2** | \\- | 98.7 |\n| GPQA (no tools) | 73.0 | **73.4** | 71.5 |\n| GPQA (with tools) | **75.0** | \\- | 74.2 |\n| LiveCodeBench (v6 2025-08–2025-05) | **68.3** | 66.0 | 61.0 |\n| SciCode (subtask) | 33.3 | 33.0 | **34.0** |\n| HLE (no tools) | 10.6 | 9.8 | **10.9** |\n| HLE (with tools) | 15.5 | \\- | **17.3** |\n| MiniF2F pass@1 | **50.0** | 5.7 | 12.1 |\n| MiniF2F pass@32 | **79.9** | 16.8 | 43.0 |\n| **Agentic** |  |  |  |\n| Terminal Bench (hard subset) | 8.5 | 5.0 | 6.0 |\n| SWE-Bench (OpenHands) | **38.8** | 22.0 | 34.0 |\n| TauBench V2 (Airline) | 48.0 | **58.0** | 38.0 |\n| TauBench V2 (Retail) | 56.9 | **58.8** | 38.0 |\n| TauBench V2 (Telecom) | 42.2 | 26.3 | **49.7** |\n| TauBench V2 (Average) | **49.0** | 47.7 | 48.7 |\n| BFCL v4 | **53.8** | 46.4\\* | \\- |\n| **Chat & Instruction Following** |  |  |  |\n| IFBench (prompt) | **71.5** | 51.0 | 65.0 |\n| Scale AI Multi Challenge | 38.5 | **44.8** | 33.8 |\n| Arena-Hard-V2 (Hard Prompt) | **72.1** | 49.6\\* | 71.2\\* |\n| Arena-Hard-V2 (Creative Writing) |  63.2 | **66.0\\*** | 25.9& |\n| Arena-Hard-V2 (Average) | **67.7** | 57.8 | 48.6 |\n| **Long Context** |  |  |  |\n| AA-LCR | 35.9 | **59.0** | 34.0 |\n| RULER-100@256k | **92.9** | 89.4 | \\- |\n| RULER-100@512k | **91.3** | 84.0 | \\- |\n| RULER-100@1M | **86.3** | 77.5 | \\- |\n| **Multilingual** |  |  |  |\n| MMLU-ProX (avg over langs) | 59.5 | **77.6\\*** | 69.1\\* |\n| WMT24++ (en-\\>xx) | **86.2** | 85.6 | 83.2 |\n\n## License/Terms of Use\n\nGoverning Terms: Use of this model is governed by the [NVIDIA Open Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/).\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "30b"
    ],
    "memory_requirements": [
      {
        "tag": "nemotron-3-nano:latest",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q4_k_m",
        "context": "1M context",
        "context_window": 1000000
      },
      {
        "tag": "latest",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q4_k_m",
        "context": "1M",
        "context_window": 1000000
      }
    ],
    "min_ram_gb": 24.0,
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for specialized AI agent development.",
    "pulls": 176100,
    "tags": 6,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "88316ae1-a0e7-497e-8d1a-c871aec80887",
    "model_identifier": "sailor2",
    "model_name": "sailor2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/sailor2",
    "description": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nSailor2 is a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the \n8B and 20B\n parameter range for production use, alongside \n1B models\n for specialized applications, such as speculative decoding and research purposes. These models, released under the \nApache 2.0 license\n, provide enhanced accessibility to advanced language technologies across the region.\n\n\nSailor2 builds upon the foundation of the awesome multilingual model Qwen 2.5 and is continuously pre-trained on 500B tokens to support 15 languages better with a unified model. These languages include English, Chinese, Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. By addressing the growing demand for diverse, robust, and accessible language models, Sailor2 seeks to serve the underserved in SEA areas with open, inclusive, and accessible multilingual LLMs. The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![logo](/assets/mchiang0610/sailor2/a76a9182-cc11-47e1-bb50-478ad4ccb157)\n\nSailor2 is a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the **8B and 20B** parameter range for production use, alongside **1B models** for specialized applications, such as speculative decoding and research purposes. These models, released under the **Apache 2.0 license**, provide enhanced accessibility to advanced language technologies across the region.\n\nSailor2 builds upon the foundation of the awesome multilingual model Qwen 2.5 and is continuously pre-trained on 500B tokens to support 15 languages better with a unified model. These languages include English, Chinese, Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. By addressing the growing demand for diverse, robust, and accessible language models, Sailor2 seeks to serve the underserved in SEA areas with open, inclusive, and accessible multilingual LLMs. The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1b",
      "8b",
      "20b"
    ],
    "memory_requirements": [
      {
        "tag": "sailor2:1b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "sailor2:latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "sailor2:20b",
        "size": "12GB",
        "size_gb": 12.0,
        "recommended_ram_gb": 15.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.1,
    "use_cases": [
      "Text Summarization",
      "Translation",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks in South-East Asia.",
    "pulls": 165500,
    "tags": 13,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "18a9b18d-d266-4a81-beb0-cf9823255a34",
    "model_identifier": "olmo-3",
    "model_name": "olmo-3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/olmo-3",
    "description": "Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOlmo 3, a new family of 7B and 32B models in both Instruct and Think variants. It has long chain-of-thought thinking to improve reasoning tasks like math and coding.\n\n\nOlmo is a series of Open language models designed to enable the science of language models.\nThese models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details.\n\n\nModels\n\n\nOlmo 3 Instruct 7B\n\n\nollama run olmo-3:7b-instruct\n\n\n\nOlmo 3 Think 7B\n\n\nollama run olmo-3:7b-think\n\n\n\nOlmo 3 Think 32B\n\n\nollama run olmo-3:32b-think\n\n\n\nEvaluation\n\n\nOlmo 3 Instruct 7B\n\n\n\n\n\n\n\n\nBenchmark\n\n\nOlmo3 Instruct 7B\n\n\nQwen 3 8B (no reasoning)\n\n\nQwen 3 VL 8B Instruct\n\n\nQwen 2.5 7B\n\n\nOlmo 2 7B Instruct\n\n\nApertus 8B Instruct\n\n\nGranite 3.3 8B Instruct\n\n\n\n\n\n\n\n\n\n\nMATH\n\n\n87.3\n\n\n82.3\n\n\n91.6\n\n\n71\n\n\n30.1\n\n\n21.9\n\n\n67.3\n\n\n\n\n\n\nAIME 2024\n\n\n44.3\n\n\n26.2\n\n\n55.1\n\n\n11.3\n\n\n1.3\n\n\n0.5\n\n\n7.3\n\n\n\n\n\n\nAIME 2025\n\n\n32.5\n\n\n21.7\n\n\n43.3\n\n\n6.3\n\n\n0.4\n\n\n0.2\n\n\n6.3\n\n\n\n\n\n\nOMEGA\n\n\n28.9\n\n\n20.5\n\n\n32.3\n\n\n13.7\n\n\n5.2\n\n\n5.0\n\n\n10.7\n\n\n\n\n\n\nBigBenchHard\n\n\n71.2\n\n\n73.7\n\n\n85.6\n\n\n68.8\n\n\n43.8\n\n\n42.2\n\n\n61.2\n\n\n\n\n\n\nZebraLogic\n\n\n32.9\n\n\n25.4\n\n\n64.3\n\n\n10.7\n\n\n5.3\n\n\n5.3\n\n\n17.6\n\n\n\n\n\n\nAGI Eval English\n\n\n64.4\n\n\n76\n\n\n84.5\n\n\n69.8\n\n\n56.1\n\n\n50.8\n\n\n64.0\n\n\n\n\n\n\nHumanEvalPlus\n\n\n77.2\n\n\n79.8\n\n\n82.9\n\n\n74.9\n\n\n25.8\n\n\n34.4\n\n\n64.0\n\n\n\n\n\n\nMBPP+\n\n\n60.2\n\n\n64.4\n\n\n66.3\n\n\n62.6\n\n\n40.7\n\n\n42.1\n\n\n54.0\n\n\n\n\n\n\nLiveCodeBench v3\n\n\n29.5\n\n\n53.2\n\n\n55.9\n\n\n34.5\n\n\n7.2\n\n\n7.8\n\n\n11.5\n\n\n\n\n\n\nIFEval\n\n\n85.6\n\n\n86.3\n\n\n87.8\n\n\n73.4\n\n\n72.2\n\n\n71.4\n\n\n77.5\n\n\n\n\n\n\nIFBench\n\n\n32.3\n\n\n29.3\n\n\n34\n\n\n28.4\n\n\n26.7\n\n\n22.1\n\n\n22.3\n\n\n\n\n\n\nMMLU\n\n\n69.1\n\n\n80.4\n\n\n83.6\n\n\n77.2\n\n\n61.6\n\n\n62.7\n\n\n63.5\n\n\n\n\n\n\nPopQA\n\n\n14.1\n\n\n20.4\n\n\n26.5\n\n\n21.5\n\n\n25.5\n\n\n25.5\n\n\n28.9\n\n\n\n\n\n\nGPQA\n\n\n40.4\n\n\n44.6\n\n\n51.1\n\n\n35.6\n\n\n31.3\n\n\n28.8\n\n\n33.0\n\n\n\n\n\n\nAlpacaEval 2 LC\n\n\n40.9\n\n\n49.8\n\n\n73.5\n\n\n23\n\n\n18.3\n\n\n8.1\n\n\n28.6\n\n\n\n\n\n\nSimpleQA\n\n\n79.3\n\n\n79\n\n\n90.3\n\n\n78\n\n\n–\n\n\n–\n\n\n–\n\n\n\n\n\n\nLitQA2\n\n\n38.2\n\n\n39.6\n\n\n30.7\n\n\n29.8\n\n\n–\n\n\n–\n\n\n–\n\n\n\n\n\n\nBFCL\n\n\n49.8\n\n\n60.2\n\n\n66.2\n\n\n55.8\n\n\n–\n\n\n–\n\n\n–\n\n\n\n\n\n\nSafety\n\n\n87.3\n\n\n78\n\n\n80.2\n\n\n73.4\n\n\n93.1\n\n\n72.2\n\n\n73.7\n\n\n\n\n\n\n\n\nOlmo 3 Think 7B\n\n\n\n\n\n\n\n\nBenchmark\n\n\nOlmo 3 Think 7B\n\n\nOpenThinker3-7B\n\n\nNemotron-Nano-9B-v2\n\n\nDeepSeek-R1-Distill-Qwen-7B\n\n\nQwen 3 8B (reasoning)\n\n\nQwen 3 VL 8B Thinker\n\n\nOpenReasoning Nemotron 7B\n\n\n\n\n\n\n\n\n\n\nMATH\n\n\n95.1\n\n\n94.5\n\n\n94.4\n\n\n87.9\n\n\n95.1\n\n\n95.2\n\n\n94.6\n\n\n\n\n\n\nAIME 2024\n\n\n71.6\n\n\n67.7\n\n\n72.1\n\n\n54.9\n\n\n74.0\n\n\n70.9\n\n\n77.0\n\n\n\n\n\n\nAIME 2025\n\n\n64.6\n\n\n57.2\n\n\n58.9\n\n\n40.2\n\n\n67.8\n\n\n61.5\n\n\n73.1\n\n\n\n\n\n\nOMEGA\n\n\n37.8\n\n\n38.4\n\n\n42.4\n\n\n28.5\n\n\n43.4\n\n\n38.1\n\n\n43.2\n\n\n\n\n\n\nBBH\n\n\n86.6\n\n\n77.1\n\n\n86.2\n\n\n73.5\n\n\n84.4\n\n\n86.8\n\n\n81.3\n\n\n\n\n\n\nZebraLogic\n\n\n66.5\n\n\n34.9\n\n\n60.8\n\n\n26.1\n\n\n85.2\n\n\n91.2\n\n\n22.4\n\n\n\n\n\n\nAGI Eval\n\n\n81.5\n\n\n78.6\n\n\n83.1\n\n\n69.5\n\n\n87.0\n\n\n90.1\n\n\n81.4\n\n\n\n\n\n\nHumanEval+\n\n\n89.9\n\n\n87.4\n\n\n89.7\n\n\n83.0\n\n\n80.2\n\n\n83.7\n\n\n89.7\n\n\n\n\n\n\nMBPP+\n\n\n64.7\n\n\n61.4\n\n\n66.1\n\n\n63.5\n\n\n69.1\n\n\n63.0\n\n\n61.2\n\n\n\n\n\n\nLCB v3\n\n\n75.2\n\n\n68.0\n\n\n83.4\n\n\n58.8\n\n\n86.2\n\n\n85.5\n\n\n82.3\n\n\n\n\n\n\nIFEval\n\n\n88.2\n\n\n51.7\n\n\n86.0\n\n\n59.6\n\n\n87.4\n\n\n85.5\n\n\n42.5\n\n\n\n\n\n\nIFBench\n\n\n41.6\n\n\n23.0\n\n\n34.6\n\n\n16.7\n\n\n37.1\n\n\n40.4\n\n\n23.4\n\n\n\n\n\n\nMMLU\n\n\n77.8\n\n\n77.4\n\n\n84.3\n\n\n67.9\n\n\n85.4\n\n\n86.5\n\n\n80.7\n\n\n\n\n\n\nPopQA\n\n\n23.7\n\n\n18.0\n\n\n17.9\n\n\n12.8\n\n\n24.3\n\n\n29.3\n\n\n14.5\n\n\n\n\n\n\nGPQA\n\n\n46.2\n\n\n47.6\n\n\n56.2\n\n\n54.4\n\n\n57.7\n\n\n61.5\n\n\n56.6\n\n\n\n\n\n\nAE 2\n\n\n52.1\n\n\n24.0\n\n\n58.0\n\n\n7.7\n\n\n60.5\n\n\n73.5\n\n\n8.6\n\n\n\n\n\n\n\n\n70.7\n\n\n31.3\n\n\n72.1\n\n\n54.0\n\n\n68.3\n\n\n82.9\n\n\n30.3\n\n\n\n\n\n\n\n\nOlmo 3 Think 32B\n\n\n\n\n\n\n\n\nBenchmark\n\n\nOlmo 3 Think 32B\n\n\nQwen 3 32B\n\n\nQwen 3 VL 32B Thinking\n\n\nQwen 2.5 32B\n\n\nGemma 3 27B Instruct\n\n\nGemma 2 27B Instruct\n\n\nOlmo 2 32B Instruct\n\n\nDeepSeek-R1-Distill-Qwen-32B\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMATH\n\n\n96.1\n\n\n95.4\n\n\n96.7\n\n\n80.2\n\n\n87.4\n\n\n51.5\n\n\n49.2\n\n\n92.6\n\n\n\n\n\n\nAIME 2024\n\n\n76.8\n\n\n80.8\n\n\n86.3\n\n\n15.7\n\n\n28.9\n\n\n4.7\n\n\n4.6\n\n\n70.3\n\n\n\n\n\n\nAIME 2025\n\n\n72.5\n\n\n70.9\n\n\n78.8\n\n\n13.4\n\n\n22.9\n\n\n0.9\n\n\n0.9\n\n\n56.3\n\n\n\n\n\n\nOMEGA\n\n\n50.8\n\n\n47.7\n\n\n50.8\n\n\n19.2\n\n\n24.0\n\n\n9.1\n\n\n9.8\n\n\n38.9\n\n\n\n\n\n\nReasoning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBigBenchHard\n\n\n89.8\n\n\n90.6\n\n\n91.1\n\n\n80.9\n\n\n82.4\n\n\n66.0\n\n\n65.6\n\n\n89.7\n\n\n\n\n\n\nZebraLogic\n\n\n76.0\n\n\n88.3\n\n\n96.1\n\n\n24.1\n\n\n24.8\n\n\n17.2\n\n\n13.3\n\n\n69.4\n\n\n\n\n\n\nAGI Eval English\n\n\n88.2\n\n\n90.0\n\n\n92.2\n\n\n78.9\n\n\n76.9\n\n\n70.9\n\n\n68.4\n\n\n88.1\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHumanEvalPlus\n\n\n91.4\n\n\n91.2\n\n\n90.6\n\n\n82.6\n\n\n79.2\n\n\n67.5\n\n\n44.4\n\n\n92.3\n\n\n\n\n\n\nMBPP+\n\n\n68.0\n\n\n70.6\n\n\n66.2\n\n\n66.6\n\n\n65.7\n\n\n61.2\n\n\n49.0\n\n\n70.1\n\n\n\n\n\n\nLiveCodeBench v3\n\n\n83.5\n\n\n90.2\n\n\n84.8\n\n\n49.9\n\n\n39.0\n\n\n28.7\n\n\n10.6\n\n\n79.5\n\n\n\n\n\n\nIF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIFEval\n\n\n89.0\n\n\n86.5\n\n\n85.5\n\n\n81.9\n\n\n85.4\n\n\n62.1\n\n\n85.8\n\n\n78.7\n\n\n\n\n\n\nIFBench\n\n\n47.6\n\n\n37.3\n\n\n55.1\n\n\n36.7\n\n\n31.3\n\n\n27.8\n\n\n36.4\n\n\n23.8\n\n\n\n\n\n\nKnowledge & QA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU\n\n\n85.4\n\n\n88.8\n\n\n90.1\n\n\n84.6\n\n\n74.6\n\n\n76.1\n\n\n77.1\n\n\n88.0\n\n\n\n\n\n\nPopQA\n\n\n31.9\n\n\n30.7\n\n\n32.2\n\n\n28.0\n\n\n30.2\n\n\n30.4\n\n\n37.2\n\n\n26.7\n\n\n\n\n\n\nGPQA\n\n\n58.1\n\n\n67.3\n\n\n67.4\n\n\n44.6\n\n\n45.0\n\n\n39.9\n\n\n36.4\n\n\n61.8\n\n\n\n\n\n\nChat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpacaEval 2 LC\n\n\n74.2\n\n\n75.6\n\n\n80.9\n\n\n81.9\n\n\n65.5\n\n\n39.8\n\n\n38.0\n\n\n26.2\n\n\n\n\n\n\nSafety\n\n\n68.8\n\n\n69.0\n\n\n82.7\n\n\n81.9\n\n\n68.6\n\n\n74.3\n\n\n83.8\n\n\n63.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Olmo3.png](/assets/library/olmo-3/16042246-aec6-47b4-a434-1a185c9d1522)\n\nOlmo 3, a new family of 7B and 32B models in both Instruct and Think variants. It has long chain-of-thought thinking to improve reasoning tasks like math and coding.\n\nOlmo is a series of Open language models designed to enable the science of language models. \nThese models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details. \n\n### Models\n\n**Olmo 3 Instruct 7B** \n\n```\nollama run olmo-3:7b-instruct\n```\n\n**Olmo 3 Think 7B** \n\n```\nollama run olmo-3:7b-think\n```\n\n\n**Olmo 3 Think 32B**\n\n```\nollama run olmo-3:32b-think\n```\n\n\n\n### Evaluation \n\n**Olmo 3 Instruct 7B**\n\n| **Benchmark** | **Olmo3 Instruct 7B** | **Qwen 3 8B (no reasoning)** | **Qwen 3 VL 8B Instruct** | **Qwen 2.5 7B** | **Olmo 2 7B Instruct** | **Apertus 8B Instruct** | **Granite 3.3 8B Instruct** |\n|:---|---:|---:|---:|---:|:---|:---|:---|\n| MATH | 87.3 | 82.3 | 91.6 | 71 | 30.1 | 21.9 | 67.3 |\n| AIME 2024 | 44.3 | 26.2 | 55.1 | 11.3 | 1.3 | 0.5 | 7.3 |\n| AIME 2025 | 32.5 | 21.7 | 43.3 | 6.3 | 0.4 | 0.2 | 6.3 |\n| OMEGA | 28.9 | 20.5 | 32.3 | 13.7 | 5.2 | 5.0 | 10.7 |\n| BigBenchHard | 71.2 | 73.7 | 85.6 | 68.8 | 43.8 | 42.2 | 61.2 |\n| ZebraLogic | 32.9 | 25.4 | 64.3 | 10.7 | 5.3 | 5.3 | 17.6 |\n| AGI Eval English | 64.4 | 76 | 84.5 | 69.8 | 56.1 | 50.8 | 64.0 |\n| HumanEvalPlus | 77.2 | 79.8 | 82.9 | 74.9 | 25.8 | 34.4 | 64.0 |\n| MBPP+ | 60.2 | 64.4 | 66.3 | 62.6 | 40.7 | 42.1 | 54.0 |\n| LiveCodeBench v3 | 29.5 | 53.2 | 55.9 | 34.5 | 7.2 | 7.8 | 11.5 |\n| IFEval | 85.6 | 86.3 | 87.8 | 73.4 | 72.2 | 71.4 | 77.5 |\n| IFBench | 32.3 | 29.3 | 34 | 28.4 | 26.7 | 22.1 | 22.3 |\n| MMLU | 69.1 | 80.4 | 83.6 | 77.2 | 61.6 | 62.7 | 63.5 |\n| PopQA | 14.1 | 20.4 | 26.5 | 21.5 | 25.5 | 25.5 | 28.9 |\n| GPQA | 40.4 | 44.6 | 51.1 | 35.6 | 31.3 | 28.8 | 33.0 |\n| AlpacaEval 2 LC | 40.9 | 49.8 | 73.5 | 23 | 18.3 | 8.1 | 28.6 |\n| SimpleQA | 79.3 | 79 | 90.3 | 78 | – | – | – |\n| LitQA2 | 38.2 | 39.6 | 30.7 | 29.8 | – | – | – |\n| BFCL | 49.8 | 60.2 | 66.2 | 55.8 | – | – | – |\n| Safety | 87.3 | 78 | 80.2 | 73.4 | 93.1 | 72.2 | 73.7 |\n\n**Olmo 3 Think 7B** \n\n| Benchmark | Olmo 3 Think 7B | OpenThinker3-7B | Nemotron-Nano-9B-v2 | DeepSeek-R1-Distill-Qwen-7B | Qwen 3 8B (reasoning) | Qwen 3 VL 8B Thinker | OpenReasoning Nemotron 7B |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| MATH | 95.1 | 94.5 | 94.4 | 87.9 | 95.1 | 95.2 | 94.6 |\n| AIME 2024 | 71.6 | 67.7 | 72.1 | 54.9 | 74.0 | 70.9 | 77.0 |\n| AIME 2025 | 64.6 | 57.2 | 58.9 | 40.2 | 67.8 | 61.5 | 73.1 |\n| OMEGA | 37.8 | 38.4 | 42.4 | 28.5 | 43.4 | 38.1 | 43.2 |\n| BBH | 86.6 | 77.1 | 86.2 | 73.5 | 84.4 | 86.8 | 81.3 |\n| ZebraLogic | 66.5 | 34.9 | 60.8 | 26.1 | 85.2 | 91.2 | 22.4 |\n| AGI Eval | 81.5 | 78.6 | 83.1 | 69.5 | 87.0 | 90.1 | 81.4 |\n| HumanEval+ | 89.9 | 87.4 | 89.7 | 83.0 | 80.2 | 83.7 | 89.7 |\n| MBPP+ | 64.7 | 61.4 | 66.1 | 63.5 | 69.1 | 63.0 | 61.2 |\n| LCB v3 | 75.2 | 68.0 | 83.4 | 58.8 | 86.2 | 85.5 | 82.3 |\n| IFEval | 88.2 | 51.7 | 86.0 | 59.6 | 87.4 | 85.5 | 42.5 |\n| IFBench | 41.6 | 23.0 | 34.6 | 16.7 | 37.1 | 40.4 | 23.4 |\n| MMLU | 77.8 | 77.4 | 84.3 | 67.9 | 85.4 | 86.5 | 80.7 |\n| PopQA | 23.7 | 18.0 | 17.9 | 12.8 | 24.3 | 29.3 | 14.5 |\n| GPQA | 46.2 | 47.6 | 56.2 | 54.4 | 57.7 | 61.5 | 56.6 |\n| AE 2 | 52.1 | 24.0 | 58.0 | 7.7 | 60.5 | 73.5 | 8.6 |\n| | 70.7 | 31.3 | 72.1 | 54.0 | 68.3 | 82.9 | 30.3 |\n\n**Olmo 3 Think 32B**\n| Benchmark | Olmo 3 Think 32B | Qwen 3 32B | Qwen 3 VL 32B Thinking | Qwen 2.5 32B | Gemma 3 27B Instruct | Gemma 2 27B Instruct | Olmo 2 32B Instruct | DeepSeek-R1-Distill-Qwen-32B |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Math** | | | | | | | | |\n| MATH | 96.1 | 95.4 | 96.7 | 80.2 | 87.4 | 51.5 | 49.2 | 92.6 |\n| AIME 2024 | 76.8 | 80.8 | 86.3 | 15.7 | 28.9 | 4.7 | 4.6 | 70.3 |\n| AIME 2025 | 72.5 | 70.9 | 78.8 | 13.4 | 22.9 | 0.9 | 0.9 | 56.3 |\n| OMEGA | 50.8 | 47.7 | 50.8 | 19.2 | 24.0 | 9.1 | 9.8 | 38.9 |\n| **Reasoning** | | | | | | | | |\n| BigBenchHard | 89.8 | 90.6 | 91.1 | 80.9 | 82.4 | 66.0 | 65.6 | 89.7 |\n| ZebraLogic | 76.0 | 88.3 | 96.1 | 24.1 | 24.8 | 17.2 | 13.3 | 69.4 |\n| AGI Eval English | 88.2 | 90.0 | 92.2 | 78.9 | 76.9 | 70.9 | 68.4 | 88.1 |\n| **Coding** | | | | | | | | |\n| HumanEvalPlus | 91.4 | 91.2 | 90.6 | 82.6 | 79.2 | 67.5 | 44.4 | 92.3 |\n| MBPP+ | 68.0 | 70.6 | 66.2 | 66.6 | 65.7 | 61.2 | 49.0 | 70.1 |\n| LiveCodeBench v3 | 83.5 | 90.2 | 84.8 | 49.9 | 39.0 | 28.7 | 10.6 | 79.5 |\n| **IF** | | | | | | | | |\n| IFEval | 89.0 | 86.5 | 85.5 | 81.9 | 85.4 | 62.1 | 85.8 | 78.7 |\n| IFBench | 47.6 | 37.3 | 55.1 | 36.7 | 31.3 | 27.8 | 36.4 | 23.8 |\n| **Knowledge & QA** | | | | | | | | |\n| MMLU | 85.4 | 88.8 | 90.1 | 84.6 | 74.6 | 76.1 | 77.1 | 88.0 |\n| PopQA | 31.9 | 30.7 | 32.2 | 28.0 | 30.2 | 30.4 | 37.2 | 26.7 |\n| GPQA | 58.1 | 67.3 | 67.4 | 44.6 | 45.0 | 39.9 | 36.4 | 61.8 |\n| **Chat** | | | | | | |",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "olmo-3:latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      },
      {
        "tag": "latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "64K",
        "context_window": 64000
      },
      {
        "tag": "olmo-3:32b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 4.5,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Researchers and Data Scientists for advanced language tasks.",
    "pulls": 160100,
    "tags": 15,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "27383d5a-29ef-467c-bdeb-ace6e939e1a1",
    "model_identifier": "devstral-small-2",
    "model_name": "devstral-small-2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/devstral-small-2",
    "description": "24B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.13.3 or later. \nDownload Ollama\n\n\n\n\nDevstral Small 2\n\n\nDevstral is an agentic LLM for software engineering tasks. \nDevstral 2\n models excel at using tools to explore codebases, editing multiple files and power software engineering agents.\n\nThe model achieves remarkable performance on SWE-bench.\n\n\n24B model\n\n\nollama run devstral-small-2\n\n\n\nKey Features\n\n\nThe Devstral 2 Instruct model offers the following capabilities:\n\n\n\n\nAgentic Coding\n: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n\n\nImproved Performance\n: Devstral 2 is a step-up compared to its predecessors.\n\n\nBetter Generalization\n: Generalises better to diverse prompts and coding environments.\n\n\n\n\nUse Cases\n\n\nAI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments.\n\n\nBenchmark Results\n\n\n\n\n\n\n\n\nModel/Benchmark\n\n\nSize (B Tokens)\n\n\nSWE Bench Verified\n\n\nSWE Bench Multilingual\n\n\nTerminal Bench\n\n\n\n\n\n\n\n\n\n\nDevstral 2\n\n\n123\n\n\n72.2%\n\n\n61.3%\n\n\n40.5%\n\n\n\n\n\n\nDevstral Small 2\n\n\n24\n\n\n65.8%\n\n\n51.6%\n\n\n32.0%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek v3.2\n\n\n671\n\n\n73.1%\n\n\n70.2%\n\n\n46.4%\n\n\n\n\n\n\nKimi K2 Thinking\n\n\n1000\n\n\n71.3%\n\n\n61.1%\n\n\n35.7%\n\n\n\n\n\n\nMiniMax M2\n\n\n230\n\n\n69.4%\n\n\n56.5%\n\n\n30.0%\n\n\n\n\n\n\nGLM 4.6\n\n\n455\n\n\n68.0%\n\n\n–\n\n\n40.5%\n\n\n\n\n\n\nQwen 3 Coder Plus\n\n\n480\n\n\n69.6%\n\n\n54.7%\n\n\n37.5%\n\n\n\n\n\n\nGemini 3 Pro\n\n\n–\n\n\n76.2%\n\n\n–\n\n\n54.2%\n\n\n\n\n\n\nClaude Sonnet 4.5\n\n\n–\n\n\n77.2%\n\n\n68.0%\n\n\n42.8%\n\n\n\n\n\n\nGPT 5.1 Codex Max\n\n\n–\n\n\n77.9%\n\n\n–\n\n\n58.1%\n\n\n\n\n\n\nGPT 5.1 Codex High\n\n\n–\n\n\n73.7%\n\n\n–\n\n\n52.8%\n\n\n\n\n\n\n\n\nLicense\n\n\nDevstral Small 2 - 24B\n\n\nApache 2.0\n\n\nReference\n\n\nDevstral 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/devstral-2/22065d6d-626a-4fc8-af4c-2efe10844651\" width=\"72\" />\n\n> Note: this model requires Ollama 0.13.3 or later. [Download Ollama](https://ollama.com/download)\n\n# Devstral Small 2 \nDevstral is an agentic LLM for software engineering tasks. **Devstral 2** models excel at using tools to explore codebases, editing multiple files and power software engineering agents.  \nThe model achieves remarkable performance on SWE-bench. \n\n**[24B model](https://ollama.com/library/devstral-small-2)** \n\n```\nollama run devstral-small-2\n```\n\n### Key Features\n\nThe Devstral 2 Instruct model offers the following capabilities:\n\n- **Agentic Coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n\n- **Improved Performance**: Devstral 2 is a step-up compared to its predecessors.\n\n- **Better Generalization**: Generalises better to diverse prompts and coding environments.\n\n### Use Cases\n\nAI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments.\n\n### Benchmark Results\n\n| Model/Benchmark               | Size (B Tokens) | SWE Bench Verified | SWE Bench Multilingual | Terminal Bench |\n|-------------------------------|-----------------|--------------------|------------------------|----------------|\n| **Devstral 2**                | 123             | 72.2%              | 61.3%                  | 40.5%          |\n| **Devstral Small 2**          | 24              | 65.8%              | 51.6%                  | 32.0%          |\n|                               |                 |                    |                        |                |\n| DeepSeek v3.2                 | 671             | 73.1%              | 70.2%                  | 46.4%          |\n| Kimi K2 Thinking              | 1000            | 71.3%              | 61.1%                  | 35.7%          |\n| MiniMax M2                    | 230             | 69.4%              | 56.5%                  | 30.0%          |\n| GLM 4.6                       | 455             | 68.0%              | --                     | 40.5%          |\n| Qwen 3 Coder Plus             | 480             | 69.6%              | 54.7%                  | 37.5%          |\n| Gemini 3 Pro                  | --              | 76.2%              | --                     | 54.2%          |\n| Claude Sonnet 4.5             | --              | 77.2%              | 68.0%                  | 42.8%          |\n| GPT 5.1 Codex Max             | --              | 77.9%              | --                     | 58.1%          |\n| GPT 5.1 Codex High            | --              | 73.7%              | --                     | 52.8%          |\n\n\n### License \n\n**[Devstral Small 2 - 24B](https://ollama.com/library/devstral-small-2)**\n\nApache 2.0\n\n### Reference\n\n[Devstral 2](https://ollama.com/library/devstral-2)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "devstral-small-2:latest",
        "size": "15GB",
        "size_gb": 15.0,
        "recommended_ram_gb": 18.8,
        "quantization": "q4_k_m",
        "context": "384K context",
        "context_window": 384000
      },
      {
        "tag": "latest",
        "size": "15GB",
        "size_gb": 15.0,
        "recommended_ram_gb": 18.8,
        "quantization": "q4_k_m",
        "context": "384K",
        "context_window": 384000
      }
    ],
    "min_ram_gb": 15.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Software Engineering Agents and Code Exploration Tasks",
    "pulls": 158800,
    "tags": 6,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "126c0971-9522-4be3-9703-55d0c4d3b72c",
    "model_identifier": "smallthinker",
    "model_name": "smallthinker",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/smallthinker",
    "description": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nA new model fine-tuned from the Qwen2.5-3b-Instruct model.\n\n\nSmallThinker is designed for the following use cases:\n\n\n\n\nEdge Deployment: Its small size makes it ideal for deployment on resource-constrained devices.\n\n\nDraft Model for QwQ-32B-Preview: SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model, yielding a 70% speedup.\n\n\n\n\nFor achieving reasoning capabilities, it’s crucial to generate long chains of COT reasoning. Therefore, based on QWQ-32B-Preview, the authors used various synthetic techniques(such as personahub) to create the QWQ-LONGCOT-500K dataset. Compared to other similar datasets, over 75% of the author’s samples have output tokens exceeding 8K. To encourage research in the open-source community, the dataset was also made publicly available.\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/smallthinker/1d25cb29-e27d-492c-be53-ce79b20def5b)\n\nA new model fine-tuned from the Qwen2.5-3b-Instruct model.\n\nSmallThinker is designed for the following use cases:\n\n* Edge Deployment: Its small size makes it ideal for deployment on resource-constrained devices.\n* Draft Model for QwQ-32B-Preview: SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model, yielding a 70% speedup.\n\nFor achieving reasoning capabilities, it's crucial to generate long chains of COT reasoning. Therefore, based on QWQ-32B-Preview, the authors used various synthetic techniques(such as personahub) to create the QWQ-LONGCOT-500K dataset. Compared to other similar datasets, over 75% of the author's samples have output tokens exceeding 8K. To encourage research in the open-source community, the dataset was also made publicly available.\n\n## References\n\n[Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-3B-Preview)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "smallthinker:latest",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "3.6GB",
        "size_gb": 3.6,
        "recommended_ram_gb": 4.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 3.6,
    "use_cases": [
      "Reasoning"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Ideal for resource-constrained devices and as a draft model for larger models.",
    "pulls": 155100,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "566929f8-0e9b-46bf-93e3-e326ea47d247",
    "model_identifier": "command-r7b",
    "model_name": "command-r7b",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/command-r7b",
    "description": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nC4AI Command R7B is an open weights research release of a 7B billion parameter model with advanced capabilities optimized for a variety of use cases including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities with the ability to use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.\n\n\nModel Details\n\n\nModel Architecture:\n This is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety. The model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n\n\nLanguages covered:\n The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![r7b.jpg](/assets/library/command-r7b/709a1bd1-3f55-4e90-b3ca-40db45e13683)\n\nC4AI Command R7B is an open weights research release of a 7B billion parameter model with advanced capabilities optimized for a variety of use cases including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities with the ability to use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.\n\n### Model Details\n\n**Model Architecture:** This is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety. The model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n\n**Languages covered:** The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "command-r7b:latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 5.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for building AI applications.",
    "pulls": 154300,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "2fe26565-4537-4317-b634-a0d84dc82197",
    "model_identifier": "phi4-mini-reasoning",
    "model_name": "phi4-mini-reasoning",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi4-mini-reasoning",
    "description": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ability.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nPhi 4 mini reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.\n\n\n\n\nThe graph compares the performance of various models on popular math benchmarks for long sentence generation. Phi-4-mini-reasoning outperforms its base model on long sentence generation across each evaluation, as well as larger models like OpenThinker-7B, Llama-3.2-3B-instruct, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, and Bespoke-Stratos-7B. Phi-4-mini-reasoning is comparable to OpenAI o1-mini across math benchmarks, surpassing the model’s performance during Math-500 and GPQA Diamond evaluations. As seen above, Phi-4-mini-reasoning with 3.8B parameters outperforms models of over twice its size. \n\n\nReferences\n\n\nBlog post \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhi 4 mini reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.\n\n![image.png](/assets/library/phi4-mini-reasoning/986a6e3b-daf1-48b6-97cc-2177ea38a598)\n<small>The graph compares the performance of various models on popular math benchmarks for long sentence generation. Phi-4-mini-reasoning outperforms its base model on long sentence generation across each evaluation, as well as larger models like OpenThinker-7B, Llama-3.2-3B-instruct, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, and Bespoke-Stratos-7B. Phi-4-mini-reasoning is comparable to OpenAI o1-mini across math benchmarks, surpassing the model’s performance during Math-500 and GPQA Diamond evaluations. As seen above, Phi-4-mini-reasoning with 3.8B parameters outperforms models of over twice its size. </small>\n\n## References\n\n[Blog post ](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3.8b"
    ],
    "memory_requirements": [
      {
        "tag": "phi4-mini-reasoning:latest",
        "size": "3.2GB",
        "size_gb": 3.2,
        "recommended_ram_gb": 4.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "3.2GB",
        "size_gb": 3.2,
        "recommended_ram_gb": 4.0,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 3.2,
    "use_cases": [
      "Reasoning",
      "Math"
    ],
    "domain": "Math",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Formal proof generation and advanced mathematical problem-solving tasks.",
    "pulls": 152500,
    "tags": 5,
    "last_updated": "2025-04-25",
    "last_updated_str": "10 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "a50f9bdb-7ec1-445b-9688-614ecf26449d",
    "model_identifier": "deepseek-v2.5",
    "model_name": "deepseek-v2.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-v2.5",
    "description": "An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.\n\n\nDeepSeek-V2.5 better aligns with human preferences and has been optimized in various aspects, including writing and instruction following:\n\n\n\n\n\n\n\n\nMetric\n\n\nDeepSeek-V2-0628\n\n\nDeepSeek-Coder-V2-0724\n\n\nDeepSeek-V2.5\n\n\n\n\n\n\n\n\n\n\nAlpacaEval 2.0\n\n\n46.6\n\n\n44.5\n\n\n50.5\n\n\n\n\n\n\nArenaHard\n\n\n68.3\n\n\n66.3\n\n\n76.2\n\n\n\n\n\n\nAlignBench\n\n\n7.88\n\n\n7.91\n\n\n8.04\n\n\n\n\n\n\nMT-Bench\n\n\n8.85\n\n\n8.91\n\n\n9.02\n\n\n\n\n\n\nHumanEval python\n\n\n84.5\n\n\n87.2\n\n\n89\n\n\n\n\n\n\nHumanEval Multi\n\n\n73.8\n\n\n74.8\n\n\n73.8\n\n\n\n\n\n\nLiveCodeBench(01-09)\n\n\n36.6\n\n\n39.7\n\n\n41.8\n\n\n\n\n\n\nAider\n\n\n69.9\n\n\n72.9\n\n\n72.2\n\n\n\n\n\n\nSWE-verified\n\n\nN/A\n\n\n19\n\n\n16.8\n\n\n\n\n\n\nDS-FIM-Eval\n\n\nN/A\n\n\n73.2\n\n\n78.3\n\n\n\n\n\n\nDS-Arena-Code\n\n\nN/A\n\n\n49.5\n\n\n63.1\n\n\n\n\n\n\n\n\nReference\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/deepseek-v2.5/4d4970a3-b9bd-430c-9ee8-d01acef3da1b\" width=\"320\" />\n\nDeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.\n\nDeepSeek-V2.5 better aligns with human preferences and has been optimized in various aspects, including writing and instruction following:\n\n| Metric                 | DeepSeek-V2-0628 | DeepSeek-Coder-V2-0724 | DeepSeek-V2.5 |\n|:-----------------------|:-----------------|:-----------------------|:--------------|\n| AlpacaEval 2.0          | 46.6             | 44.5                   | 50.5          |\n| ArenaHard              | 68.3             | 66.3                   | 76.2          |\n| AlignBench             | 7.88             | 7.91                   | 8.04          |\n| MT-Bench               | 8.85             | 8.91                   | 9.02          |\n| HumanEval python       | 84.5             | 87.2                   | 89            |\n| HumanEval Multi        | 73.8             | 74.8                   | 73.8          |\n| LiveCodeBench(01-09)   | 36.6             | 39.7                   | 41.8          |\n| Aider                  | 69.9             | 72.9                   | 72.2          |\n| SWE-verified           | N/A              | 19                     | 16.8          |\n| DS-FIM-Eval            | N/A              | 73.2                   | 78.3          |\n| DS-Arena-Code          | N/A              | 49.5                   | 63.1          |\n\n## Reference\n\n[Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "236b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-v2.5:latest",
        "size": "133GB",
        "size_gb": 133.0,
        "recommended_ram_gb": 166.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "133GB",
        "size_gb": 133.0,
        "recommended_ram_gb": 166.2,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 133.0,
    "use_cases": [
      "Code Generation",
      "Question Answering",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": null,
    "best_for": "Developers and researchers who need a model with both general and coding abilities.",
    "pulls": 152400,
    "tags": 7,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "ba9e598b-927e-40a6-9874-47e3851961c5",
    "model_identifier": "granite3-guardian",
    "model_name": "granite3-guardian",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/granite3-guardian",
    "description": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nGranite guardian models\n\n\nThe IBM Granite Guardian 3.0 \n2B and 8B models\n are designed to detect risks in prompts and/or responses. They can help with risk detection along many key dimensions catalogued in the \nIBM AI Risk Atlas\n. They are trained on unique data comprising human annotations and synthetic data informed by internal red-teaming, and they outperform other open-source models in the same space on standard benchmarks.\n\n\nParameter Sizes\n\n\nThe model will produce a single output token, either \nYes\n or \nNo\n. By default, the general-purpose \nharm\n category is used, but other categories can be selected by setting the system prompt.\n\n\n2B:\n\n\nollama run granite3-guardian:2b\n>>> /set system profanity\n\n\n\n8B:\n\n\nollama run granite3-guardian:8b\n>>> /set system violence\n\n\n\nSupported Uses\n\n\n\n\nRisk detection in prompt text or model response (i.e. as guardrails), such as:\n\n\n\n\nHarm (\nharm\n): content considered generally harmful\n\n\nSocial Bias (\nsocial_bias\n): prejudice based on identity or characteristics\n\n\nJailbreaking (\njailbreak\n): deliberate instances of manipulating AI to generate harmful, undesired, or inappropriate content\n\n\nViolence (\nviolence\n): content promoting physical, mental, or sexual harm\n\n\nProfanity (\nprofanity\n): use of offensive language or insults\n\n\nSexual Content (\nsexual_content\n): explicit or suggestive material of a sexual nature\n\n\nUnethical Behavior (\nunethical_behavior\n): actions that violate moral or legal standards\n\n\n\n\nRAG (retrieval-augmented generation) to assess:\n\n\n\n\nContext relevance (\nrelevance\n): whether the retrieved context is relevant to the query\n\n\nGroundedness (\ngroundedness\n): whether the response is accurate and faithful to the provided context\n\n\nAnswer relevance (\nanswer_relevance\n): whether the response directly addresses the user’s query\n\n\n\n\n\n\nGranite dense models\n\n\nThe Granite dense models are available in \n2B and 8B\n parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n\nSee model page\n\n\nGranite mixture of experts models\n\n\nThe Granite MoE models are available in \n1B and 3B\n parameter sizes designed for low latency usage and to support deployment in on-device applications or situations requiring instantaneous inference.\n\n\nSee model page\n\n\nLearn more\n\n\n\n\nDevelopers:\n IBM Research\n\n\nGitHub Repository:\n \nibm-granite/granite-guardian\n\n\nWebsite\n: \nGranite Guardian Docs\n\n\nCookbook\n: \nGranite Guardian Snack\n\n\nRelease Date\n: October 21st, 2024\n\n\nLicense:\n \nApache 2.0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Granite guardian models\n\nThe IBM Granite Guardian 3.0 **2B and 8B models** are designed to detect risks in prompts and/or responses. They can help with risk detection along many key dimensions catalogued in the [IBM AI Risk Atlas](https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas). They are trained on unique data comprising human annotations and synthetic data informed by internal red-teaming, and they outperform other open-source models in the same space on standard benchmarks.\n\n### Parameter Sizes\n\nThe model will produce a single output token, either `Yes` or `No`. By default, the general-purpose `harm` category is used, but other categories can be selected by setting the system prompt.\n\n**2B:**\n  \n```\nollama run granite3-guardian:2b\n>>> /set system profanity\n```\n\n**8B:**\n\n```\nollama run granite3-guardian:8b\n>>> /set system violence\n```\n\n### Supported Uses\n\n* Risk detection in prompt text or model response (i.e. as guardrails), such as:\n    * Harm (`harm`): content considered generally harmful\n    * Social Bias (`social_bias`): prejudice based on identity or characteristics\n    * Jailbreaking (`jailbreak`): deliberate instances of manipulating AI to generate harmful, undesired, or inappropriate content\n    * Violence (`violence`): content promoting physical, mental, or sexual harm\n    * Profanity (`profanity`): use of offensive language or insults\n    * Sexual Content (`sexual_content`): explicit or suggestive material of a sexual nature\n    * Unethical Behavior (`unethical_behavior`): actions that violate moral or legal standards\n\n\n* RAG (retrieval-augmented generation) to assess: \n    * Context relevance (`relevance`): whether the retrieved context is relevant to the query \n    * Groundedness (`groundedness`): whether the response is accurate and faithful to the provided context\n    * Answer relevance (`answer_relevance`): whether the response directly addresses the user's query\n\n## Granite dense models\n\nThe Granite dense models are available in **2B and 8B** parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n\n[See model page](https://ollama.com/library/granite3-dense) \n\n## Granite mixture of experts models\n\nThe Granite MoE models are available in **1B and 3B** parameter sizes designed for low latency usage and to support deployment in on-device applications or situations requiring instantaneous inference.\n\n[See model page](https://ollama.com/library/granite3-moe) \n\n## Learn more\n\n- **Developers:** IBM Research\n- **GitHub Repository:** [ibm-granite/granite-guardian](https://github.com/ibm-granite/granite-guardian)\n- **Website**: [Granite Guardian Docs](https://www.ibm.com/granite/docs/models/guardian/)\n- **Cookbook**: [Granite Guardian Snack](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/Granite_Guardian/Granite_Guardian_Detailed_Guide.ipynb)\n- **Release Date**: October 21st, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "granite3-guardian:latest",
        "size": "2.7GB",
        "size_gb": 2.7,
        "recommended_ram_gb": 3.4,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "2.7GB",
        "size_gb": 2.7,
        "recommended_ram_gb": 3.4,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "granite3-guardian:8b",
        "size": "5.8GB",
        "size_gb": 5.8,
        "recommended_ram_gb": 7.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 2.7,
    "use_cases": [],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": null,
    "best_for": "Risk detection in prompts and responses",
    "pulls": 145500,
    "tags": 10,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "94e1b4f9-07a0-4059-a58c-a0a4ec6033a6",
    "model_identifier": "command-a",
    "model_name": "command-a",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/command-a",
    "description": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCommand A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks while‬ being deployable on just two GPUs.\n\n\nLanguages covered:\n The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n\nContext Window:\n Up to 256K.\n\n\nUse cases\n\n\nCommand A is designed with the following capabilities.\n\n\nChat\n\n\nBy default, Command A is configured as a conversational model. A preamble conditions the model on interactive behaviour, meaning it is expected to reply in a conversational fashion, provides introductory statements and follow-up questions, and uses Markdown as well as LaTeX where appropriate. This is desired for interactive experiences, such as chatbots, where the model engages in dialogue.\n\n\nRetrieval augmented generation (RAG)\n\n\nCommand A has been trained specifically for tasks like the final step of Retrieval Augmented Generation (RAG).\n\n\nTool Support\n\n\nCommand A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines.\n\n\nCode\n\n\nCommand A has meaningfully improved on code capabilities.  In addition to academic code benchmarks, we have evaluated it on enterprise-relevant scenarios, including SQL generation and code translation, where it outperforms other models of similar size. Try these out by requesting code snippets, code explanations, or code rewrites. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Hero-Image.webp](/assets/library/command-a/5849b9b2-34cd-4827-92d4-97613dc8a5e5)\n\nCommand A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks while‬ being deployable on just two GPUs. \n\n**Languages covered:** The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n**Context Window:** Up to 256K.\n\n### Use cases \n\nCommand A is designed with the following capabilities.    \n\n#### Chat\n\nBy default, Command A is configured as a conversational model. A preamble conditions the model on interactive behaviour, meaning it is expected to reply in a conversational fashion, provides introductory statements and follow-up questions, and uses Markdown as well as LaTeX where appropriate. This is desired for interactive experiences, such as chatbots, where the model engages in dialogue. \n\n#### Retrieval augmented generation (RAG)\n\nCommand A has been trained specifically for tasks like the final step of Retrieval Augmented Generation (RAG). \n\n#### Tool Support \n\nCommand A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines.\n\n#### Code \n\nCommand A has meaningfully improved on code capabilities.  In addition to academic code benchmarks, we have evaluated it on enterprise-relevant scenarios, including SQL generation and code translation, where it outperforms other models of similar size. Try these out by requesting code snippets, code explanations, or code rewrites. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "111b"
    ],
    "memory_requirements": [
      {
        "tag": "command-a:latest",
        "size": "67GB",
        "size_gb": 67.0,
        "recommended_ram_gb": 83.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "67GB",
        "size_gb": 67.0,
        "recommended_ram_gb": 83.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 67.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for demanding enterprise AI tasks.",
    "pulls": 129800,
    "tags": 5,
    "last_updated": "2025-03-25",
    "last_updated_str": "11 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "b8324120-7360-4990-9353-e1385122c818",
    "model_identifier": "marco-o1",
    "model_name": "marco-o1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/marco-o1",
    "description": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tuning with CoT Data:\n We develop \nMarco-o1-CoT\n by performing full-parameter fine-tuning on the base model using open-source CoT dataset combined with our self-developed synthetic data.\n\n\nSolution Space Expansion via MCTS:\n We integrate LLMs with MCTS (\nMarco-o1-MCTS\n), using the model’s output confidence to guide the search and expand the solution space.\n\n\nReasoning Action Strategy:\n We implement novel reasoning action strategies and a reflection mechanism (\nMarco-o1-MCTS mini-step\n), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the model’s ability to solve complex problems.\n\n\nApplication in Translation Tasks:\n We are the first to apply Large Reasoning Models (LRM) to \nMachine Translation task\n, exploring inference time scaling laws in the multilingual and translation domain.\n\n\n\n\nUsage\n\n\nollama run marco-o1 \"How many Rs are in strawberry?\"\n\n\n\nParse the resulting string between \n<Output>\n and \n</Output>\n:\n\n\n...\n<Output>\nThere are 3 Rs in strawberry.\n</Output>\n\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/marco-o1/93bea833-d0c0-48b6-8ece-9c2de26cba27\" width=\"200\" />\n\n* **Fine-Tuning with CoT Data:** We develop <ins>Marco-o1-CoT</ins> by performing full-parameter fine-tuning on the base model using open-source CoT dataset combined with our self-developed synthetic data. \n* **Solution Space Expansion via MCTS:** We integrate LLMs with MCTS (<ins>Marco-o1-MCTS</ins>), using the model's output confidence to guide the search and expand the solution space. \n* **Reasoning Action Strategy:** We implement novel reasoning action strategies and a reflection mechanism (<ins>Marco-o1-MCTS mini-step</ins>), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the model's ability to solve complex problems.\n* **Application in Translation Tasks:** We are the first to apply Large Reasoning Models (LRM) to <ins>Machine Translation task</ins>, exploring inference time scaling laws in the multilingual and translation domain.\n\n## Usage\n\n```\nollama run marco-o1 \"How many Rs are in strawberry?\"\n```\n\nParse the resulting string between `<Output>` and `</Output>`:\n\n```\n...\n<Output>\nThere are 3 Rs in strawberry.\n</Output>\n```\n\n\n\n## References\n\n[GitHub](https://github.com/AIDC-AI/Marco-o1?tab=readme-ov-file)\n\n[HuggingFace](https://huggingface.co/AIDC-AI/Marco-o1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "marco-o1:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.7,
    "use_cases": [
      "Reasoning",
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for complex problem solving and text summarization tasks.",
    "pulls": 118300,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "481f782b-b2b3-4383-99e7-9a823feccda5",
    "model_identifier": "alfred",
    "model_name": "alfred",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/alfred",
    "description": "A robust conversational model designed to be used for both chat and instruct use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nAlfred is a robust conversational model created by LightOn, based on the \nFalcon\n 40B model. It’s designed to be used for both chat and instruct use cases.\n\n\nSome of its features include:\n\n\n\n\nReduced Hallucinations: ability to minimize hallucinations, ensuring more accurate and reliable outputs.\n\n\nEnhanced Self-Awareness: In situations where the model lacks a definitive answer, Alfred-40B-1023 is now programmed to state, “I don’t know”, enhancing its transparency and trustworthiness.\n\n\nSuperior ‘Chat with Docs’ Capability: Alfred-40B-1023 is trained to perform ‘Chat with Docs’ tasks like no other, streamlining document interaction and information retrieval.\n\n\nExpanded Context: With an increased context of 8K tokens, Alfred-40B-1023 can comprehend and generate longer and more intricate content, ensuring detailed and comprehensive responses.\n\n\n\n\nReferences\n\n\nHuggingFace\n\n\nBlog Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/a689335b-a2a1-4be9-a132-aaaf7171e6b8\" width=\"300\">\n\nAlfred is a robust conversational model created by LightOn, based on the [Falcon](https://ollama.ai/library/falcon) 40B model. It's designed to be used for both chat and instruct use cases.\n\nSome of its features include:\n\n* Reduced Hallucinations: ability to minimize hallucinations, ensuring more accurate and reliable outputs.\n* Enhanced Self-Awareness: In situations where the model lacks a definitive answer, Alfred-40B-1023 is now programmed to state, \"I don't know\", enhancing its transparency and trustworthiness.\n* Superior 'Chat with Docs' Capability: Alfred-40B-1023 is trained to perform 'Chat with Docs' tasks like no other, streamlining document interaction and information retrieval.\n* Expanded Context: With an increased context of 8K tokens, Alfred-40B-1023 can comprehend and generate longer and more intricate content, ensuring detailed and comprehensive responses.\n\n\n## References\n\n[HuggingFace](https://huggingface.co/lightonai/alfred-40b-1023)\n\n[Blog Post](https://www.lighton.ai/blog/lighton-s-blog-4/alfred-40b-1023-44)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "40b"
    ],
    "memory_requirements": [
      {
        "tag": "alfred:latest",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      },
      {
        "tag": "latest",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q4_k_m",
        "context": "2K",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 24.0,
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 109500,
    "tags": 7,
    "last_updated": "2024-02-25",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f56804eb-4f78-4bb6-be78-d4475c789ab6",
    "model_identifier": "olmo-3.1",
    "model_name": "olmo-3.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/olmo-3.1",
    "description": "Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOlmo 3.1 models are available either as a 32B parameter thinking or instruct model. It has long chain-of-thought thinking to improve reasoning tasks like math and coding.\n\n\nOlmo is a series of Open language models designed to enable the science of language models.\nThese models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details.\n\n\nModels\n\n\nOlmo 3.1 Instruct 32B\n\n\nollama run olmo-3.1:32b-instruct\n\n\n\nOlmo 3.1 Think 32B\n\n\nollama run olmo-3.1:32b-think\n\n\n\nBenchmark\n\n\n\n\n\n\n\n\nBenchmark\n\n\nOlmo 3.1 32B Think\n\n\nOlmo 3 Think 32B\n\n\nQwen 3 32B\n\n\nQwen 3 VL 32B Thinking\n\n\nQwen 2.5 32B\n\n\nGemma 3 27B Instruct\n\n\nGemma 2 27B Instruct\n\n\nOlmo 2 32B Instruct\n\n\nDeepSeek-R1-Distill-Qwen-32B\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMATH\n\n\n96.2\n\n\n96.1\n\n\n95.4\n\n\n96.7\n\n\n80.2\n\n\n87.4\n\n\n51.5\n\n\n49.2\n\n\n92.6\n\n\n\n\n\n\nAIME 2024\n\n\n80.6\n\n\n76.8\n\n\n80.8\n\n\n86.3\n\n\n15.7\n\n\n28.9\n\n\n4.7\n\n\n4.6\n\n\n70.3\n\n\n\n\n\n\nAIME 2025\n\n\n78.1\n\n\n72.5\n\n\n70.9\n\n\n78.8\n\n\n13.4\n\n\n22.9\n\n\n0.9\n\n\n0.9\n\n\n56.3\n\n\n\n\n\n\nOMEGA\n\n\n53.4\n\n\n50.8\n\n\n47.7\n\n\n50.8\n\n\n19.2\n\n\n24.0\n\n\n9.1\n\n\n9.8\n\n\n38.9\n\n\n\n\n\n\nReasoning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBigBenchHard\n\n\n88.6\n\n\n89.8\n\n\n90.6\n\n\n91.1\n\n\n80.9\n\n\n82.4\n\n\n66.0\n\n\n65.6\n\n\n89.7\n\n\n\n\n\n\nZebraLogic\n\n\n80.1\n\n\n76.0\n\n\n88.3\n\n\n96.1\n\n\n24.1\n\n\n24.8\n\n\n17.2\n\n\n13.3\n\n\n69.4\n\n\n\n\n\n\nAGI Eval English\n\n\n89.2\n\n\n88.2\n\n\n90.0\n\n\n92.2\n\n\n78.9\n\n\n76.9\n\n\n70.9\n\n\n68.4\n\n\n88.1\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHumanEvalPlus\n\n\n91.5\n\n\n91.4\n\n\n91.2\n\n\n90.6\n\n\n82.6\n\n\n79.2\n\n\n67.5\n\n\n44.4\n\n\n92.3\n\n\n\n\n\n\nMBPP+\n\n\n68.3\n\n\n68.0\n\n\n70.6\n\n\n66.2\n\n\n66.6\n\n\n65.7\n\n\n61.2\n\n\n49.0\n\n\n70.1\n\n\n\n\n\n\nLiveCodeBench v3\n\n\n83.3\n\n\n83.5\n\n\n90.2\n\n\n84.8\n\n\n49.9\n\n\n39.0\n\n\n28.7\n\n\n10.6\n\n\n79.5\n\n\n\n\n\n\nIF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIFEval\n\n\n93.8\n\n\n89.0\n\n\n86.5\n\n\n85.5\n\n\n81.9\n\n\n85.4\n\n\n62.1\n\n\n85.8\n\n\n78.7\n\n\n\n\n\n\nIFBench\n\n\n68.1\n\n\n47.6\n\n\n37.3\n\n\n55.1\n\n\n36.7\n\n\n31.3\n\n\n27.8\n\n\n36.4\n\n\n23.8\n\n\n\n\n\n\nKnowledge & QA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU\n\n\n86.4\n\n\n85.4\n\n\n88.8\n\n\n90.1\n\n\n84.6\n\n\n74.6\n\n\n76.1\n\n\n77.1\n\n\n88.0\n\n\n\n\n\n\nPopQA\n\n\n30.9\n\n\n31.9\n\n\n30.7\n\n\n32.2\n\n\n28.0\n\n\n30.2\n\n\n30.4\n\n\n37.2\n\n\n26.7\n\n\n\n\n\n\nGPQA\n\n\n57.5\n\n\n58.1\n\n\n67.3\n\n\n67.4\n\n\n44.6\n\n\n45.0\n\n\n39.9\n\n\n36.4\n\n\n61.8\n\n\n\n\n\n\nChat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpacaEval 2 LC\n\n\n69.1\n\n\n74.2\n\n\n75.6\n\n\n80.9\n\n\n81.9\n\n\n65.5\n\n\n39.8\n\n\n38.0\n\n\n26.2\n\n\n\n\n\n\nSafety\n\n\n83.6\n\n\n68.8\n\n\n69.0\n\n\n82.7\n\n\n81.9\n\n\n68.6\n\n\n74.3\n\n\n83.8\n\n\n63.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Olmo3.png](/assets/library/olmo-3.1/8d27e58b-a05f-4ffd-94d2-cd61a48e7303)\n\nOlmo 3.1 models are available either as a 32B parameter thinking or instruct model. It has long chain-of-thought thinking to improve reasoning tasks like math and coding.\n\nOlmo is a series of Open language models designed to enable the science of language models. \nThese models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details.  \n\n### Models \n\n**Olmo 3.1 Instruct 32B** \n\n```\nollama run olmo-3.1:32b-instruct\n```\n\n**Olmo 3.1 Think 32B** \n\n```\nollama run olmo-3.1:32b-think\n```\n\n### Benchmark\n\n| Benchmark | Olmo 3.1 32B Think | Olmo 3 Think 32B | Qwen 3 32B | Qwen 3 VL 32B Thinking | Qwen 2.5 32B | Gemma 3 27B Instruct | Gemma 2 27B Instruct | Olmo 2 32B Instruct | DeepSeek-R1-Distill-Qwen-32B |\n|-----------|---------------------:|-----------------:|-----------:|------------------------:|-------------:|----------------------:|----------------------:|---------------------:|----------------------------:|\n| **Math** | | | | | | | | | |\n| MATH | 96.2 | 96.1 | 95.4 | 96.7 | 80.2 | 87.4 | 51.5 | 49.2 | 92.6 |\n| AIME 2024 | 80.6 | 76.8 | 80.8 | 86.3 | 15.7 | 28.9 | 4.7 | 4.6 | 70.3 |\n| AIME 2025 | 78.1 | 72.5 | 70.9 | 78.8 | 13.4 | 22.9 | 0.9 | 0.9 | 56.3 |\n| OMEGA | 53.4 | 50.8 | 47.7 | 50.8 | 19.2 | 24.0 | 9.1 | 9.8 | 38.9 |\n| **Reasoning** | | | | | | | | | |\n| BigBenchHard | 88.6 | 89.8 | 90.6 | 91.1 | 80.9 | 82.4 | 66.0 | 65.6 | 89.7 |\n| ZebraLogic | 80.1 | 76.0 | 88.3 | 96.1 | 24.1 | 24.8 | 17.2 | 13.3 | 69.4 |\n| AGI Eval English | 89.2 | 88.2 | 90.0 | 92.2 | 78.9 | 76.9 | 70.9 | 68.4 | 88.1 |\n| **Coding** | | | | | | | | | |\n| HumanEvalPlus | 91.5 | 91.4 | 91.2 | 90.6 | 82.6 | 79.2 | 67.5 | 44.4 | 92.3 |\n| MBPP+ | 68.3 | 68.0 | 70.6 | 66.2 | 66.6 | 65.7 | 61.2 | 49.0 | 70.1 |\n| LiveCodeBench v3 | 83.3 | 83.5 | 90.2 | 84.8 | 49.9 | 39.0 | 28.7 | 10.6 | 79.5 |\n| **IF** | | | | | | | | | |\n| IFEval | 93.8 | 89.0 | 86.5 | 85.5 | 81.9 | 85.4 | 62.1 | 85.8 | 78.7 |\n| IFBench | 68.1 | 47.6 | 37.3 | 55.1 | 36.7 | 31.3 | 27.8 | 36.4 | 23.8 |\n| **Knowledge & QA** | | | | | | | | | |\n| MMLU | 86.4 | 85.4 | 88.8 | 90.1 | 84.6 | 74.6 | 76.1 | 77.1 | 88.0 |\n| PopQA | 30.9 | 31.9 | 30.7 | 32.2 | 28.0 | 30.2 | 30.4 | 37.2 | 26.7 |\n| GPQA | 57.5 | 58.1 | 67.3 | 67.4 | 44.6 | 45.0 | 39.9 | 36.4 | 61.8 |\n| **Chat** | | | | | | | | | |\n| AlpacaEval 2 LC | 69.1 | 74.2 | 75.6 | 80.9 | 81.9 | 65.5 | 39.8 | 38.0 | 26.2 |\n| **Safety** | 83.6 | 68.8 | 69.0 | 82.7 | 81.9 | 68.6 | 74.3 | 83.8 | 63.6 |\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "olmo-3.1:latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "64K context",
        "context_window": 64000
      },
      {
        "tag": "latest",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "64K",
        "context_window": 64000
      }
    ],
    "min_ram_gb": 19.0,
    "use_cases": [
      "Reasoning",
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 96700,
    "tags": 10,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "8ee809c5-3ae8-4e7c-b366-99f6e15fb373",
    "model_identifier": "devstral-2",
    "model_name": "devstral-2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/devstral-2",
    "description": "123B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDevstral 2\n\n\nDevstral is an agentic LLM for software engineering tasks. \nDevstral 2\n excels at using tools to explore codebases, editing multiple files and power software engineering agents.\n\nThe model achieves remarkable performance on SWE-bench.\n\n\n123B model\n\n\nollama run devstral-2\n\n\n\nOllama’s Cloud\n\n\nollama run devstral-2:123b-cloud\n\n\n\nKey Features\n\n\nThe Devstral 2 Instruct model offers the following capabilities:\n\n\n\n\nAgentic Coding\n: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n\n\nImproved Performance\n: Devstral 2 is a step-up compared to its predecessors.\n\n\nBetter Generalization\n: Generalises better to diverse prompts and coding environments.\n\n\n\n\nUse Cases\n\n\nAI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments.\n\n\nBenchmark Results\n\n\n\n\n\n\n\n\nModel/Benchmark\n\n\nSize (B Tokens)\n\n\nSWE Bench Verified\n\n\nSWE Bench Multilingual\n\n\nTerminal Bench\n\n\n\n\n\n\n\n\n\n\nDevstral 2\n\n\n123\n\n\n72.2%\n\n\n61.3%\n\n\n40.5%\n\n\n\n\n\n\nDevstral Small 2\n\n\n24\n\n\n65.8%\n\n\n51.6%\n\n\n32.0%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek v3.2\n\n\n671\n\n\n73.1%\n\n\n70.2%\n\n\n46.4%\n\n\n\n\n\n\nKimi K2 Thinking\n\n\n1000\n\n\n71.3%\n\n\n61.1%\n\n\n35.7%\n\n\n\n\n\n\nMiniMax M2\n\n\n230\n\n\n69.4%\n\n\n56.5%\n\n\n30.0%\n\n\n\n\n\n\nGLM 4.6\n\n\n455\n\n\n68.0%\n\n\n–\n\n\n40.5%\n\n\n\n\n\n\nQwen 3 Coder Plus\n\n\n480\n\n\n69.6%\n\n\n54.7%\n\n\n37.5%\n\n\n\n\n\n\nGemini 3 Pro\n\n\n–\n\n\n76.2%\n\n\n–\n\n\n54.2%\n\n\n\n\n\n\nClaude Sonnet 4.5\n\n\n–\n\n\n77.2%\n\n\n68.0%\n\n\n42.8%\n\n\n\n\n\n\nGPT 5.1 Codex Max\n\n\n–\n\n\n77.9%\n\n\n–\n\n\n58.1%\n\n\n\n\n\n\nGPT 5.1 Codex High\n\n\n–\n\n\n73.7%\n\n\n–\n\n\n52.8%\n\n\n\n\n\n\n\n\nLicense\n\n\nDevstral 2 - 123B\n\n\nModified MIT License\n\n\nAttribution notice: 2025 - Mistral AI\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of the weights of this model and associated documentation files (the “Model”), to deal in the Model without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Model, and to permit persons to whom the Model is furnished to do so, subject to the following conditions:\n\n\n\n\nThe above attribution notice and this permission notice shall be included in all copies or substantial portions of the Model.\n\n\nYou are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company (or that of your employer) exceeds $20 million (or its equivalent in another currency) for the preceding month. This restriction in (b) applies to the Model and any derivatives, modifications, or combined works based on it, whether provided by Mistral AI or by a third party. You may contact Mistral AI (sales@mistral.ai) to request a commercial license, which Mistral AI may grant you at its sole discretion, or choose to use the Model on Mistral AI’s hosted services available at \nhttps://mistral.ai/\n.\n\n\n\n\nTHE MODEL IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL MISTRAL AI BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.\n\n\nReference\n\n\nDevstral Small 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/devstral-2/22065d6d-626a-4fc8-af4c-2efe10844651\" width=\"72\" />\n\n# Devstral 2 \nDevstral is an agentic LLM for software engineering tasks. **Devstral 2** excels at using tools to explore codebases, editing multiple files and power software engineering agents.  \nThe model achieves remarkable performance on SWE-bench. \n\n**[123B model](https://ollama.com/library/devstral-2)**\n\n```\nollama run devstral-2\n```\n\nOllama's Cloud\n```\nollama run devstral-2:123b-cloud\n```\n\n### Key Features\n\nThe Devstral 2 Instruct model offers the following capabilities:\n\n- **Agentic Coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n\n- **Improved Performance**: Devstral 2 is a step-up compared to its predecessors.\n\n- **Better Generalization**: Generalises better to diverse prompts and coding environments.\n\n### Use Cases\n\nAI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments.\n\n### Benchmark Results\n\n| Model/Benchmark               | Size (B Tokens) | SWE Bench Verified | SWE Bench Multilingual | Terminal Bench |\n|-------------------------------|-----------------|--------------------|------------------------|----------------|\n| **Devstral 2**                | 123             | 72.2%              | 61.3%                  | 40.5%          |\n| **Devstral Small 2**          | 24              | 65.8%              | 51.6%                  | 32.0%          |\n|                               |                 |                    |                        |                |\n| DeepSeek v3.2                 | 671             | 73.1%              | 70.2%                  | 46.4%          |\n| Kimi K2 Thinking              | 1000            | 71.3%              | 61.1%                  | 35.7%          |\n| MiniMax M2                    | 230             | 69.4%              | 56.5%                  | 30.0%          |\n| GLM 4.6                       | 455             | 68.0%              | --                     | 40.5%          |\n| Qwen 3 Coder Plus             | 480             | 69.6%              | 54.7%                  | 37.5%          |\n| Gemini 3 Pro                  | --              | 76.2%              | --                     | 54.2%          |\n| Claude Sonnet 4.5             | --              | 77.2%              | 68.0%                  | 42.8%          |\n| GPT 5.1 Codex Max             | --              | 77.9%              | --                     | 58.1%          |\n| GPT 5.1 Codex High            | --              | 73.7%              | --                     | 52.8%          |\n\n\n### License \n\n**[Devstral 2 - 123B](https://ollama.com/library/devstral-2)** \n\nModified MIT License \n\nAttribution notice: 2025 - Mistral AI\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of the weights of this model and associated documentation files (the “Model”), to deal in the Model without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Model, and to permit persons to whom the Model is furnished to do so, subject to the following conditions: \n\n1. The above attribution notice and this permission notice shall be included in all copies or substantial portions of the Model. \n2. You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company (or that of your employer) exceeds $20 million (or its equivalent in another currency) for the preceding month. This restriction in (b) applies to the Model and any derivatives, modifications, or combined works based on it, whether provided by Mistral AI or by a third party. You may contact Mistral AI (sales@mistral.ai) to request a commercial license, which Mistral AI may grant you at its sole discretion, or choose to use the Model on Mistral AI's hosted services available at https://mistral.ai/.\n\nTHE MODEL IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL MISTRAL AI BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.\n\n### Reference \n\n[Devstral Small 2](https://ollama.com/library/devstral-small-2)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "123b"
    ],
    "memory_requirements": [
      {
        "tag": "devstral-2:latest",
        "size": "75GB",
        "size_gb": 75.0,
        "recommended_ram_gb": 93.8,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "latest",
        "size": "75GB",
        "size_gb": 75.0,
        "recommended_ram_gb": 93.8,
        "quantization": "q4_k_m",
        "context": "256K",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 75.0,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Software Engineering Agents and Code Analysis Tasks",
    "pulls": 89400,
    "tags": 6,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6a65d870-a69a-4d94-9671-ef955e341cf2",
    "model_identifier": "command-r7b-arabic",
    "model_name": "command-r7b-arabic",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/command-r7b-arabic",
    "description": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCommand R7B Arabic is a fast and highly efficient model that can be served on low-end GPUs, a MacBook, or even CPUs. Similar to other models in the R series, it offers a context length of 128k and industry-leading performance in its class across capabilities that matter most to businesses like regional language understanding and strong accuracy with citations using retrieval-augmented generation (RAG). Its compact size enables businesses to more easily scale Arabic language AI applications to production.\n\n\nReferences\n\n\nBlog Post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Hero--1-.webp](/assets/library/command-r7b-arabic/e96310ca-5a9a-4e86-8eda-5a0a68513fc6)\n\nCommand R7B Arabic is a fast and highly efficient model that can be served on low-end GPUs, a MacBook, or even CPUs. Similar to other models in the R series, it offers a context length of 128k and industry-leading performance in its class across capabilities that matter most to businesses like regional language understanding and strong accuracy with citations using retrieval-augmented generation (RAG). Its compact size enables businesses to more easily scale Arabic language AI applications to production. \n\n## References\n\n[Blog Post](https://cohere.com/blog/command-r7b-arabic)\n\n[Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "command-r7b-arabic:latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "5.1GB",
        "size_gb": 5.1,
        "recommended_ram_gb": 6.4,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 5.1,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "RAG / Retrieval",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "Arabic"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers working with Arabic language AI applications.",
    "pulls": 85900,
    "tags": 5,
    "last_updated": "2025-02-25",
    "last_updated_str": "12 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5fccfbdb-8585-4402-ba03-3f15db0f3e32",
    "model_identifier": "kimi-k2.5",
    "model_name": "kimi-k2.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/kimi-k2.5",
    "description": "Kimi K2.5 is an open-source, native multimodal agentic model that seamlessly integrates vision and language understanding with advanced agentic capabilities, instant and thinking modes, as well as conversational and agentic paradigms.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nKimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately 15 trillion mixed visual and text tokens atop Kimi-K2-Base. It seamlessly integrates vision and language understanding with advanced agentic capabilities, instant and thinking modes, as well as conversational and agentic paradigms.\n\n\nKey Features\n\n\n\n\nNative Multimodality\n: Pre-trained on vision–language tokens, K2.5 excels in visual knowledge, cross-modal reasoning, and agentic tool use grounded in visual inputs.\n\n\nCoding with Vision\n: K2.5 generates code from visual specifications (UI designs, video workflows) and autonomously orchestrates tools for visual data processing.\n\n\nAgent Swarm\n: K2.5 transitions from single-agent scaling to a self-directed, coordinated swarm-like execution scheme. It decomposes complex tasks into parallel sub-tasks executed by dynamically instantiated, domain-specific agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/kimi-k2.5/3e4026fd-dc73-4a59-9322-7f015a92925e\" width=\"180\" />\n\n\nKimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately 15 trillion mixed visual and text tokens atop Kimi-K2-Base. It seamlessly integrates vision and language understanding with advanced agentic capabilities, instant and thinking modes, as well as conversational and agentic paradigms.\n\n### Key Features\n- **Native Multimodality**: Pre-trained on vision–language tokens, K2.5 excels in visual knowledge, cross-modal reasoning, and agentic tool use grounded in visual inputs.\n- **Coding with Vision**: K2.5 generates code from visual specifications (UI designs, video workflows) and autonomously orchestrates tools for visual data processing.\n- **Agent Swarm**: K2.5 transitions from single-agent scaling to a self-directed, coordinated swarm-like execution scheme. It decomposes complex tasks into parallel sub-tasks executed by dynamically instantiated, domain-specific agents.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "vision",
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Image Understanding",
      "Reasoning",
      "Function Calling"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working with multimodal data.",
    "pulls": 86300,
    "tags": 0,
    "last_updated": "2026-01-28",
    "last_updated_str": "4 weeks ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "21ab736c-ec1b-4728-9ee9-4ff2446febd1",
    "model_identifier": "cogito-2.1",
    "model_name": "cogito-2.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/cogito-2.1",
    "description": "The Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\ncogito-2.1 models require \nOllama v0.13.0\n or later.\n\n\n\n\nThe Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.\n\n\n\n\nThe best open-weight LLM by a US company: Cogito v2.1 671B\n\n\nOn most industry benchmarks and our internal evals, the model performs competitively to frontier closed and open models, while being ahead of any other US open model.\n\n\nThis model uses significantly fewer tokens amongst any similar capability models, because it has better reasoning capabilities. It also has improvements across instruction following, coding, longer queries, multi-turn and creativity.\n\n\nThis model is trained in over 30 languages and supports a context length of 128k.\n\n\n\n\nModels\n\n\n671B\n\n\nollama run cogito-2.1\n\n\n\nOllama’s cloud\n\n\nollama run cogito-2.1:671b-cloud\n\n\n\nEvaluation\n\n\nNote from Cogito:\n\n\nWhile these benchmarks provide a useful signal, they do not fully capture real-world performance. That said, our models have been tested across multiple internal and external evaluations and consistently perform well.\n\n\nUltimately, the best evals are the ones closest to the user’s needs.\n\n\nWe are confident that our models will stand up to such real-world evaluations and deliver strong results in practice.\n\n\n\n\n\nCogito v2.1 model has the lowest average tokens3 used with respect to reasoning models of similar capabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/cogito/44ceefc5-6a71-4d18-958e-21d45d309b18\" width=\"320\" />\n\n> cogito-2.1 models require [Ollama v0.13.0](https://github.com/ollama/ollama/releases) or later.\n\nThe Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.\n\n- The best open-weight LLM by a US company: Cogito v2.1 671B\n\n- On most industry benchmarks and our internal evals, the model performs competitively to frontier closed and open models, while being ahead of any other US open model.\n\n- This model uses significantly fewer tokens amongst any similar capability models, because it has better reasoning capabilities. It also has improvements across instruction following, coding, longer queries, multi-turn and creativity.\n\n- This model is trained in over 30 languages and supports a context length of 128k.\n\n### Models\n\n**671B** \n```\nollama run cogito-2.1\n```\n\n**Ollama's cloud** \n```\nollama run cogito-2.1:671b-cloud\n```\n\n### Evaluation\n\n**Note from Cogito:**\n\nWhile these benchmarks provide a useful signal, they do not fully capture real-world performance. That said, our models have been tested across multiple internal and external evaluations and consistently perform well.\n\nUltimately, the best evals are the ones closest to the user's needs. \n\nWe are confident that our models will stand up to such real-world evaluations and deliver strong results in practice.\n![benchmark1](/assets/library/cogito-2.1/f2be7eb9-2f2d-43e3-b9eb-3ce07238389b)\n\n![benchmark2](/assets/library/cogito-2.1/8a4bed5c-2cb5-4b5a-8814-e5093676e476)\n\nCogito v2.1 model has the lowest average tokens3 used with respect to reasoning models of similar capabilities.\n\n![avgtokenusage](/assets/library/cogito-2.1/1e393452-5c47-4e82-a225-166bc0bf3629)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Cloud"
    ],
    "capability": "Cloud",
    "labels": [
      "671b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Code Generation",
      "Question Answering",
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 72900,
    "tags": 6,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "c11d65f6-4a0c-4908-b3e6-6378da0fbb0f",
    "model_identifier": "gpt-oss-safeguard",
    "model_name": "gpt-oss-safeguard",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gpt-oss-safeguard",
    "description": "gpt-oss-safeguard-20b and gpt-oss-safeguard-120b are safety reasoning models built-upon gpt-oss",
    "readme": "Readme\n\n\n\n\n\n\n\n\nGet started\n\n\n20B:\n\n\nollama run gpt-oss-safeguard:20b\n\n\n\nThis model is designed to fit into GPUs with 16GB of VRAM. (21B parameters with 3.6B active parameters).\n\n\n120B:\n\n\nollama run gpt-oss-safeguard:120b\n\n\n\nThis model is designed to fit into a single NVIDIA H100 GPU (117B parameters with 5.1B active parameters).\n\n\nHighlights\n\n\n\n\nTrained to reason about safety\n : Trained and tuned for safety reasoning to accommodate use cases like LLM input-output filtering, online content labeling and offline labeling for Trust and Safety use cases.\n\n\n\n\nBring your own policy:\n Interprets your written policy, so it generalizes across products and use cases with minimal engineering.\n\n\n\n\nReasoned decisions, not just scores:\n Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in policy decisions. Keep in mind Raw CoT is meant for developers and safety practitioners. It’s not intended for exposure to general users or use cases outside of safety contexts.\n\n\n\n\nConfigurable reasoning effort:\n Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n\n\n\n\nPermissive Apache 2.0 license:\n Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n\n\n\n\nJoin the ROOST Model Community\n\n\ngpt-oss-safeguard is a model partner of the \nRobust Open Online Safety Tools (ROOST)\n Model Community. The ROOST Model Community (RMC) is a group of safety practitioners exploring open source AI models to protect online spaces. As an RMC model partner, OpenAI is committed to incorporating user feedback and jointly iterating on future releases in pursuit of open safety. Visit the \nRMC GitHub repo\n to learn more about this partnership and how to get involved.\n\n\nReferences\n\n\n\n\nOpenAI blog\n\n\nOpenAI \ngpt-oss-safeguard developer cookbook\n\n\nROOST’s \nmodel community repository\n on GitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Get started \n\n**20B:**\n```\nollama run gpt-oss-safeguard:20b\n```\nThis model is designed to fit into GPUs with 16GB of VRAM. (21B parameters with 3.6B active parameters).\n\n**120B:**\n```\nollama run gpt-oss-safeguard:120b\n```\nThis model is designed to fit into a single NVIDIA H100 GPU (117B parameters with 5.1B active parameters).   \n\n### Highlights\n\n* **Trained to reason about safety** : Trained and tuned for safety reasoning to accommodate use cases like LLM input-output filtering, online content labeling and offline labeling for Trust and Safety use cases.  \n* **Bring your own policy:** Interprets your written policy, so it generalizes across products and use cases with minimal engineering.  \n* **Reasoned decisions, not just scores:** Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in policy decisions. Keep in mind Raw CoT is meant for developers and safety practitioners. It’s not intended for exposure to general users or use cases outside of safety contexts.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n\n### Join the ROOST Model Community\n\ngpt-oss-safeguard is a model partner of the [Robust Open Online Safety Tools (ROOST)](http://roost.tools/) Model Community. The ROOST Model Community (RMC) is a group of safety practitioners exploring open source AI models to protect online spaces. As an RMC model partner, OpenAI is committed to incorporating user feedback and jointly iterating on future releases in pursuit of open safety. Visit the [RMC GitHub repo](https://github.com/roostorg/open-models) to learn more about this partnership and how to get involved.  \n\n### References \n* [OpenAI blog](https://openai.com/index/introducing-gpt-oss-safeguard/)\n* OpenAI [gpt-oss-safeguard developer cookbook](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide)\n* ROOST’s [model community repository](https://github.com/roostorg/open-models) on GitHub\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "20b",
      "120b"
    ],
    "memory_requirements": [
      {
        "tag": "gpt-oss-safeguard:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "gpt-oss-safeguard:120b",
        "size": "65GB",
        "size_gb": 65.0,
        "recommended_ram_gb": 81.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 14.0,
    "use_cases": [
      "Reasoning",
      "Translation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for safety reasoning tasks.",
    "pulls": 72300,
    "tags": 3,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1a59dcf0-9cd5-4ddc-8ee4-1ca333a06e67",
    "model_identifier": "functiongemma",
    "model_name": "functiongemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/functiongemma",
    "description": "FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nRequires Ollama v0.13.5 or later\n\n\n\n\nFunctionGemma\n\n\nFunctionGemma is a lightweight, open model from Google, built as a foundation for creating your own specialized function calling models. The model is well suited for text-only function calling. The uniquely small size makes it possible to deploy in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n\n\nFunctionGemma is not intended for use as a direct dialogue model, and is designed to be highly performant after further fine-tuning, as is typical of models this size.\n\n\nBuilt on the Gemma 3 270M model and with the same research and technology used to create the Gemini models, FunctionGemma has been trained specifically for function calling. The model has the same architecture as Gemma 3, but uses a different chat format.\n\n\nFurthermore, akin to the base Gemma 270M, the model has been optimized to be extremely versatile, performant on a variety of hardware in single turn scenarios, but should be finetuned on single turn or multiturn task specific data to achieve best accuracy in specific domains.\n\n\nExamples\n\n\nPython\n\n\nRun the python example with \nuv run tool.py\n\n\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"ollama\",\n#     \"rich\",\n# ]\n# ///\n\"\"\"\nSingle tool, single turn example.\nRun with: uv run tool.py\n\"\"\"\n\nimport json\n\nfrom rich import print\n\nfrom ollama import chat\n\nmodel = 'functiongemma'\n\n\ndef get_weather(city: str) -> str:\n  \"\"\"\n  Get the current weather for a city.\n\n  Args:\n    city: The name of the city\n\n  Returns:\n    A string describing the weather\n  \"\"\"\n  return json.dumps({'city': city, 'temperature': 22, 'unit': 'celsius', 'condition': 'sunny'})\n\n\nmessages = [{'role': 'user', 'content': 'What is the weather in Paris?'}]\nprint('Prompt:', messages[0]['content'])\n\nresponse = chat(model, messages=messages, tools=[get_weather])\n\nif response.message.tool_calls:\n  tool = response.message.tool_calls[0]\n  print(f'Calling: {tool.function.name}({tool.function.arguments})')\n\n  result = get_weather(**tool.function.arguments)\n  print(f'Result: {result}')\n\n  messages.append(response.message)\n  messages.append({'role': 'tool', 'content': result})\n\n  final = chat(model, messages=messages)\n  print('Response:', final.message.content)\nelse:\n  print('Response:', response.message.content)\n\n\n\n\nTypeScript\n\n\nRun the TypeScript example with \nbun run tool.ts\n or \nnpx tsx tool.ts\n\n\n/**\n * Single tool, single turn example.\n * Run with: bun run tool.ts or npx tsx tool.ts\n */\n\nconst OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://localhost:11434';\nconst MODEL = 'functiongemma';\n\nfunction getWeather(city: string): string {\n  return JSON.stringify({ city, temperature: 22, unit: 'celsius', condition: 'sunny' });\n}\n\nconst tools = [\n  {\n    type: 'function',\n    function: {\n      name: 'get_weather',\n      description: 'Get the current weather for a city.',\n      parameters: {\n        type: 'object',\n        properties: {\n          city: { type: 'string', description: 'The name of the city' },\n        },\n        required: ['city'],\n      },\n    },\n  },\n];\n\ninterface Message {\n  role: string;\n  content: string;\n  tool_calls?: { function: { name: string; arguments: Record<string, string> } }[];\n}\n\ninterface ChatResponse {\n  message: Message;\n}\n\nasync function chat(messages: Message[]): Promise<ChatResponse> {\n  const response = await fetch(`${OLLAMA_HOST}/api/chat`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ model: MODEL, messages, tools, stream: false }),\n  });\n\n  if (!response.ok) {\n    throw new Error(`HTTP error: ${response.status} ${await response.text()}`);\n  }\n\n  return response.json();\n}\n\nasync function main() {\n  const messages: Message[] = [{ role: 'user', content: 'What is the weather in Paris?' }];\n  console.log('Prompt:', messages[0].content);\n\n  const response = await chat(messages);\n\n  if (response.message.tool_calls?.length) {\n    const tool = response.message.tool_calls[0];\n    console.log(`Calling: ${tool.function.name}(${JSON.stringify(tool.function.arguments)})\\n`);\n\n    const result = getWeather(tool.function.arguments.city);\n    console.log('Function Result:', result);\n\n    messages.push(response.message);\n    messages.push({ role: 'tool', content: result });\n\n    const final = await chat(messages);\n    console.log('Response:', final.message.content);\n  } else {\n    console.log('Response:', response.message.content);\n  }\n}\n\nmain().catch(console.error);\n\n\n\nBenchmark\n\n\n\n\n\n\n\n\nBenchmark\n\n\nn-shot\n\n\nFunction Gemma 270m\n\n\n\n\n\n\n\n\n\n\nBFCL Simple\n\n\n0-shot\n\n\n61.6\n\n\n\n\n\n\nBFCL Multiple\n\n\n0-shot\n\n\n63.5\n\n\n\n\n\n\nBFCL Parallel\n\n\n0-shot\n\n\n39\n\n\n\n\n\n\nBFCL Parallel Multiple\n\n\n0-shot\n\n\n29.5\n\n\n\n\n\n\nBFCL Live Simple\n\n\n0-shot\n\n\n36.2\n\n\n\n\n\n\nBFCL Live Multiple\n\n\n0-shot\n\n\n25.7\n\n\n\n\n\n\nBFCL Live Parallel\n\n\n0-shot\n\n\n22.9\n\n\n\n\n\n\nBFCL Live Parallel Multiple\n\n\n0-shot\n\n\n20.8\n\n\n\n\n\n\nBFCL Relevance\n\n\n0-shot\n\n\n61.1\n\n\n\n\n\n\nBFCL Irrelevance\n\n\n0-shot\n\n\n73.7\n\n\n\n\n\n\n\n\nTraining Dataset\n\n\nThese models were trained on a dataset of text data that includes a wide variety of sources. The model was trained with 6T tokens. The knowledge cutoff date for the training data was August 2024. There are the key components:\n\n\n\n\nPublic Tool Definitions - Common APIs found on the web\n\n\nTool Use Interactions - These are a mix of prompts, function calls, function responses, and natural language responses from the model to summarise the function call response, or request clarifications when the prompt is ambiguous or incomplete.\n\n\n\n\nData Preprocessing\n\n\nHere are the key data cleaning and filtering methods applied to the training data:\n\n\n\n\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\n\n\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\n\n\nAdditional methods: Filtering based on content quality and safety in line with Google’s \npolicies\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Requires Ollama v0.13.5 or later \n\n### FunctionGemma\n\nFunctionGemma is a lightweight, open model from Google, built as a foundation for creating your own specialized function calling models. The model is well suited for text-only function calling. The uniquely small size makes it possible to deploy in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n\nFunctionGemma is not intended for use as a direct dialogue model, and is designed to be highly performant after further fine-tuning, as is typical of models this size. \n\nBuilt on the Gemma 3 270M model and with the same research and technology used to create the Gemini models, FunctionGemma has been trained specifically for function calling. The model has the same architecture as Gemma 3, but uses a different chat format.  \n\nFurthermore, akin to the base Gemma 270M, the model has been optimized to be extremely versatile, performant on a variety of hardware in single turn scenarios, but should be finetuned on single turn or multiturn task specific data to achieve best accuracy in specific domains.\n\n### Examples\n\n#### Python\nRun the python example with `uv run tool.py`\n```python\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"ollama\",\n#     \"rich\",\n# ]\n# ///\n\"\"\"\nSingle tool, single turn example.\nRun with: uv run tool.py\n\"\"\"\n\nimport json\n\nfrom rich import print\n\nfrom ollama import chat\n\nmodel = 'functiongemma'\n\n\ndef get_weather(city: str) -> str:\n  \"\"\"\n  Get the current weather for a city.\n\n  Args:\n    city: The name of the city\n\n  Returns:\n    A string describing the weather\n  \"\"\"\n  return json.dumps({'city': city, 'temperature': 22, 'unit': 'celsius', 'condition': 'sunny'})\n\n\nmessages = [{'role': 'user', 'content': 'What is the weather in Paris?'}]\nprint('Prompt:', messages[0]['content'])\n\nresponse = chat(model, messages=messages, tools=[get_weather])\n\nif response.message.tool_calls:\n  tool = response.message.tool_calls[0]\n  print(f'Calling: {tool.function.name}({tool.function.arguments})')\n\n  result = get_weather(**tool.function.arguments)\n  print(f'Result: {result}')\n\n  messages.append(response.message)\n  messages.append({'role': 'tool', 'content': result})\n\n  final = chat(model, messages=messages)\n  print('Response:', final.message.content)\nelse:\n  print('Response:', response.message.content)\n\n```\n\n### TypeScript\n\nRun the TypeScript example with `bun run tool.ts` or `npx tsx tool.ts`\n```typescript\n/**\n * Single tool, single turn example.\n * Run with: bun run tool.ts or npx tsx tool.ts\n */\n\nconst OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://localhost:11434';\nconst MODEL = 'functiongemma';\n\nfunction getWeather(city: string): string {\n  return JSON.stringify({ city, temperature: 22, unit: 'celsius', condition: 'sunny' });\n}\n\nconst tools = [\n  {\n    type: 'function',\n    function: {\n      name: 'get_weather',\n      description: 'Get the current weather for a city.',\n      parameters: {\n        type: 'object',\n        properties: {\n          city: { type: 'string', description: 'The name of the city' },\n        },\n        required: ['city'],\n      },\n    },\n  },\n];\n\ninterface Message {\n  role: string;\n  content: string;\n  tool_calls?: { function: { name: string; arguments: Record<string, string> } }[];\n}\n\ninterface ChatResponse {\n  message: Message;\n}\n\nasync function chat(messages: Message[]): Promise<ChatResponse> {\n  const response = await fetch(`${OLLAMA_HOST}/api/chat`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n  ",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "270m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "Fine-tuning for function calling tasks in environments with limited resources.",
    "pulls": 70700,
    "tags": 4,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "f9d83159-f36c-498b-8458-5c7a54761043",
    "model_identifier": "glm-4.6",
    "model_name": "glm-4.6",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/glm-4.6",
    "description": "Advanced agentic, reasoning and coding capabilities.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nModel Introduction\n\n\nGLM-4.6\n  brings several key improvements:\n\n\n\n\nLonger context window:\n The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\n\n\nSuperior coding performance:\n The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\n\n\nAdvanced reasoning:\n GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\n\n\nMore capable agents:\n GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\n\n\nRefined writing:\n Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\n\n\n\n\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as \nDeepSeek-V3.1-Terminus\n and \nClaude Sonnet 4\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/glm-4.6/a2820004-9cb2-45ea-af09-7074224ddd49\" width=\"120\" />\n\n## Model Introduction\n\n**GLM-4.6**  brings several key improvements:\n\n* **Longer context window:** The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\n* **Superior coding performance:** The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\n* **Advanced reasoning:** GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\n* **More capable agents:** GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\n* **Refined writing:** Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\n\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as **DeepSeek-V3.1-Terminus** and **Claude Sonnet 4**.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Reasoning"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced coding tasks and reasoning.",
    "pulls": 69800,
    "tags": 0,
    "last_updated": "2025-10-25",
    "last_updated_str": "4 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4a1897d3-43b9-4e2d-989e-0c8cef9a284c",
    "model_identifier": "gemini-3-flash-preview",
    "model_name": "gemini-3-flash-preview",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemini-3-flash-preview",
    "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nModel\n\n\nGemini 3 Flash Preview on Ollama’s cloud:\n\n\nollama run gemini-3-flash-preview:cloud\n\n\n\nBenchmarks\n\n\nGemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/gemini-3-flash-preview/cc29c7e5-1edc-44bc-9710-1fd5885799fd)\n\n### Model \n\nGemini 3 Flash Preview on Ollama's cloud: \n\n```\nollama run gemini-3-flash-preview:cloud\n```\n\n### Benchmarks\n\nGemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro.\n\n![](/assets/library/gemini-3-flash-preview/cf40550d-c81b-4fcf-aa2c-9473efa2d028)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "vision",
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Reasoning",
      "Translation",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Data Scientists and researchers for advanced language tasks.",
    "pulls": 64400,
    "tags": 2,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "92f922ca-30d3-4a4d-ae9f-2bfda61e76bb",
    "model_identifier": "minimax-m2",
    "model_name": "minimax-m2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/minimax-m2",
    "description": "MiniMax M2 is a high-efficiency large language model built for coding and agentic workflows.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMiniMax M2 is a high-efficiency large language model built for coding and agentic workflows.\n\n\nGet started\n\n\nollama run minimax-m2:cloud\n\n\n\nHighlights\n\n\nSuperior Intelligence\n. According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use. \nIts composite score ranks #1 among open-source models globally\n.\n\n\nAdvanced Coding\n. Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench–style tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages.\n\n\nAgent Performance\n. MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps.\n\n\nEfficient Design\n. With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling—perfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks.\n\n\nCoding & Agentic Benchmarks\n\n\nThese comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI.\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMiniMax-M2\n\n\nClaude Sonnet 4\n\n\nClaude Sonnet 4.5\n\n\nGemini 2.5 Pro\n\n\nGPT-5 (thinking)\n\n\nGLM-4.6\n\n\nKimi K2 0905\n\n\nDeepSeek-V3.2\n\n\n\n\n\n\n\n\n\n\nSWE-bench Verified\n\n\n69.4\n\n\n72.7 *\n\n\n77.2 *\n\n\n63.8 *\n\n\n74.9 *\n\n\n68 *\n\n\n69.2 *\n\n\n67.8 *\n\n\n\n\n\n\nMulti-SWE-Bench\n\n\n36.2\n\n\n35.7 *\n\n\n44.3\n\n\n/\n\n\n/\n\n\n30\n\n\n33.5\n\n\n30.6\n\n\n\n\n\n\nSWE-bench Multilingual\n\n\n56.5\n\n\n56.9 *\n\n\n68\n\n\n/\n\n\n/\n\n\n53.8\n\n\n55.9 *\n\n\n57.9 *\n\n\n\n\n\n\nTerminal-Bench\n\n\n46.3\n\n\n36.4 *\n\n\n50 *\n\n\n25.3 *\n\n\n43.8 *\n\n\n40.5 *\n\n\n44.5 *\n\n\n37.7 *\n\n\n\n\n\n\nArtifactsBench\n\n\n66.8\n\n\n57.3*\n\n\n61.5\n\n\n57.7*\n\n\n73*\n\n\n59.8\n\n\n54.2\n\n\n55.8\n\n\n\n\n\n\nBrowseComp\n\n\n44\n\n\n12.2\n\n\n19.6\n\n\n9.9\n\n\n54.9*\n\n\n45.1*\n\n\n14.1\n\n\n40.1*\n\n\n\n\n\n\nBrowseComp-zh\n\n\n48.5\n\n\n29.1\n\n\n40.8\n\n\n32.2\n\n\n65\n\n\n49.5\n\n\n28.8\n\n\n47.9*\n\n\n\n\n\n\nGAIA (text only)\n\n\n75.7\n\n\n68.3\n\n\n71.2\n\n\n60.2\n\n\n76.4\n\n\n71.9\n\n\n60.2\n\n\n63.5\n\n\n\n\n\n\nxbench-DeepSearch\n\n\n72\n\n\n64.6\n\n\n66\n\n\n56\n\n\n77.8\n\n\n70\n\n\n61\n\n\n71\n\n\n\n\n\n\nHLE (w/ tools)\n\n\n31.8\n\n\n20.3\n\n\n24.5\n\n\n28.4 *\n\n\n35.2 *\n\n\n30.4 *\n\n\n26.9 *\n\n\n27.2 *\n\n\n\n\n\n\nτ²-Bench\n\n\n77.2\n\n\n65.5*\n\n\n84.7*\n\n\n59.2\n\n\n80.1*\n\n\n75.9*\n\n\n70.3\n\n\n66.7\n\n\n\n\n\n\nFinSearchComp-global\n\n\n65.5\n\n\n42\n\n\n60.8\n\n\n42.6*\n\n\n63.9*\n\n\n29.2\n\n\n29.5*\n\n\n26.2\n\n\n\n\n\n\nAgentCompany\n\n\n36\n\n\n37\n\n\n41\n\n\n39.3*\n\n\n/\n\n\n35\n\n\n30\n\n\n34\n\n\n\n\n\n\n\n\n\n\nNotes: Data points marked with an asterisk (*) are taken directly from the model’s official tech report or blog. All other metrics were obtained using the evaluation methods described below.\n- SWE-bench Verified:  We use the same scaffold as \nR2E-Gym\n (Jain et al. 2025) on top of OpenHands to test with agents on SWE tasks. All scores are validated on our internal infrastructure with 128k context length, 100 max steps, and no test-time scaling. All git-related content is removed to ensure agent sees only the code at the issue point.\n- Multi-SWE-Bench & SWE-bench Multilingual: All scores are averaged across 8 runs using the \nclaude-code\n CLI (300 max steps) as the evaluation scaffold.\n- Terminal-Bench: All scores are evaluated with the official claude-code from the original \nTerminal-Bench\n repository(commit \n94bf692\n), averaged over 8 runs to report the mean pass rate.\n- ArtifactsBench: All Scores are computed by averaging three runs with the official implementation of \nArtifactsBench\n, using the stable Gemini-2.5-Pro as the judge model.\n- BrowseComp & BrowseComp-zh & GAIA (text only) & xbench-DeepSearch: All scores reported use the same agent framework as \nWebExplorer\n (Liu et al. 2025), with minor tools description adjustment. We use the 103-sample text-only GAIA validation subset following \nWebExplorer\n (Liu et al. 2025).\n- HLE (w/ tools): All reported scores are obtained using search tools and a Python tool. The search tools employ the same agent framework as \nWebExplorer\n (Liu et al. 2025), and the Python tool runs in a Jupyter environment. We use the text-only HLE subset.\n- τ²-Bench: All scores reported use “extended thinking with tool use”, and employ GPT-4.1 as the user simulator.\n- FinSearchComp-global: Official results are reported for GPT-5-Thinking, Gemini 2.5 Pro, and Kimi-K2. Other models are evaluated using the open-source \nFinSearchComp\n (Hu et al. 2025) framework using both  search and Python tools, launched simultaneously for consistency.\n- AgentCompany: All scores reported use OpenHands 0.42 agent framework.\n\n\n\n\nIntelligence Benchmarks\n\n\nWe align with \nArtificial Analysis\n (AA), which aggregates challenging benchmarks using a consistent methodology to reflect a model’s broader \nintelligence profile\n across math, science, instruction following, coding, and agentic tool use.\n\n\n\n\n\n\n\n\nMetric (AA)\n\n\nMiniMax-M2\n\n\nClaude Sonnet 4\n\n\nClaude Sonnet 4.5\n\n\nGemini 2.5 Pro\n\n\nGPT-5 (thinking)\n\n\nGLM-4.6\n\n\nKimi K2 0905\n\n\nDeepSeek-V3.2\n\n\n\n\n\n\n\n\n\n\nAIME25\n\n\n78\n\n\n74\n\n\n88\n\n\n88\n\n\n94\n\n\n86\n\n\n57\n\n\n88\n\n\n\n\n\n\nMMLU-Pro\n\n\n82\n\n\n84\n\n\n88\n\n\n86\n\n\n87\n\n\n83\n\n\n82\n\n\n85\n\n\n\n\n\n\nGPQA-Diamond\n\n\n78\n\n\n78\n\n\n83\n\n\n84\n\n\n85\n\n\n78\n\n\n77\n\n\n80\n\n\n\n\n\n\nHLE (w/o tools)\n\n\n12.5\n\n\n9.6\n\n\n17.3\n\n\n21.1\n\n\n26.5\n\n\n13.3\n\n\n6.3\n\n\n13.8\n\n\n\n\n\n\nLiveCodeBench (LCB)\n\n\n83\n\n\n66\n\n\n71\n\n\n80\n\n\n85\n\n\n70\n\n\n61\n\n\n79\n\n\n\n\n\n\nSciCode\n\n\n36\n\n\n40\n\n\n45\n\n\n43\n\n\n43\n\n\n38\n\n\n31\n\n\n38\n\n\n\n\n\n\nIFBench\n\n\n72\n\n\n55\n\n\n57\n\n\n49\n\n\n73\n\n\n43\n\n\n42\n\n\n54\n\n\n\n\n\n\nAA-LCR\n\n\n61\n\n\n65\n\n\n66\n\n\n66\n\n\n76\n\n\n54\n\n\n52\n\n\n69\n\n\n\n\n\n\nτ²-Bench-Telecom\n\n\n87\n\n\n65\n\n\n78\n\n\n54\n\n\n85\n\n\n71\n\n\n73\n\n\n34\n\n\n\n\n\n\nTerminal-Bench-Hard\n\n\n24\n\n\n30\n\n\n33\n\n\n25\n\n\n31\n\n\n23\n\n\n23\n\n\n29\n\n\n\n\n\n\nAA Intelligence\n\n\n61\n\n\n57\n\n\n63\n\n\n60\n\n\n69\n\n\n56\n\n\n50\n\n\n57\n\n\n\n\n\n\n\n\n\n\nAA: All scores of MiniMax-M2 aligned with Artificial Analysis Intelligence Benchmarking Methodology (\nhttps://artificialanalysis.ai/methodology/intelligence-benchmarking\n). All scores of other models reported from \nhttps://artificialanalysis.ai/\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![MiniMax M2 banner](/assets/library/minimax-m2/855052f9-7ace-44ae-a870-08f43ebd5a7c)\n\nMiniMax M2 is a high-efficiency large language model built for coding and agentic workflows. \n\n### Get started \n\n```\nollama run minimax-m2:cloud\n```\n\n### Highlights\n\n**Superior Intelligence**. According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use. **Its composite score ranks #1 among open-source models globally**.\n\n**Advanced Coding**. Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench–style tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages.\n\n**Agent Performance**. MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps.\n\n**Efficient Design**. With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling—perfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks.\n\n### Coding & Agentic Benchmarks\n\nThese comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI.\n\n| **Benchmark** | **MiniMax-M2** | **Claude Sonnet 4** | **Claude Sonnet 4.5** | **Gemini 2.5 Pro** | **GPT-5 (thinking)** | **GLM-4.6** | **Kimi K2 0905** | **DeepSeek-V3.2** |\n|-----------|------------|-----------------|-------------------|-----------------|------------------|---------|---------------|----------------|\n| **SWE-bench Verified** | 69.4 | 72.7 * | 77.2 * | 63.8 * | 74.9 * | 68 * | 69.2 * | 67.8 * |\n| **Multi-SWE-Bench** | 36.2 | 35.7 * | 44.3 | / | / | 30 | 33.5 | 30.6 |\n| **SWE-bench Multilingual** | 56.5 | 56.9 * | 68 | / | / | 53.8 | 55.9 * | 57.9 * |\n| **Terminal-Bench** | 46.3 | 36.4 * | 50 * | 25.3 * | 43.8 * | 40.5 * | 44.5 * | 37.7 * |\n| **ArtifactsBench** | 66.8 | 57.3* | 61.5 | 57.7* | 73* | 59.8 | 54.2 | 55.8 |\n| **BrowseComp** | 44 | 12.2 | 19.6 | 9.9 | 54.9* | 45.1* | 14.1 | 40.1* |\n| **BrowseComp-zh** | 48.5 | 29.1 | 40.8 | 32.2 | 65 | 49.5 | 28.8 | 47.9* |\n| **GAIA (text only)** | 75.7 | 68.3 | 71.2 | 60.2 | 76.4 | 71.9 | 60.2 | 63.5 |\n| **xbench-DeepSearch** | 72 | 64.6 | 66 | 56 | 77.8 | 70 | 61 | 71 |\n| **HLE (w/ tools)** | 31.8 | 20.3 | 24.5 | 28.4 * | 35.2 * | 30.4 * | 26.9 * | 27.2 * |\n| **τ²-Bench** | 77.2 | 65.5* | 84.7* | 59.2 | 80.1* | 75.9* | 70.3 | 66.7 |\n| **FinSearchComp-global** | 65.5 | 42 | 60.8 | 42.6* | 63.9* | 29.2 | 29.5* | 26.2 |\n| **AgentCompany** | 36 | 37 | 41 | 39.3* | / | 35 | 30 | 34 |\n\n>Notes: Data points marked with an asterisk (*) are taken directly from the model's official tech report or blog. All other metrics were obtained using the evaluation methods described below.\n>- SWE-bench Verified:  We use the same scaffold as [R2E-Gym](https://arxiv.org/pdf/2504.07164) (Jain et al. 2025) on top of OpenHands to test with agents on SWE tasks. All scores are validated on our internal infrastructure with 128k context length, 100 max steps, and no test",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on coding tasks.",
    "pulls": 62400,
    "tags": 0,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "73371748-c2dc-4a04-b2ea-e9a81aa25ed6",
    "model_identifier": "glm-5",
    "model_name": "glm-5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/glm-5",
    "description": "A strong reasoning and agentic model from Z.ai with 744B total parameters (40B active), built for complex systems engineering and long-horizon tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGLM-5 is a mixture-of-experts model from \nZ.ai\n with 744B total parameters and 40B active parameters. It scales up from GLM-4.5’s 355B parameters and is designed for complex reasoning, coding, and agentic tasks.\n\n\nThe model uses DeepSeek Sparse Attention (DSA) to reduce deployment costs while preserving long-context capacity, and was post-trained using a novel asynchronous RL infrastructure for improved training efficiency.\n\n\nKey capabilities\n\n\n\n\nReasoning and math:\n Scores 92.7% on AIME 2026 I and 86.0% on GPQA-Diamond\n\n\nCoding:\n Achieves 77.8% on SWE-bench Verified and 73.3% on SWE-bench Multilingual\n\n\nAgentic tasks:\n 62.0 on BrowseComp and 56.2 on Terminal-Bench 2.0\n\n\nLong context:\n Supports up to 128K+ context through DSA optimization\n\n\nMultilingual:\n Supports English and Chinese\n\n\n\n\nBenchmarks\n\n\n\n\nFootnote\n\n\n\n\nHumanity’s Last Exam (HLE) & other reasoning tasks\n: We evaluate with a maximum generation length of 131,072 tokens (\ntemperature=1.0, top_p=0.95, max_new_tokens=131072\n). By default, we report the text-only subset; results marked with * are from the full set. We use GPT-5.2 (medium) as the judge model. For HLE-with-tools, we use a maximum context length of 202,752 tokens.\n\n\nSWE-bench & SWE-bench Multilingual\n: We run the SWE-bench suite with OpenHands using a tailored instruction prompt. Settings: \ntemperature=0.7, top_p=0.95, max_new_tokens=16384\n, with a 200K context window.\n\n\nBrowserComp\n: Without context management, we retain details from the most recent 5 turns. With context management, we use the same discard-all strategy as DeepSeek-v3.2 and Kimi K2.5.\n\n\nTerminal-Bench 2.0 (Terminus 2)\n: We evaluate with the Terminus framework using \ntimeout=2h, temperature=0.7, top_p=1.0, max_new_tokens=8192\n, with a 128K context window. Resource limits are capped at 16 CPUs and 32 GB RAM.\n\n\nTerminal-Bench 2.0 (Claude Code)\n: We evaluate in Claude Code 2.1.14 (think mode, default effort) with \ntemperature=1.0, top_p=0.95, max_new_tokens=65536\n. We remove wall-clock time limits due to generation speed, while preserving per-task CPU and memory constraints. Scores are averaged over 5 runs. We fix environment issues introduced by Claude Code and also report results on a verified Terminal-Bench 2.0 dataset that resolves ambiguous instructions (see: \nhttps://huggingface.co/datasets/zai-org/terminal-bench-2-verified\n).\n\n\nCyberGym\n: We evaluate in Claude Code 2.1.18 (think mode, no web tools) with (\ntemperature=1.0, top_p=1.0, max_new_tokens=32000\n) and a 250-minute timeout per task. Results are single-run Pass@1 over 1,507 tasks.\n\n\nMCP-Atlas\n: All models are evaluated in think mode on the 500-task public subset with a 10-minute timeout per task. We use Gemini 3 Pro as the judge model.\n\n\nτ²-bench\n: We add a small prompt adjustment in Retail and Telecom to avoid failures caused by premature user termination. For Airline, we apply the domain fixes proposed in the Claude Opus 4.5 system card.\n\n\nVending Bench 2\n: Runs are conducted independently by \nAndon Labs\n.\n\n\n\n\nSupported languages\n\n\n\n\nEnglish\n\n\nChinese\n\n\n\n\nLicense\n\n\nMIT\n\n\nReference\n\n\n\n\nZ.ai Technical Blog\n\n\nGLM-5 on Hugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/glm-4.7-flash/8e1f3c2e-cfb1-4516-a57c-312b7daac14a\" width=\"128\" />\n\nGLM-5 is a mixture-of-experts model from [Z.ai](https://z.ai) with 744B total parameters and 40B active parameters. It scales up from GLM-4.5's 355B parameters and is designed for complex reasoning, coding, and agentic tasks.\n\nThe model uses DeepSeek Sparse Attention (DSA) to reduce deployment costs while preserving long-context capacity, and was post-trained using a novel asynchronous RL infrastructure for improved training efficiency.\n\n### Key capabilities\n\n* **Reasoning and math:** Scores 92.7% on AIME 2026 I and 86.0% on GPQA-Diamond\n* **Coding:** Achieves 77.8% on SWE-bench Verified and 73.3% on SWE-bench Multilingual\n* **Agentic tasks:** 62.0 on BrowseComp and 56.2 on Terminal-Bench 2.0\n* **Long context:** Supports up to 128K+ context through DSA optimization\n* **Multilingual:** Supports English and Chinese\n\n### Benchmarks\n\n![Ollama screenshot 2026-02-11 at 14.37.11@2x.png](/assets/library/glm-5/b0e27be4-2f36-4ca3-a7fc-78e965c385bb)\n\n### Footnote\n\n* **Humanity’s Last Exam (HLE) & other reasoning tasks**: We evaluate with a maximum generation length of 131,072 tokens (`temperature=1.0, top_p=0.95, max_new_tokens=131072`). By default, we report the text-only subset; results marked with * are from the full set. We use GPT-5.2 (medium) as the judge model. For HLE-with-tools, we use a maximum context length of 202,752 tokens.\n* **SWE-bench & SWE-bench Multilingual**: We run the SWE-bench suite with OpenHands using a tailored instruction prompt. Settings: `temperature=0.7, top_p=0.95, max_new_tokens=16384`, with a 200K context window.\n* **BrowserComp**: Without context management, we retain details from the most recent 5 turns. With context management, we use the same discard-all strategy as DeepSeek-v3.2 and Kimi K2.5.\n* **Terminal-Bench 2.0 (Terminus 2)**: We evaluate with the Terminus framework using `timeout=2h, temperature=0.7, top_p=1.0, max_new_tokens=8192`, with a 128K context window. Resource limits are capped at 16 CPUs and 32 GB RAM.\n* **Terminal-Bench 2.0 (Claude Code)**: We evaluate in Claude Code 2.1.14 (think mode, default effort) with `temperature=1.0, top_p=0.95, max_new_tokens=65536`. We remove wall-clock time limits due to generation speed, while preserving per-task CPU and memory constraints. Scores are averaged over 5 runs. We fix environment issues introduced by Claude Code and also report results on a verified Terminal-Bench 2.0 dataset that resolves ambiguous instructions (see: [https://huggingface.co/datasets/zai-org/terminal-bench-2-verified](https://huggingface.co/datasets/zai-org/terminal-bench-2-verified)).\n* **CyberGym**: We evaluate in Claude Code 2.1.18 (think mode, no web tools) with (`temperature=1.0, top_p=1.0, max_new_tokens=32000`) and a 250-minute timeout per task. Results are single-run Pass@1 over 1,507 tasks.\n* **MCP-Atlas**: All models are evaluated in think mode on the 500-task public subset with a 10-minute timeout per task. We use Gemini 3 Pro as the judge model.\n* **τ²-bench**: We add a small prompt adjustment in Retail and Telecom to avoid failures caused by premature user termination. For Airline, we apply the domain fixes proposed in the Claude Opus 4.5 system card.\n* **Vending Bench 2**: Runs are conducted independently by [Andon Labs](https://andonlabs.com/evals/vending-bench-2).\n\n\n### Supported languages\n\n* English\n* Chinese\n\n### License\n\n[MIT](https://opensource.org/licenses/MIT)\n\n### Reference\n\n* [Z.ai Technical Blog](https://z.ai/blog/glm-5)\n* [GLM-5 on Hugging Face](https://huggingface.co/zai-org/GLM-5)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Reasoning",
      "Code Generation",
      "Math"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Complex systems engineering and long-horizon tasks involving reasoning, coding, and agentic tasks.",
    "pulls": 54500,
    "tags": 0,
    "last_updated": "2026-02-18",
    "last_updated_str": "1 week ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "90852aab-b394-44af-878c-0a3a9ca19ae6",
    "model_identifier": "glm-4.7",
    "model_name": "glm-4.7",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/glm-4.7",
    "description": "Advancing the Coding Capability",
    "readme": "Readme\n\n\n\n\n\n\n\n\nGLM-4.7\n, your new coding partner, is coming with the following features:\n\n\n\n\nCore Coding:\n GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code.\n\n\nVibe Coding:\n GLM-4.7 takes a major step forward in UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.\n\n\nTool Using:\n GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as τ^2-Bench and on web browsing via BrowseComp.\n\n\nComplex Reasoning:\n GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity’s Last Exam) benchmark compared to GLM-4.6.\n\n\n\n\nYou can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.\n\n\n\n\nBenchmark Performance.\n More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.\n\n\n\n\nCoding:\n AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it \nfeels\n. True intelligence isn’t just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-“coding” this time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**GLM-4.7**, your new coding partner, is coming with the following features:\n\n- **Core Coding:** GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code.\n- **Vibe Coding:** GLM-4.7 takes a major step forward in UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.\n- **Tool Using:** GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as τ^2-Bench and on web browsing via BrowseComp.\n- **Complex Reasoning:** GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity’s Last Exam) benchmark compared to GLM-4.6.\n\nYou can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.\n\n![image.png](/assets/library/glm-4.7/668b8777-c7c9-4786-9370-d440c6195c6f)\n\n\n**Benchmark Performance.** More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.\n\n![Ollama screenshot 2025-12-22 at 15.10.19@2x.png](/assets/library/glm-4.7/03e4b9bf-a7c8-471f-818b-aa7b15d4bc10)\n\n**Coding:** AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it *feels*. True intelligence isn't just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-\"coding\" this time.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on complex coding tasks.",
    "pulls": 52000,
    "tags": 0,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "6c6a10e6-8f74-47ab-88d3-6a1c2891024c",
    "model_identifier": "glm-ocr",
    "model_name": "glm-ocr",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/glm-ocr",
    "description": "GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder–decoder architecture.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder–decoder architecture. The model integrates the CogViT visual encoder pre-trained on large-scale image–text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder.\n\n\n\n\nUsage\n\n\nText recognition\n\n\nollama run glm-ocr Text Recognition: ./image.png\n\n\n\nTable recognition\n\n\nollama run glm-ocr Table Recognition: ./image.png\n\n\n\nFigure recognition\n\n\nollama run glm-ocr Figure Recognition: ./image.png\n\n\n\nKey features\n\n\n\n\nState-of-the-Art Performance\n: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks,\nincluding formula recognition, table recognition, and information extraction.\n\n\nOptimized for Real-World Scenarios\n: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.\n\n\nEfficient Inference\n: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.\n\n\nEasy to Use\n: Fully open-sourced and equipped with a comprehensive \nSDK\n and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![logo.svg](/assets/library/glm-ocr/c08dfddc-0734-4983-a450-3466d01594ee)\n\nGLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder–decoder architecture. The model integrates the CogViT visual encoder pre-trained on large-scale image–text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder.\n\n![image.png](/assets/library/glm-ocr/7b4bd5ed-e6bc-4f36-889d-a02fa36e88ae)\n\n### Usage\n\n#### Text recognition\n\n```shell\nollama run glm-ocr Text Recognition: ./image.png\n```\n\n#### Table recognition\n\n```shell\nollama run glm-ocr Table Recognition: ./image.png\n```\n\n#### Figure recognition\n\n```shell\nollama run glm-ocr Figure Recognition: ./image.png\n```\n\n### Key features\n\n- **State-of-the-Art Performance**: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, \nincluding formula recognition, table recognition, and information extraction.\n\n- **Optimized for Real-World Scenarios**: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.\n\n- **Efficient Inference**: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.\n\n- **Easy to Use**: Fully open-sourced and equipped with a comprehensive [SDK](https://github.com/zai-org/GLM-OCR) and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools"
    ],
    "capability": "Vision",
    "labels": [
      "vision",
      "tools"
    ],
    "memory_requirements": [
      {
        "tag": "glm-ocr:q8_0",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q8_0",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "glm-ocr:latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "glm-ocr:bf16",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "bf16",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.6,
    "use_cases": [
      "Image Understanding",
      "Function Calling"
    ],
    "domain": "Vision",
    "ai_languages": [],
    "complexity": null,
    "best_for": "For image recognition tasks such as text and table detection.",
    "pulls": 44000,
    "tags": 3,
    "last_updated": "2026-02-04",
    "last_updated_str": "3 weeks ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "1ed8d20a-351d-49f7-bd86-cc6048b8ed5d",
    "model_identifier": "nomic-embed-text-v2-moe",
    "model_name": "nomic-embed-text-v2-moe",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nomic-embed-text-v2-moe",
    "description": "nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nnomic-embed-text-v2-moe\n is a multilingual MoE text embedding model that excels at multilingual retrieval.\n\n\n\n\nHigh Performance\n: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n\n\nMultilinguality\n: Supports ~100 languages and trained on over 1.6B pairs\n\n\nFlexible Embedding Dimension\n: Trained with \nMatryoshka Embeddings\n with 3x reductions in storage cost with minimal performance degradations\n\n\nFully Open-Source\n: Model weights, \ncode\n, and training data\n\n\n\n\n\n\n\n\n\n\nModel\n\n\nParams (M)\n\n\nEmb Dim\n\n\nBEIR\n\n\nMIRACL\n\n\nPretrain Data\n\n\nFinetune Data\n\n\nCode\n\n\n\n\n\n\n\n\n\n\nNomic Embed v2\n\n\n305\n\n\n768\n\n\n52.86\n\n\n65.80\n\n\n✅\n\n\n✅\n\n\n✅\n\n\n\n\n\n\nmE5 Base\n\n\n278\n\n\n768\n\n\n48.88\n\n\n62.30\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\n\n\nmGTE Base\n\n\n305\n\n\n768\n\n\n51.10\n\n\n63.40\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\n\n\nArctic Embed v2 Base\n\n\n305\n\n\n768\n\n\n55.40\n\n\n59.90\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBGE M3\n\n\n568\n\n\n1024\n\n\n48.80\n\n\n69.20\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n\n\n\n\nArctic Embed v2 Large\n\n\n568\n\n\n1024\n\n\n55.65\n\n\n66.00\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\n\n\nmE5 Large\n\n\n560\n\n\n1024\n\n\n51.40\n\n\n66.50\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\n\n\n\n\nBest practices\n\n\n\n\nAdd appropriate prefixes to your text:\n\n\n\n\nFor queries: “search_query: “\n\n\nFor documents: “search_document: “- Maximum input length is 512 tokens\n\n\n\n\nFor optimal efficiency, consider using the 256-dimension embeddings if storage/compute is a concern\n\n\n\n\nModel Architecture\n\n\n\n\nTotal Parameters\n: 475M\n\n\nActive Parameters During Inference\n: 305M\n\n\nArchitecture Type\n: Mixture of Experts (MoE)\n\n\nMoE Configuration\n: 8 experts with top-2 routing\n\n\nEmbedding Dimensions\n: Supports flexible dimension from 768 to 256 through Matryoshka representation learning\n\n\nMaximum Sequence Length\n: 512 tokens\n\n\nLanguages\n: See below for supported languages and its training pairs per different languages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`nomic-embed-text-v2-moe` is a multilingual MoE text embedding model that excels at multilingual retrieval. \n\n- **High Performance**: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n- **Multilinguality**: Supports ~100 languages and trained on over 1.6B pairs\n- **Flexible Embedding Dimension**: Trained with [Matryoshka Embeddings](https://arxiv.org/abs/2205.13147) with 3x reductions in storage cost with minimal performance degradations\n- **Fully Open-Source**: Model weights, [code](https://github.com/nomic-ai/contrastors), and training data\n\n| Model | Params (M) | Emb Dim | BEIR | MIRACL | Pretrain Data | Finetune Data | Code |\n|-------|------------|----------|------|---------|---------------|---------------|------|\n| **Nomic Embed v2** | 305 | 768 | 52.86 | **65.80** | ✅ | ✅ | ✅ |\n| mE5 Base | 278 | 768 | 48.88 | 62.30 | ❌   | ❌   | ❌   |\n| mGTE Base | 305 | 768 | 51.10 | 63.40 | ❌ | ❌ | ❌ |\n| Arctic Embed v2 Base | 305 | 768 | **55.40** | 59.90 | ❌ | ❌ | ❌ |\n|   |\n| BGE M3 | 568 | 1024 | 48.80 | **69.20** | ❌ | ✅ | ❌ |\n| Arctic Embed v2 Large | 568 | 1024 | **55.65** | 66.00 | ❌ | ❌ | ❌ |\n| mE5 Large | 560 | 1024 | 51.40 | 66.50 | ❌ | ❌ | ❌ |\n\n### Best practices \n\n- Add appropriate prefixes to your text:\n  - For queries: \"search_query: \"\n  - For documents: \"search_document: \"- Maximum input length is 512 tokens\n- For optimal efficiency, consider using the 256-dimension embeddings if storage/compute is a concern\n\n### Model Architecture\n- **Total Parameters**: 475M\n- **Active Parameters During Inference**: 305M\n- **Architecture Type**: Mixture of Experts (MoE)\n- **MoE Configuration**: 8 experts with top-2 routing\n- **Embedding Dimensions**: Supports flexible dimension from 768 to 256 through Matryoshka representation learning\n- **Maximum Sequence Length**: 512 tokens\n- **Languages**: See below for supported languages and its training pairs per different languages\n\n![image.png](/assets/library/nomic-embed-text-v2-moe/f235fb9c-f5ee-407c-aaa1-54ea35d95e0e)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "embedding"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Embedding",
      "RAG / Retrieval"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "For tasks requiring multilingual text embedding and retrieval.",
    "pulls": 42000,
    "tags": 0,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "0cb18ff1-907c-4dc9-8408-add27666c988",
    "model_identifier": "minimax-m2.5",
    "model_name": "minimax-m2.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/minimax-m2.5",
    "description": "MiniMax-M2.5 is a state-of-the-art large language model designed for real-world productivity and coding tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMiniMax M2.5 is the fastest-improving model series for coding and agentic workflows, trained with large-scale reinforcement learning across hundreds of thousands of real-world environments.\n\n\nHighlights\n\n\n\n\nCoding\n. Trained on large-scale RL across 10+ languages (Python, Go, C, C++, TypeScript, Rust, Kotlin, Java, JavaScript, PHP, Lua, Dart, Ruby) and hundreds of thousands of real environments, M2.5 develops native “spec behavior”—planning architecture, structure, and design before writing code. It handles the full development lifecycle: system design from zero, environment setup, feature iteration, code review, and testing across Web, Android, iOS, Windows, and Mac.\n\n\nLeading agent performance\n. M2.5 achieves 80.2% on SWE-Bench Verified, 51.3% on Multi-SWE-Bench, and 76.3% on BrowseComp—reaching a level comparable to the Claude Opus series on coding tasks. In search and tool-use tasks, the model demonstrates higher decision maturity—solving problems in fewer rounds with better token efficiency, saving roughly 20% in tool-call rounds compared to M2.1.\n\n\n37% faster task completion\n. Through improved task decomposition and more efficient chain-of-thought reasoning, M2.5 completes complex agentic tasks significantly faster. On SWE-Bench Verified, end-to-end runtime dropped from 31.3 minutes (M2.1) to 22.8 minutes—on par with Claude Opus 4.6’s 22.9 minutes—while also consuming fewer tokens per task (3.52M vs 3.72M).\n\n\n\n\n\n\nBenchmarks\n\n\nCoding\n\n\nM2.5 represents a significant step up from M2.1, reaching performance comparable to the Claude Opus series on core software engineering tasks.\n\n\n\n\nMiniMax tested performance on the SWE-Bench Verified evaluation set using different coding agent harnesses.\n\n\nOn Droid: 79.7(M2.5) > 78.9(Opus 4.6)\n\n\nOn OpenCode: 76.1(M2.5) > 75.9(Opus 4.6)\n\n\nSearch and tool calling\n\n\n\n\nOffice & Productivity\n\n\nM2.5 was trained in collaboration with domain experts in finance, law, and social sciences to produce genuinely deliverable outputs. In evaluations on advanced office tasks—including Word documents, PowerPoint presentations, and Excel financial modeling—M2.5 achieved a 59.0% average win rate against mainstream models using pairwise comparison (GDPval-MM framework).\n\n\nAdditional internal benchmarks include MEWC (based on Microsoft Excel World Championship problems, 2021–2026) and Finance Modeling (expert-constructed financial modeling tasks scored via rubrics). Detailed scores for these benchmarks are available as charts in the \noriginal blog post\n.\n\n\nReference\n\n\n\n\nMiniMax M2.5 announcement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![logo](/assets/library/minimax-m2.5/7bf8b4e2-abdc-4f35-af56-ac075749d26c)\n\nMiniMax M2.5 is the fastest-improving model series for coding and agentic workflows, trained with large-scale reinforcement learning across hundreds of thousands of real-world environments.\n\n### Highlights\n\n- **Coding**. Trained on large-scale RL across 10+ languages (Python, Go, C, C++, TypeScript, Rust, Kotlin, Java, JavaScript, PHP, Lua, Dart, Ruby) and hundreds of thousands of real environments, M2.5 develops native \"spec behavior\"—planning architecture, structure, and design before writing code. It handles the full development lifecycle: system design from zero, environment setup, feature iteration, code review, and testing across Web, Android, iOS, Windows, and Mac.\n\n- **Leading agent performance**. M2.5 achieves 80.2% on SWE-Bench Verified, 51.3% on Multi-SWE-Bench, and 76.3% on BrowseComp—reaching a level comparable to the Claude Opus series on coding tasks. In search and tool-use tasks, the model demonstrates higher decision maturity—solving problems in fewer rounds with better token efficiency, saving roughly 20% in tool-call rounds compared to M2.1.\n\n- **37% faster task completion**. Through improved task decomposition and more efficient chain-of-thought reasoning, M2.5 completes complex agentic tasks significantly faster. On SWE-Bench Verified, end-to-end runtime dropped from 31.3 minutes (M2.1) to 22.8 minutes—on par with Claude Opus 4.6's 22.9 minutes—while also consuming fewer tokens per task (3.52M vs 3.72M).\n\n![m2.5 benchmark](/assets/library/minimax-m2.5/d0fc55de-dbe8-4a30-8bb7-545cd0d9da04)\n\n### Benchmarks\n\n**Coding**\n\nM2.5 represents a significant step up from M2.1, reaching performance comparable to the Claude Opus series on core software engineering tasks. \n\n![image.png](/assets/library/minimax-m2.5/65275c3b-5ed8-4d1f-940a-407b65bbdd78)\n\nMiniMax tested performance on the SWE-Bench Verified evaluation set using different coding agent harnesses.\n\nOn Droid: 79.7(M2.5) > 78.9(Opus 4.6)\n\nOn OpenCode: 76.1(M2.5) > 75.9(Opus 4.6) \n\n**Search and tool calling** \n\n![image.png](/assets/library/minimax-m2.5/38ae04b3-e0c8-4643-9c16-7b1d9d30b03c)\n\n### Office & Productivity\n\nM2.5 was trained in collaboration with domain experts in finance, law, and social sciences to produce genuinely deliverable outputs. In evaluations on advanced office tasks—including Word documents, PowerPoint presentations, and Excel financial modeling—M2.5 achieved a 59.0% average win rate against mainstream models using pairwise comparison (GDPval-MM framework).\n\nAdditional internal benchmarks include MEWC (based on Microsoft Excel World Championship problems, 2021–2026) and Finance Modeling (expert-constructed financial modeling tasks scored via rubrics). Detailed scores for these benchmarks are available as charts in the [original blog post](https://www.minimax.io/news/minimax-m25).\n\n### Reference\n\n* [MiniMax M2.5 announcement](https://www.minimax.io/news/minimax-m25)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Code Review",
      "Function Calling"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Chinese",
      "Japanese",
      "Korean"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on coding tasks.",
    "pulls": 44500,
    "tags": 0,
    "last_updated": "2026-02-18",
    "last_updated_str": "1 week ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "fddd7a21-a1d8-45bd-8981-6eb74d727e7d",
    "model_identifier": "kimi-k2",
    "model_name": "kimi-k2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/kimi-k2",
    "description": "A state-of-the-art mixture-of-experts (MoE) language model. Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nKimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters.\n\n\nKey Features\n\n\n\n\nEnhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.\n\n\nImproved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming.\n\n\nExtended context length: Kimi K2-Instruct-0905’s context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/kimi-k2/02232216-54d9-449f-8e3a-1054f17f3b18\" width=\"320\" />\n\nKimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters.\n\n\n### Key Features\n- Enhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.\n- Improved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming.\n- Extended context length: Kimi K2-Instruct-0905’s context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Text Summarization",
      "Question Answering"
    ],
    "domain": "Code",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers working on coding tasks.",
    "pulls": 40000,
    "tags": 0,
    "last_updated": "2025-09-25",
    "last_updated_str": "5 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "5f6662a7-f6d6-4ac0-bf38-21ed330fe962",
    "model_identifier": "deepseek-v3.2",
    "model_name": "deepseek-v3.2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-v3.2",
    "description": "DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek-V3.2 is a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n\n\n\nDeepSeek Sparse Attention (DSA)\n: an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n\n\nScalable Reinforcement Learning Framework\n: By implementing a robust RL protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5.\n\n\nLarge-Scale Agentic Task Synthesis Pipeline\n: To integrate reasoning into tool-use scenarios, [DeepSeek team] developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n\n\n\nReference\n\n\nDeepSeek-V3.2: Pushing the Frontier of Open Large Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" />\n\n![DeepSeek v3.2](/assets/library/deepseek-v3.2/1fd79185-cd9a-47e2-b416-c3467c6edcc9)\n\nDeepSeek-V3.2 is a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA)**: an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios. \n\n2. **Scalable Reinforcement Learning Framework**: By implementing a robust RL protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. \n\n3. **Large-Scale Agentic Task Synthesis Pipeline**: To integrate reasoning into tool-use scenarios, [DeepSeek team] developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n### Reference \n\n[DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Reasoning",
      "Function Calling"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for complex language tasks.",
    "pulls": 34200,
    "tags": 0,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "d509240b-fbc4-4202-a404-935fe090f675",
    "model_identifier": "kimi-k2-thinking",
    "model_name": "kimi-k2-thinking",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/kimi-k2-thinking",
    "description": "Kimi K2 Thinking, Moonshot AI's best open-source thinking model.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nKimi K2 Thinking is Moonshot AI’s best open-source thinking model.\n\n\nBuilt as a \nthinking agent\n, it reasons step by step while using tools, achieving state-of-the-art performance on Humanity’s Last Exam (HLE), BrowseComp, and other benchmarks, with major gains in reasoning, agentic search, coding, writing, and general capabilities.\n\n\nKimi K2 Thinking can execute up to \n200 – 300 sequential tool calls\n without human interference, reasoning coherently across hundreds of steps to solve complex problems.\n\n\nIt marks Moonshot AI’s latest efforts in \ntest-time scaling\n, by scaling both thinking tokens and tool calling steps.\n\n\nAgentic Coding\n\n\nK2 Thinking exhibits substantial gains in coding and software development tasks. It achieves scores of 61.1% on SWE-Multilingual, 71.3% on SWE-Bench Verified, and 47.1% on Terminal-Bench, showcasing strong generalization across programming languages and agent scaffolds.\n\n\nThe model delivers notable improvements on HTML, React, and component-intensive front-end tasks—translating ideas into fully functional, responsive products. In agentic coding settings, it reasons while invoking tools, integrating fluidly into software agents to execute complex, multi-step development workflows with precision and adaptability.\n\n\nAgentic Search and Browsing\n\n\nK2 Thinking demonstrates strong performance in agentic search and browsing scenarios. On BrowseComp—a challenging benchmark designed to evaluate models’ ability to ​\ncontinuously browse, search, and reason over hard-to-find real-world web information​\n—K2 Thinking achieved a score of \n​​60.​2%\n, significantly outperforming the human baseline of 29.2%. This result highlights K2 Thinking’s superior capability for goal-directed, web-based reasoning and its robustness in dynamic, information-rich environments.\n\n\nK2 Thinking can execute ​\n200–300 sequential tool calls\n​, driven by \nlong-horizon planning\n and ​\nadaptive reasoning​\n. It performs dynamic cycles of ​think → search → browser use → think → code​, continually generating and refining hypotheses, verifying evidence, reasoning, and constructing coherent answers. This interleaved reasoning allows it to decompose ambiguous, open-ended problems into clear, actionable subtasks.\n\n\nGeneral Capabilities\n\n\nCreative Writing:\n ​K2 Thinking delivers improvements in completeness and richness. It shows stronger command of style and instruction, handling diverse tones and formats with natural fluency. Its writing becomes more vivid and imaginative—poetic imagery carries deeper associations, while stories and scripts feel more human, emotional, and purposeful. The ideas it expresses often reach greater thematic depth and resonance.\n\n\nPractical Writing:\n ​K2 Thinking demonstrates marked gains in reasoning depth, perspective breadth, and instruction adherence. It follows prompts with higher precision, addressing each requirement clearly and systematically—often expanding on every mentioned point to ensure thorough coverage. In academic, research, and long-form analytical writing, it excels at producing rigorous, logically coherent, and substantively rich content, making it particularly effective in scholarly and professional contexts.\n\n\nPersonal & Emotional:\n ​When addressing personal or emotional questions, K2 Thinking responds with more empathy and balance. Its reflections are thoughtful and specific, offering nuanced perspectives and actionable next steps. It helps users navigate complex decisions with clarity and care—grounded, practical, and genuinely human in tone.\n\n\nBenchmarks\n\n\nKimi K2 Thinking sets new records across benchmarks that assess reasoning, coding, and agent capabilities. K2 Thinking achieves 44.9% on HLE with tools, 60.2% on BrowseComp, and 71.3% on SWE-Bench Verified, demonstrating strong generalization as a state-of-the-art thinking agent model.\n\n\n\n\n\n\n\n\n\n\nBenchmark\n\n\nIntro\n\n\nK2 Thinking\n\n\nGPT-5\n\n\nClaude Sonnet 4.5 (Thinking)\n\n\nK2 0905\n\n\nDeepSeek-V3.2\n\n\nGrok-4\n\n\n\n\n\n\n\n\n\n\nReasoning Tasks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHumanity’s Last Exam\n(Text-only)\n\n\nno tools\n\n\n23.9\n\n\n26.3\n\n\n19.8\n\n\n7.9\n\n\n19.8\n\n\n25.4\n\n\n\n\n\n\n\n\nw/ tools\n\n\n44.9\n\n\n41.7\n\n\n32.0\n\n\n21.7\n\n\n20.3\n\n\n41.0\n\n\n\n\n\n\n\n\nheavy\n\n\n51.0\n\n\n42.0\n\n\n—\n\n\n—\n\n\n—\n\n\n50.7\n\n\n\n\n\n\nAIME 2025\n\n\nno tools\n\n\n94.5\n\n\n94.6\n\n\n87.0\n\n\n51.0\n\n\n89.3\n\n\n91.7\n\n\n\n\n\n\n\n\nw/ python\n\n\n99.1\n\n\n99.6\n\n\n100.0\n\n\n75.2\n\n\n58.1\n\n\n98.8\n\n\n\n\n\n\n\n\nheavy\n\n\n100.0\n\n\n100.0\n\n\n—\n\n\n—\n\n\n—\n\n\n100.0\n\n\n\n\n\n\nHMMT 2025\n\n\nno tools\n\n\n89.4\n\n\n93.3\n\n\n74.6\n\n\n38.8\n\n\n83.6\n\n\n90.0\n\n\n\n\n\n\n\n\nw/ python\n\n\n95.1\n\n\n96.7\n\n\n88.8\n\n\n70.4\n\n\n49.5\n\n\n93.9\n\n\n\n\n\n\n\n\nheavy\n\n\n97.5\n\n\n100.0\n\n\n—\n\n\n—\n\n\n—\n\n\n96.7\n\n\n\n\n\n\nIMO-AnswerBench\n\n\nno tools\n\n\n78.6\n\n\n76.0\n\n\n65.9\n\n\n45.8\n\n\n76.0\n\n\n73.1\n\n\n\n\n\n\nGPQA-Diamond\n\n\nno tools\n\n\n84.5\n\n\n85.7\n\n\n83.4\n\n\n74.2\n\n\n79.9\n\n\n87.5\n\n\n\n\n\n\nGeneral Tasks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU-Pro\n\n\nno tools\n\n\n84.6\n\n\n87.1\n\n\n87.5\n\n\n81.9\n\n\n85.0\n\n\n—\n\n\n\n\n\n\nMMLU-Redux\n\n\nno tools\n\n\n94.4\n\n\n95.3\n\n\n95.6\n\n\n92.7\n\n\n93.7\n\n\n—\n\n\n\n\n\n\nLongform Writing\n\n\nno tools\n\n\n73.8\n\n\n71.4\n\n\n79.8\n\n\n62.8\n\n\n72.5\n\n\n—\n\n\n\n\n\n\nHealthBench\n\n\nno tools\n\n\n58.0\n\n\n67.2\n\n\n44.2\n\n\n43.8\n\n\n46.9\n\n\n—\n\n\n\n\n\n\nAgentic Search Tasks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrowseComp\n\n\nw/ tools\n\n\n60.2\n\n\n54.9\n\n\n24.1\n\n\n7.4\n\n\n40.1\n\n\n—\n\n\n\n\n\n\nBrowseComp-ZH\n\n\nw/ tools\n\n\n62.3\n\n\n63.0\n\n\n42.4\n\n\n22.2\n\n\n47.9\n\n\n—\n\n\n\n\n\n\nSeal-0\n\n\nw/ tools\n\n\n56.3\n\n\n51.4\n\n\n53.4\n\n\n25.2\n\n\n38.5\n\n\n—\n\n\n\n\n\n\nFinSearchComp-T3\n\n\nw/ tools\n\n\n47.4\n\n\n48.5\n\n\n44.0\n\n\n10.4\n\n\n27.0\n\n\n—\n\n\n\n\n\n\nFrames\n\n\nw/ tools\n\n\n87.0\n\n\n86.0\n\n\n85.0\n\n\n58.1\n\n\n80.2\n\n\n—\n\n\n\n\n\n\nCoding Tasks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSWE-bench Verified\n\n\nw/ tools\n\n\n71.3\n\n\n74.9\n\n\n77.2\n\n\n69.2\n\n\n67.8\n\n\n—\n\n\n\n\n\n\nSWE-bench Multilingual\n\n\nw/ tools\n\n\n61.1\n\n\n55.3\n\n\n68.0\n\n\n55.9\n\n\n57.9\n\n\n—\n\n\n\n\n\n\nMulti-SWE-bench\n\n\nw/ tools\n\n\n41.9\n\n\n39.3\n\n\n44.3\n\n\n33.5\n\n\n30.6\n\n\n—\n\n\n\n\n\n\nSciCode\n\n\nno tools\n\n\n44.8\n\n\n42.9\n\n\n44.7\n\n\n30.7\n\n\n37.7\n\n\n—\n\n\n\n\n\n\nLiveCodeBench v6\n\n\nno tools\n\n\n83.1\n\n\n87.0\n\n\n64.0\n\n\n56.1\n\n\n74.1\n\n\n—\n\n\n\n\n\n\nOJ-Bench\n(cpp)\n\n\nno tools\n\n\n48.7\n\n\n56.2\n\n\n30.4\n\n\n25.5\n\n\n38.2\n\n\n—\n\n\n\n\n\n\nTerminal-Bench\n\n\nw/ simulated tools (JSON)\n\n\n47.1\n\n\n43.8\n\n\n51.0\n\n\n44.5\n\n\n37.7\n\n\n—\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/kimi-k2/02232216-54d9-449f-8e3a-1054f17f3b18\" width=\"320\" />\n\nKimi K2 Thinking is Moonshot AI's best open-source thinking model. \n\nBuilt as a **thinking agent**, it reasons step by step while using tools, achieving state-of-the-art performance on Humanity's Last Exam (HLE), BrowseComp, and other benchmarks, with major gains in reasoning, agentic search, coding, writing, and general capabilities.\n\nKimi K2 Thinking can execute up to **200 – 300 sequential tool calls** without human interference, reasoning coherently across hundreds of steps to solve complex problems.\n\nIt marks Moonshot AI's latest efforts in **test-time scaling**, by scaling both thinking tokens and tool calling steps.\n\n### Agentic Coding\n\nK2 Thinking exhibits substantial gains in coding and software development tasks. It achieves scores of 61.1% on SWE-Multilingual, 71.3% on SWE-Bench Verified, and 47.1% on Terminal-Bench, showcasing strong generalization across programming languages and agent scaffolds.\n\nThe model delivers notable improvements on HTML, React, and component-intensive front-end tasks—translating ideas into fully functional, responsive products. In agentic coding settings, it reasons while invoking tools, integrating fluidly into software agents to execute complex, multi-step development workflows with precision and adaptability.\n\n### Agentic Search and Browsing\n\nK2 Thinking demonstrates strong performance in agentic search and browsing scenarios. On BrowseComp—a challenging benchmark designed to evaluate models' ability to ​**continuously browse, search, and reason over hard-to-find real-world web information​**—K2 Thinking achieved a score of **​​60.​2%**, significantly outperforming the human baseline of 29.2%. This result highlights K2 Thinking's superior capability for goal-directed, web-based reasoning and its robustness in dynamic, information-rich environments.\n\nK2 Thinking can execute ​**200–300 sequential tool calls**​, driven by **long-horizon planning** and ​**adaptive reasoning​**. It performs dynamic cycles of ​think → search → browser use → think → code​, continually generating and refining hypotheses, verifying evidence, reasoning, and constructing coherent answers. This interleaved reasoning allows it to decompose ambiguous, open-ended problems into clear, actionable subtasks.\n\n### General Capabilities\n**Creative Writing:** ​K2 Thinking delivers improvements in completeness and richness. It shows stronger command of style and instruction, handling diverse tones and formats with natural fluency. Its writing becomes more vivid and imaginative—poetic imagery carries deeper associations, while stories and scripts feel more human, emotional, and purposeful. The ideas it expresses often reach greater thematic depth and resonance.\n\n**Practical Writing:** ​K2 Thinking demonstrates marked gains in reasoning depth, perspective breadth, and instruction adherence. It follows prompts with higher precision, addressing each requirement clearly and systematically—often expanding on every mentioned point to ensure thorough coverage. In academic, research, and long-form analytical writing, it excels at producing rigorous, logically coherent, and substantively rich content, making it particularly effective in scholarly and professional contexts.\n\n**Personal & Emotional:** ​When addressing personal or emotional questions, K2 Thinking responds with more empathy and balance. Its reflections are thoughtful and specific, offering nuanced perspectives and actionable next steps. It helps users navigate complex decisions with clarity and care—grounded, practical, and genuinely human in tone.\n\n### Benchmarks \n\nKimi K2 Thinking sets new records across benchmarks that assess reasoning, coding, and agent capabilities. K2 Thinking achieves 44.9% on HLE with tools, 60.2% on BrowseComp, and 71.3% on SWE-Bench Verified, demonstrating strong generalizat",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "thinking",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Reasoning",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Chinese"
    ],
    "complexity": "advanced",
    "best_for": "Complex problem solving and tool execution in a wide range of applications.",
    "pulls": 31200,
    "tags": 0,
    "last_updated": "2025-11-25",
    "last_updated_str": "3 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "4ae2ae6d-e0fd-424f-9039-f35fbfe2da86",
    "model_identifier": "minimax-m2.1",
    "model_name": "minimax-m2.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/minimax-m2.1",
    "description": "Exceptional multilingual capabilities to elevate code engineering",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\nollama run minimax-m2.1:cloud\n\n\n\nModel Highlights\n\n\n1. Multilingual Coding Excellence (Beyond Python)\n\n\nWhile many models focus primarily on Python, real-world engineering requires cross-language proficiency. M2.1 delivers significant performance gains across \nRust, Java, Golang, C++, Kotlin, Objective-C, TypeScript, and JavaScript\n.\n\n\n\n\nWeb3 Dominance:\n Special optimization for Web3 protocols, offering superior performance for blockchain and decentralized projects.\n\n\nBenchmark Leadership:\n Achieved \n49.4% on Multi-SWE-Bench\n, surpassing industry leaders like Anthropic Claude 3.5 Sonnet and Gemini 1.5 Pro.\n\n\nDeep Comprehension:\n Advanced code review capabilities, including sophisticated performance optimization and structural analysis.\n\n\n\n\n2. Optimized for “Vibe” AppDev & Native Mobile\n\n\nWe have bridged the gap between aesthetic design and technical implementation.\n\n\n\n\nWeb & Scientific Simulation:\n Enhanced “web aesthetics” for better UI/UX generation and more realistic scientific scenario simulations.\n\n\nNative Mobile Powerhouse:\n Addressing a common industry weakness, M2.1 significantly boosts native \nAndroid and iOS\n development capabilities.\n\n\n“Not only vibe webdev, but vibe appdev.”\n\n\n\n\n3. Concise, High-Efficiency Responses\n\n\nCompared to the previous generation, MiniMax-M2.1 provides cleaner outputs and more streamlined \nChain-of-Thought (CoT)\n reasoning. This reduction in “verbosity” results in a noticeably faster “feel” and near-instant response times for developer workflows.\n\n\n4. Advanced Interleaved Thinking & Instruction Following\n\n\nM2.1 is the first open-source model series to implement \nAdvanced Interleaved Thinking\n, upgrading its systematic problem-solving capacity.\n\n\n\n\nComplex Constraints:\n The model doesn’t just focus on code correctness; it excels at integrating “composite instruction constraints” (as seen in \nOctoCodingBench\n).\n\n\nOffice Readiness:\n These improvements make the model viable for complex administrative and office automation tasks (demonstrated in our \nToolathlon\n showcase).\n\n\n\n\n5. Enhanced Scaffolding & Agent Generalization\n\n\nM2.1 is designed to be the “brain” behind your favorite tools. It shows exceptional performance across various programming agents and IDE extensions, including \nClaude Code, Droid (Factory AI), Cline, Kilo Code, and Roo Code.\n\n\n\n\nContext Management:\n Seamless support for framework-specific configurations like \nSkill.md\n, \nClaude.md\n, \nagent.md\n, \n.cursorrules\n, and Slash Commands.\n\n\n\n\n6. The Most Lightweight SOTA Model (10B Activated)\n\n\nIn just two months, we have achieved a massive leap in utility while maintaining the model’s legendary efficiency.\n\n\n\n\nEfficiency Powerhouse:\n With only \n10B activated parameters\n, M2.1 remains the most cost-effective SOTA-performance model in the open-source community.\n\n\n\n\n7. High-Quality Dialogue & Creative Writing\n\n\nM2.1 isn’t just a coding specialist—it’s a more capable all-around assistant. Compared to M2, the chat and writing experience has been significantly refined, delivering more nuanced, detailed, and contextually rich answers for non-technical queries.\n\n\nBenchmarks\n\n\nMiniMax-M2.1 delivers a significant leap over M2 on core software engineering leaderboards[cite: 14]. It shines particularly bright in multilingual scenarios, where it outperforms Claude Sonnet 4.5 and closely approaches Claude Opus 4.5.\n\n\nCore Software Engineering Benchmarks\n\n\n\n\n\n\n\n\nBenchmark\n\n\nSWE-bench Verified\n\n\nMulti-SWE-bench\n\n\nSWE-bench Multilingual\n\n\nTerminal-bench 2.0\n\n\n\n\n\n\n\n\n\n\nMiniMax-M2.1\n\n\n74.0\n\n\n49.4\n\n\n72.5\n\n\n47.9\n\n\n\n\n\n\nMiniMax-M2\n\n\n69.4\n\n\n36.2\n\n\n56.5\n\n\n30.0\n\n\n\n\n\n\nKimi K2 Thinking\n\n\n71.3\n\n\n41.9\n\n\n61.1\n\n\n35.2\n\n\n\n\n\n\nDeepSeek V3.2\n\n\n73.1\n\n\n37.4\n\n\n70.2\n\n\n46.4\n\n\n\n\n\n\nGLM 4.6\n\n\n68.0\n\n\n30.0\n\n\n53.8\n\n\n24.5\n\n\n\n\n\n\nClaude Sonnet 4.5\n\n\n77.2\n\n\n44.3\n\n\n68 ± 0.5\n\n\n50.0\n\n\n\n\n\n\nClaude Opus 4.5\n\n\n80.9\n\n\n50.0\n\n\n77.5 ± 1.5\n\n\n57.8\n\n\n\n\n\n\nGemini 3 Pro\n\n\n78.0\n\n\n38.0\n\n\n65.0\n\n\n54.2\n\n\n\n\n\n\nGPT-5.2 (thinking)\n\n\n80.0\n\n\nx\n\n\n72.0\n\n\n54.0\n\n\n\n\n\n\n\n\nFramework Generalization\n\n\n\n\n\n\n\n\nBenchmark\n\n\nSWE-bench Verified (Claude Code)\n\n\nSWE-bench Verified (Droid)\n\n\nSWE-bench Verified (mini-swe-agent)\n\n\nSWT-bench\n\n\nSWE-Perf\n\n\nSWE-Review\n\n\nOctoCodingbench\n\n\n\n\n\n\n\n\n\n\nMiniMax-M2.1\n\n\n74.0\n\n\n71.3\n\n\n67.0\n\n\n69.3\n\n\n3.1\n\n\n8.9\n\n\n26.1\n\n\n\n\n\n\nMiniMax-M2\n\n\n69.4\n\n\n68.1\n\n\n61.0\n\n\n32.8\n\n\n1.4\n\n\n3.4\n\n\n13.3\n\n\n\n\n\n\nKimi K2 Thinking\n\n\n71.3\n\n\n64.0\n\n\n63.4\n\n\n38.2\n\n\n1.0\n\n\n5.3\n\n\n16.8\n\n\n\n\n\n\nDeepSeek V3.2\n\n\n73.1\n\n\n67.0\n\n\n60.0\n\n\n62.0\n\n\n0.9\n\n\n6.4\n\n\n26.0\n\n\n\n\n\n\nGLM 4.6\n\n\n68.0\n\n\n64.9\n\n\n55.4\n\n\n45.9\n\n\n0.9\n\n\n5.6\n\n\n19.2\n\n\n\n\n\n\nClaude Sonnet 4.5\n\n\n77.2\n\n\n72.3\n\n\n70.6\n\n\n69.5\n\n\n3.0\n\n\n10.5\n\n\n22.8\n\n\n\n\n\n\nClaude Opus 4.5\n\n\n80.9\n\n\n75.2\n\n\n74.4\n\n\n80.2\n\n\n4.7\n\n\n16.2\n\n\n36.2\n\n\n\n\n\n\nGemini 3 Pro\n\n\n78.0\n\n\nx\n\n\n71.8\n\n\n79.7\n\n\n6.5\n\n\nx\n\n\n22.9\n\n\n\n\n\n\nGPT-5.2 (thinking)\n\n\n80.0\n\n\nx\n\n\n74.2\n\n\n80.7\n\n\n3.6\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\nVIBE Benchmark (Visual & Interactive Benchmark for Execution)\n\n\n\n\n\n\n\n\nBenchmark\n\n\nVIBE (Average)\n\n\nVIBE-Web\n\n\nVIBE-Simulation\n\n\nVIBE-Android\n\n\nVIBE-iOS\n\n\nVIBE-Backend\n\n\n\n\n\n\n\n\n\n\nMiniMax-M2.1\n\n\n88.6\n\n\n91.5\n\n\n87.1\n\n\n89.7\n\n\n88\n\n\n86.7\n\n\n\n\n\n\nMiniMax-M2\n\n\n67.5\n\n\n80.4\n\n\n77\n\n\n69.2\n\n\n39.5\n\n\n67.8\n\n\n\n\n\n\nGLM 4.6\n\n\n72.9\n\n\n86.7\n\n\n82.4\n\n\n58.2\n\n\n59.1\n\n\n78.3\n\n\n\n\n\n\nClaude Sonnet 4.5\n\n\n85.2\n\n\n87.3\n\n\n79.1\n\n\n87.5\n\n\n81.2\n\n\n90.8\n\n\n\n\n\n\nClaude Opus 4.5\n\n\n90.7\n\n\n89.1\n\n\n84\n\n\n92.2\n\n\n90\n\n\n98\n\n\n\n\n\n\nGemini 3 Pro\n\n\n82.4\n\n\n89.5\n\n\n89.2\n\n\n78.7\n\n\n75.8\n\n\n78.7\n\n\n\n\n\n\n\n\nLong-Horizon Tool Use & Intelligence Metric\n\n\n\n\n\n\n\n\nBenchmark\n\n\nToolathlon\n\n\nBrowseComp\n\n\nBrowseComp (context management)\n\n\nAA-Index\n\n\n\n\n\n\n\n\n\n\nMiniMax-M2.1\n\n\n43.5\n\n\n47.4\n\n\n62\n\n\n64\n\n\n\n\n\n\nMiniMax-M2\n\n\n16.7\n\n\n44\n\n\n56.9\n\n\n61\n\n\n\n\n\n\nKimi K2 Thinking\n\n\n17.6\n\n\n41.5\n\n\n60.2\n\n\n67\n\n\n\n\n\n\nDeepSeek V3.2\n\n\n35.2\n\n\n51.4\n\n\n67.6\n\n\n66\n\n\n\n\n\n\nGLM 4.6\n\n\n18.8\n\n\n45.1\n\n\n50.2\n\n\n56\n\n\n\n\n\n\nClaude Sonnet 4.5\n\n\n38.9\n\n\n19.6\n\n\n26.1\n\n\n63\n\n\n\n\n\n\nClaude Opus 4.5\n\n\n43.5\n\n\n37\n\n\n57.8\n\n\n70\n\n\n\n\n\n\nGemini 3 Pro\n\n\n36.4\n\n\n37.8\n\n\n59.2\n\n\n73\n\n\n\n\n\n\nGPT-5.2 (thinking)\n\n\n41.7\n\n\n65.8\n\n\n70\n\n\n73\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/minimax-m2.1/8e6a3ede-ffe9-430f-b3d2-ca3af4fd4a76)\n\n\n### Get started \n\n```\nollama run minimax-m2.1:cloud\n```\n\n### Model Highlights \n\n**1. Multilingual Coding Excellence (Beyond Python)**\n\nWhile many models focus primarily on Python, real-world engineering requires cross-language proficiency. M2.1 delivers significant performance gains across **Rust, Java, Golang, C++, Kotlin, Objective-C, TypeScript, and JavaScript**.\n\n- **Web3 Dominance:** Special optimization for Web3 protocols, offering superior performance for blockchain and decentralized projects.\n- **Benchmark Leadership:** Achieved **49.4% on Multi-SWE-Bench**, surpassing industry leaders like Anthropic Claude 3.5 Sonnet and Gemini 1.5 Pro.\n- **Deep Comprehension:** Advanced code review capabilities, including sophisticated performance optimization and structural analysis.\n\n**2. Optimized for \"Vibe\" AppDev & Native Mobile**\n\nWe have bridged the gap between aesthetic design and technical implementation.\n\n- **Web & Scientific Simulation:** Enhanced \"web aesthetics\" for better UI/UX generation and more realistic scientific scenario simulations.\n- **Native Mobile Powerhouse:** Addressing a common industry weakness, M2.1 significantly boosts native **Android and iOS** development capabilities.\n- *“Not only vibe webdev, but vibe appdev.”*\n\n**3. Concise, High-Efficiency Responses**\n\nCompared to the previous generation, MiniMax-M2.1 provides cleaner outputs and more streamlined **Chain-of-Thought (CoT)** reasoning. This reduction in \"verbosity\" results in a noticeably faster \"feel\" and near-instant response times for developer workflows.\n\n**4. Advanced Interleaved Thinking & Instruction Following**\n\nM2.1 is the first open-source model series to implement **Advanced Interleaved Thinking**, upgrading its systematic problem-solving capacity.\n\n- **Complex Constraints:** The model doesn't just focus on code correctness; it excels at integrating \"composite instruction constraints\" (as seen in **OctoCodingBench**).\n- **Office Readiness:** These improvements make the model viable for complex administrative and office automation tasks (demonstrated in our **Toolathlon** showcase).\n\n**5. Enhanced Scaffolding & Agent Generalization**\n\nM2.1 is designed to be the \"brain\" behind your favorite tools. It shows exceptional performance across various programming agents and IDE extensions, including **Claude Code, Droid (Factory AI), Cline, Kilo Code, and Roo Code.**\n\n- **Context Management:** Seamless support for framework-specific configurations like `Skill.md`, `Claude.md`, `agent.md`, `.cursorrules`, and Slash Commands.\n\n**6. The Most Lightweight SOTA Model (10B Activated)**\n\nIn just two months, we have achieved a massive leap in utility while maintaining the model’s legendary efficiency.\n\n- **Efficiency Powerhouse:** With only **10B activated parameters**, M2.1 remains the most cost-effective SOTA-performance model in the open-source community.\n\n**7. High-Quality Dialogue & Creative Writing**\n\nM2.1 isn't just a coding specialist—it’s a more capable all-around assistant. Compared to M2, the chat and writing experience has been significantly refined, delivering more nuanced, detailed, and contextually rich answers for non-technical queries.\n\n### Benchmarks \n\nMiniMax-M2.1 delivers a significant leap over M2 on core software engineering leaderboards[cite: 14]. It shines particularly bright in multilingual scenarios, where it outperforms Claude Sonnet 4.5 and closely approaches Claude Opus 4.5.\n\n**Core Software Engineering Benchmarks** \n\n| Benchmark | SWE-bench Verified | Multi-SWE-bench | SWE-bench Multilingual | Terminal-bench 2.0 |\n| :--- | :--- | :--- | :--- | :--- |\n| **MiniMax-M2.1** | **74.0** | **49.4** | **72.5** | **47.9** |\n| MiniMax-M2 | 69.4 | 36.2 | 56.5 | 30.0 |\n| Kimi K2 Thinking | 71.3 | 41.9 | 61.1 | 35.2 |\n| DeepSeek V3.2 | 73.1 | 37.4 | 70.2 | 46.4 |\n| GLM 4.6 | 68.0 | 30.0 | 53.8 | 24.5 |\n| Claude Sonnet 4.5 | 77.2 | 44.3 | 68 ± 0.5 | 50.0 |\n| ",
    "capabilities": [
      "Tools",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "tools",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Code Generation",
      "Code Review",
      "Translation"
    ],
    "domain": "Code",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers focusing on multilingual code engineering.",
    "pulls": 19300,
    "tags": 0,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "89b6ffd1-07e9-480b-beb5-29516b6bc6a1",
    "model_identifier": "mistral-large-3",
    "model_name": "mistral-large-3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-large-3",
    "description": "A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral Large 3 is a state-of-the-art general-purpose Multimodal Mixture-of-Experts model, designed for production-grade tasks and enterprise workloads.\n\n\nThe Mistral Large 3 model offers the following capabilities:\n\n\n\n\nVision\n: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n\n\nMultilingual\n: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n\n\nSystem Prompt\n: Maintains strong adherence and support for system prompts.\n\n\nAgentic\n: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n\n\nFrontier\n: Delivers best-in-class performance for the most demanding tasks.\n\n\nApache 2.0 License\n: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\n\n\nLarge Context Window\n: Supports a 256k context window.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/ministral-3/83fa3859-d87f-492c-bd81-596cfbceeccb\" width=\"120\" />\n\nMistral Large 3 is a state-of-the-art general-purpose Multimodal Mixture-of-Experts model, designed for production-grade tasks and enterprise workloads.\n\nThe Mistral Large 3 model offers the following capabilities:\n\n*   **Vision**: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n*   **Multilingual**: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n*   **System Prompt**: Maintains strong adherence and support for system prompts.\n*   **Agentic**: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n*   **Frontier**: Delivers best-in-class performance for the most demanding tasks.\n*   **Apache 2.0 License**: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\n*   **Large Context Window**: Supports a 256k context window.\n\n![benchmarks](/assets/library/mistral-large-3/46298ae1-b1c6-4298-b825-8aae46123d63)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision",
      "Tools",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "vision",
      "tools",
      "cloud"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Function Calling",
      "Image Understanding",
      "Translation"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Enterprise workloads and production-grade tasks requiring a general-purpose multimodal model.",
    "pulls": 19300,
    "tags": 0,
    "last_updated": "2025-12-25",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "cd600dd6-5a15-418f-b664-521caae846a4",
    "model_identifier": "qwen3.5",
    "model_name": "qwen3.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3.5",
    "description": "Qwen 3.5 is a family of open-source multimodal models that delivers exceptional utility and performance.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nOver recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and efficiency.\n\n\nHighlights\n\n\nQwen3.5 features the following enhancement:\n\n\n\n\nUnified Vision-Language Foundation\n: Early fusion training on multimodal tokens achieves cross-generational parity with Qwen3 and outperforms Qwen3-VL models across reasoning, coding, agents, and visual understanding benchmarks.\n\n\nEfficient Hybrid Architecture\n: Gated Delta Networks combined with sparse Mixture-of-Experts deliver high-throughput inference with minimal latency and cost overhead.\n\n\nScalable RL Generalization\n: Reinforcement learning scaled across million-agent environments with progressively complex task distributions for robust real-world adaptability.\n\n\nGlobal Linguistic Coverage\n: Expanded support to 201 languages and dialects, enabling inclusive, worldwide deployment with nuanced cultural and regional understanding.\n\n\nNext-Generation Training Infrastructure\n: Near-100% multimodal training efficiency compared to text-only training and asynchronous RL frameworks supporting massive-scale agent scaffolds and environment orchestration.\n\n\n\n\nBenchmarks\n\n\n\n\nLanguage\n\n\n\n\n\n\n\n\n\n\nGPT5.2\n\n\nClaude 4.5 Opus\n\n\nGemini-3 Pro\n\n\nQwen3-Max-Thinking\n\n\nK2.5-1T-A32B\n\n\nQwen3.5-397B-A17B\n\n\n\n\n\n\nKnowledge\n\n\n\n\nMMLU-Pro\n\n\n87.4\n\n\n89.5\n\n\n89.8\n\n\n85.7\n\n\n87.1\n\n\n87.8\n\n\n\n\n\n\nMMLU-Redux\n\n\n95.0\n\n\n95.6\n\n\n95.9\n\n\n92.8\n\n\n94.5\n\n\n94.9\n\n\n\n\n\n\nSuperGPQA\n\n\n67.9\n\n\n70.6\n\n\n74.0\n\n\n67.3\n\n\n69.2\n\n\n70.4\n\n\n\n\n\n\nC-Eval\n\n\n90.5\n\n\n92.2\n\n\n93.4\n\n\n93.7\n\n\n94.0\n\n\n93.0\n\n\n\n\nInstruction Following\n\n\n\n\nIFEval\n\n\n94.8\n\n\n90.9\n\n\n93.5\n\n\n93.4\n\n\n93.9\n\n\n92.6\n\n\n\n\n\n\nIFBench\n\n\n75.4\n\n\n58.0\n\n\n70.4\n\n\n70.9\n\n\n70.2\n\n\n76.5\n\n\n\n\n\n\nMultiChallenge\n\n\n57.9\n\n\n54.2\n\n\n64.2\n\n\n63.3\n\n\n62.7\n\n\n67.6\n\n\n\n\nLong Context\n\n\n\n\nAA-LCR\n\n\n72.7\n\n\n74.0\n\n\n70.7\n\n\n68.7\n\n\n70.0\n\n\n68.7\n\n\n\n\n\n\nLongBench v2\n\n\n54.5\n\n\n64.4\n\n\n68.2\n\n\n60.6\n\n\n61.0\n\n\n63.2\n\n\n\n\nSTEM\n\n\n\n\nGPQA\n\n\n92.4\n\n\n87.0\n\n\n91.9\n\n\n87.4\n\n\n87.6\n\n\n88.4\n\n\n\n\n\n\nHLE\n\n\n35.5\n\n\n30.8\n\n\n37.5\n\n\n30.2\n\n\n30.1\n\n\n28.7\n\n\n\n\n\n\nHLE-Verified¹\n\n\n43.3\n\n\n38.8\n\n\n48\n\n\n37.6\n\n\n--\n\n\n37.6\n\n\n\n\nReasoning\n\n\n\n\nLiveCodeBench v6\n\n\n87.7\n\n\n84.8\n\n\n90.7\n\n\n85.9\n\n\n85.0\n\n\n83.6\n\n\n\n\n\n\nHMMT Feb 25\n\n\n99.4\n\n\n92.9\n\n\n97.3\n\n\n98.0\n\n\n95.4\n\n\n94.8\n\n\n\n\n\n\nHMMT Nov 25\n\n\n100\n\n\n93.3\n\n\n93.3\n\n\n94.7\n\n\n91.1\n\n\n92.7\n\n\n\n\n\n\nIMOAnswerBench\n\n\n86.3\n\n\n84.0\n\n\n83.3\n\n\n83.9\n\n\n81.8\n\n\n80.9\n\n\n\n\n\n\nAIME26\n\n\n96.7\n\n\n93.3\n\n\n90.6\n\n\n93.3\n\n\n93.3\n\n\n91.3\n\n\n\n\nGeneral Agent\n\n\n\n\nBFCL-V4\n\n\n63.1\n\n\n77.5\n\n\n72.5\n\n\n67.7\n\n\n68.3\n\n\n72.9\n\n\n\n\n\n\nTAU2-Bench\n\n\n87.1\n\n\n91.6\n\n\n85.4\n\n\n84.6\n\n\n77.0\n\n\n86.7\n\n\n\n\n\n\nVITA-Bench\n\n\n38.2\n\n\n56.3\n\n\n51.6\n\n\n40.9\n\n\n41.9\n\n\n49.7\n\n\n\n\n\n\nDeepPlanning\n\n\n44.6\n\n\n33.9\n\n\n23.3\n\n\n28.7\n\n\n14.5\n\n\n34.3\n\n\n\n\n\n\nTool Decathlon\n\n\n43.8\n\n\n43.5\n\n\n36.4\n\n\n18.8\n\n\n27.8\n\n\n38.3\n\n\n\n\n\n\nMCP-Mark\n\n\n57.5\n\n\n42.3\n\n\n53.9\n\n\n33.5\n\n\n29.5\n\n\n46.1\n\n\n\n\nSearch Agent³\n\n\n\n\nHLE w/ tool\n\n\n45.5\n\n\n43.4\n\n\n45.8\n\n\n49.8\n\n\n50.2\n\n\n48.3\n\n\n\n\n\n\nBrowseComp\n\n\n65.8\n\n\n67.8\n\n\n59.2\n\n\n53.9\n\n\n--/74.9\n\n\n69.0/78.6\n\n\n\n\n\n\nBrowseComp-zh\n\n\n76.1\n\n\n62.4\n\n\n66.8\n\n\n60.9\n\n\n--\n\n\n70.3\n\n\n\n\n\n\nWideSearch\n\n\n76.8\n\n\n76.4\n\n\n68.0\n\n\n57.9\n\n\n72.7\n\n\n74.0\n\n\n\n\n\n\nSeal-0\n\n\n45.0\n\n\n47.7\n\n\n45.5\n\n\n46.9\n\n\n57.4\n\n\n46.9\n\n\n\n\nMultilingualism\n\n\n\n\nMMMLU\n\n\n89.5\n\n\n90.1\n\n\n90.6\n\n\n84.4\n\n\n86.0\n\n\n88.5\n\n\n\n\n\n\nMMLU-ProX\n\n\n83.7\n\n\n85.7\n\n\n87.7\n\n\n78.5\n\n\n82.3\n\n\n84.7\n\n\n\n\n\n\nNOVA-63\n\n\n54.6\n\n\n56.7\n\n\n56.7\n\n\n54.2\n\n\n56.0\n\n\n59.1\n\n\n\n\n\n\nINCLUDE\n\n\n87.5\n\n\n86.2\n\n\n90.5\n\n\n82.3\n\n\n83.3\n\n\n85.6\n\n\n\n\n\n\nGlobal PIQA\n\n\n90.9\n\n\n91.6\n\n\n93.2\n\n\n86.0\n\n\n89.3\n\n\n89.8\n\n\n\n\n\n\nPolyMATH\n\n\n62.5\n\n\n79.0\n\n\n81.6\n\n\n64.7\n\n\n43.1\n\n\n73.3\n\n\n\n\n\n\nWMT24++\n\n\n78.8\n\n\n79.7\n\n\n80.7\n\n\n77.6\n\n\n77.6\n\n\n78.9\n\n\n\n\n\n\nMAXIFE\n\n\n88.4\n\n\n79.2\n\n\n87.5\n\n\n84.0\n\n\n72.8\n\n\n88.2\n\n\n\n\nCoding Agent\n\n\n\n\nSWE-bench Verified\n\n\n80.0\n\n\n80.9\n\n\n76.2\n\n\n75.3\n\n\n76.8\n\n\n76.2\n\n\n\n\n\n\nSWE-bench Multilingual\n\n\n72.0\n\n\n77.5\n\n\n65.0\n\n\n66.7\n\n\n73.0\n\n\n69.3\n\n\n\n\n\n\nSecCodeBench\n\n\n68.7\n\n\n68.6\n\n\n62.4\n\n\n57.5\n\n\n61.3\n\n\n68.3\n\n\n\n\n\n\nTerminal Bench 2\n\n\n54.0\n\n\n59.3\n\n\n54.2\n\n\n22.5\n\n\n50.8\n\n\n52.5\n\n\n\n\n\n\n\n\n\n* HLE-Verified: a verified and revised version of Humanity’s Last Exam (HLE), accompanied by a transparent, component-wise verification protocol and a fine-grained error taxonomy. We open-source the dataset at https://huggingface.co/datasets/skylenage/HLE-Verified.\n\n* TAU2-Bench: we follow the official setup except for the airline domain, where all models are evaluated by applying the fixes proposed in the Claude Opus 4.5 system card.\n\n* MCPMark: GitHub MCP server uses v0.30.3 from api.githubcopilot.com; Playwright tool responses are truncated at 32k tokens.\n\n* Search Agent: most search agents built on our model adopt a simple context-folding strategy(256k): once the cumulative Tool Response length reaches a preset threshold, earlier Tool Responses are pruned from the history to keep the context within limits.\n\n* BrowseComp: we tested two strategies, simple context-folding achieved a score of 69.0, while using the same discard-all strategy as DeepSeek-V3.2 and Kimi K2.5 achieved 78.6.\n\n* WideSearch: we use a 256k context window without any context management.\n\n* MMLU-ProX: we report the averaged accuracy on 29 languages.\n\n* WMT24++: a harder subset of WMT24 after difficulty labeling and rebalancing; we report the averaged scores on 55 languages using XCOMET-XXL.\n\n* MAXIFE: we report the accuracy on English + multilingual original prompts (totally 23 settings).\n\n* Empty cells (--) indicate scores not yet available or not applicable.\n\n\n\n\n\n\nVision Language\n\n\n\n\n\n\n\n\n\n\nGPT5.2\n\n\nClaude 4.5 Opus\n\n\nGemini-3 Pro\n\n\nQwen3-VL-235B-A22B\n\n\nK2.5-1T-A32B\n\n\nQwen3.5-397B-A17B\n\n\n\n\n\n\nSTEM and Puzzle\n\n\n\n\nMMMU\n\n\n86.7\n\n\n80.7\n\n\n87.2\n\n\n80.6\n\n\n84.3\n\n\n85.0\n\n\n\n\n\n\nMMMU-Pro\n\n\n79.5\n\n\n70.6\n\n\n81.0\n\n\n69.3\n\n\n78.5\n\n\n79.0\n\n\n\n\n\n\nMathVision\n\n\n83.0\n\n\n74.3\n\n\n86.6\n\n\n74.6\n\n\n84.2\n\n\n88.6\n\n\n\n\n\n\nMathvista(mini)\n\n\n83.1\n\n\n80.0\n\n\n87.9\n\n\n85.8\n\n\n90.1\n\n\n90.3\n\n\n\n\n\n\nWe-Math\n\n\n79.0\n\n\n70.0\n\n\n86.9\n\n\n74.8\n\n\n84.7\n\n\n87.9\n\n\n\n\n\n\nDynaMath\n\n\n86.8\n\n\n79.7\n\n\n85.1\n\n\n82.8\n\n\n84.4\n\n\n86.3\n\n\n\n\n\n\nZEROBench\n\n\n9\n\n\n3\n\n\n10\n\n\n4\n\n\n9\n\n\n12\n\n\n\n\n\n\nZEROBench_sub\n\n\n33.2\n\n\n28.4\n\n\n39.0\n\n\n28.4\n\n\n33.5\n\n\n41.0\n\n\n\n\n\n\nBabyVision\n\n\n34.4\n\n\n14.2\n\n\n49.7\n\n\n22.2\n\n\n36.5\n\n\n52.\n3\n⁄\n43\n.3\n\n\n\n\nGeneral VQA\n\n\n\n\nRealWorldQA\n\n\n83.3\n\n\n77.0\n\n\n83.3\n\n\n81.3\n\n\n81.0\n\n\n83.9\n\n\n\n\n\n\nMMStar\n\n\n77.1\n\n\n73.2\n\n\n83.1\n\n\n78.7\n\n\n80.5\n\n\n83.8\n\n\n\n\n\n\nHallusionBench\n\n\n65.2\n\n\n64.1\n\n\n68.6\n\n\n66.7\n\n\n69.8\n\n\n71.4\n\n\n\n\n\n\nMMBench\nEN-DEV-v1.1\n\n\n88.2\n\n\n89.2\n\n\n93.7\n\n\n89.7\n\n\n94.2\n\n\n93.7\n\n\n\n\n\n\nSimpleVQA\n\n\n55.8\n\n\n65.7\n\n\n73.2\n\n\n61.3\n\n\n71.2\n\n\n67.1\n\n\n\n\nText Recognition and Document Understanding\n\n\n\n\nOmniDocBench1.5\n\n\n85.7\n\n\n87.7\n\n\n88.5\n\n\n84.5\n\n\n88.8\n\n\n90.8\n\n\n\n\n\n\nCharXiv(RQ)\n\n\n82.1\n\n\n68.5\n\n\n81.4\n\n\n66.1\n\n\n77.5\n\n\n80.8\n\n\n\n\n\n\nMMLongBench-Doc\n\n\n–\n\n\n61.9\n\n\n60.5\n\n\n56.2\n\n\n58.5\n\n\n61.5\n\n\n\n\n\n\nCC-OCR\n\n\n70.3\n\n\n76.9\n\n\n79.0\n\n\n81.5\n\n\n79.7\n\n\n82.0\n\n\n\n\n\n\nAI2D_TEST\n\n\n92.2\n\n\n87.7\n\n\n94.1\n\n\n89.2\n\n\n90.8\n\n\n93.9\n\n\n\n\n\n\nOCRBench\n\n\n80.7\n\n\n85.8\n\n\n90.4\n\n\n87.5\n\n\n92.3\n\n\n93.1\n\n\n\n\nSpatial Intelligence\n\n\n\n\nERQA\n\n\n59.8\n\n\n46.8\n\n\n70.5\n\n\n52.5\n\n\n–\n\n\n67.5\n\n\n\n\n\n\nCountBench\n\n\n91.9\n\n\n90.6\n\n\n97.3\n\n\n93.7\n\n\n94.1\n\n\n97.2\n\n\n\n\n\n\nRefCOCO(avg)\n\n\n–\n\n\n–\n\n\n84.1\n\n\n91.1\n\n\n87.8\n\n\n92.3\n\n\n\n\n\n\nODInW13\n\n\n–\n\n\n–\n\n\n46.3\n\n\n43.2\n\n\n–\n\n\n47.0\n\n\n\n\n\n\nEmbSpatialBench\n\n\n81.3\n\n\n75.7\n\n\n61.2\n\n\n84.3\n\n\n77.4\n\n\n84.5\n\n\n\n\n\n\nRefSpatialBench\n\n\n–\n\n\n–\n\n\n65.5\n\n\n69.9\n\n\n–\n\n\n73.6\n\n\n\n\n\n\nLingoQA\n\n\n68.8\n\n\n78.8\n\n\n72.8\n\n\n66.8\n\n\n68.2\n\n\n81.6\n\n\n\n\n\n\nV*\n\n\n75.9\n\n\n67.0\n\n\n88.0\n\n\n85.9\n\n\n77.0\n\n\n95.\n8\n⁄\n91\n.1\n\n\n\n\n\n\nHypersim\n\n\n–\n\n\n–\n\n\n–\n\n\n11.0\n\n\n–\n\n\n12.5\n\n\n\n\n\n\nSUNRGBD\n\n\n–\n\n\n–\n\n\n–\n\n\n34.9\n\n\n–\n\n\n38.3\n\n\n\n\n\n\nNuscene\n\n\n–\n\n\n–\n\n\n–\n\n\n13.9\n\n\n–\n\n\n16.0\n\n\n\n\nVideo Understanding\n\n\n\n\nVideoMME\n(w sub.)\n\n\n86\n\n\n77.6\n\n\n88.4\n\n\n83.8\n\n\n87.4\n\n\n87.5\n\n\n\n\n\n\nVideoMME\n(w/o sub.)\n\n\n85.8\n\n\n81.4\n\n\n87.7\n\n\n79.0\n\n\n83.2\n\n\n83.7\n\n\n\n\n\n\nVideoMMMU\n\n\n85.9\n\n\n84.4\n\n\n87.6\n\n\n80.0\n\n\n86.6\n\n\n84.7\n\n\n\n\n\n\nMLVU (M-Avg)\n\n\n85.6\n\n\n81.7\n\n\n83.0\n\n\n83.8\n\n\n85.0\n\n\n86.7\n\n\n\n\n\n\nMVBench\n\n\n78.1\n\n\n67.2\n\n\n74.1\n\n\n75.2\n\n\n73.5\n\n\n77.6\n\n\n\n\n\n\nLVBench\n\n\n73.7\n\n\n57.3\n\n\n76.2\n\n\n63.6\n\n\n75.9\n\n\n75.5\n\n\n\n\n\n\nMMVU\n\n\n80.8\n\n\n77.3\n\n\n77.5\n\n\n71.1\n\n\n80.4\n\n\n75.4\n\n\n\n\nVisual Agent\n\n\n\n\nScreenSpot Pro\n\n\n–\n\n\n45.7\n\n\n72.7\n\n\n62.0\n\n\n–\n\n\n65.6\n\n\n\n\n\n\nOSWorld-Verified\n\n\n38.2\n\n\n66.3\n\n\n–\n\n\n38.1\n\n\n63.3\n\n\n62.2\n\n\n\n\n\n\nAndroidWorld\n\n\n–\n\n\n–\n\n\n–\n\n\n63.7\n\n\n–\n\n\n66.8\n\n\n\n\nMedical VQA\n\n\n\n\nSLAKE\n\n\n76.9\n\n\n76.4\n\n\n81.3\n\n\n54.7\n\n\n81.6\n\n\n79.9\n\n\n\n\n\n\nPMC-VQA\n\n\n58.9\n\n\n59.9\n\n\n62.3\n\n\n41.2\n\n\n63.3\n\n\n64.2\n\n\n\n\n\n\nMedXpertQA-MM\n\n\n73.3\n\n\n63.6\n\n\n76.0\n\n\n47.6\n\n\n65.3\n\n\n70.0\n\n\n\n\n\n\n\n\n\n* MathVision：our model’s score is evaluated using a fixed prompt, e.g., “Please reason step by step, and put your final answer within \\boxed{}.” For other models, we report the higher score between runs with and without the \\boxed{} formatting.\n\n* BabyVision: our model’s score is reported with CI (Code Interpreter) enabled; without CI, the result is 43.3.\n\n* V*: our model’s score is reported with CI (Code Interpreter) enabled; without CI, the result is 91.1.\n\n* Empty cells (--) indicate scores not yet available or not applicable.",
    "capabilities": [
      "Vision",
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "35b",
      "122b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3.5:35b",
        "size": "24GB",
        "size_gb": 24.0,
        "recommended_ram_gb": 30.0,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3.5:122b",
        "size": "81GB",
        "size_gb": 81.0,
        "recommended_ram_gb": 101.2,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 24.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Code Generation",
      "Reasoning",
      "Image Understanding"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers for multimodal tasks.",
    "pulls": 29800,
    "tags": 7,
    "last_updated": "2026-02-25",
    "last_updated_str": "11 hours ago",
    "timestamp": "2026-02-25T19:30:45.885155"
  },
  {
    "id": "2e942df8-eb88-4383-a76a-5ee6a8ca817e",
    "model_identifier": "lfm2",
    "model_name": "lfm2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/lfm2",
    "description": "LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family, scaling the architecture to 24 billion parameters while keeping inference efficient.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family, scaling the architecture to 24 billion parameters while keeping inference efficient.\n\n\n\n\nBest-in-class efficiency\n: A 24B MoE model with only 2B active parameters per token, fitting in 32 GB of RAM for deployment on consumer laptops and desktops.\n\n\nFast edge inference\n: 112 tok/s decode on AMD CPU, 293 tok/s on H100. Fits in 32B GB of RAM.\n\n\nPredictable scaling\n: Quality improves log-linearly from 350M to 24B total parameters, confirming the LFM2 hybrid architecture scales reliably across nearly two orders of magnitude.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![image.png](/assets/library/lfm2/ddca3f1a-b232-44eb-ba95-236f15edd6a6)\n\nLFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family, scaling the architecture to 24 billion parameters while keeping inference efficient.\n\n- **Best-in-class efficiency**: A 24B MoE model with only 2B active parameters per token, fitting in 32 GB of RAM for deployment on consumer laptops and desktops.\n- **Fast edge inference**: 112 tok/s decode on AMD CPU, 293 tok/s on H100. Fits in 32B GB of RAM.\n- **Predictable scaling**: Quality improves log-linearly from 350M to 24B total parameters, confirming the LFM2 hybrid architecture scales reliably across nearly two orders of magnitude.\n\n![image.png](/assets/library/lfm2/276ea517-7960-43b4-bc4d-d0d2dcbaf97b)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "24b"
    ],
    "memory_requirements": [
      {
        "tag": "lfm2:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 14.0,
    "use_cases": [
      "Text Summarization",
      "Question Answering",
      "Translation",
      "Code Generation"
    ],
    "domain": "Language",
    "ai_languages": [
      "English"
    ],
    "complexity": "advanced",
    "best_for": "Developers and Researchers for advanced language tasks.",
    "pulls": 754,
    "tags": 6,
    "last_updated": "2026-02-25",
    "last_updated_str": "yesterday",
    "timestamp": "2026-02-25T19:30:45.885155"
  }
]