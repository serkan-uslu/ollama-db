[
  {
    "id": "1dd26567-05d2-42c4-b69f-03315c79cb1b",
    "model_identifier": "llama3.1",
    "model_name": "llama3.1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.1",
    "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nMeta Llama 3.1\n\n\n\n\nLlama 3.1\n family of models available:\n\n\n\n\n8B\n\n\n70B\n\n\n405B\n\n\n\n\nLlama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation.\n\n\nThe upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta’s latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants.\n\n\nMeta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models.\n\n\nModel evaluations\n\n\nFor this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta’s experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta’s smaller models are competitive with closed and open models that have a similar number of parameters.\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nMeta AI Llama 3.1 launch blog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Meta Llama 3.1 \n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/4d0cab8e-952b-4c75-b110-1514d8db8fae)\n\n**Llama 3.1** family of models available: \n\n- **8B**\n-  **70B**\n-  **405B**\n\nLlama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. \n\nThe upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta's latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. \n\nMeta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models. \n\n### Model evaluations \n\nFor this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta's experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta's smaller models are competitive with closed and open models that have a similar number of parameters.\n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/73b11a5e-84e9-4397-9d47-f0299a6294b3)\n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/ad042a1c-bbc7-47de-bbbf-78a3cfc13485)\n\n![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/2d582df5-ce45-4326-85c5-254c917554b2)\n\n### References \n- [Meta AI Llama 3.1 launch blog post](https://ai.meta.com/blog/meta-llama-3-1/) \n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "8b",
      "70b",
      "405b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.1:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "llama3.1:70b",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "llama3.1:405b",
        "size": "243GB",
        "size_gb": 243.0,
        "recommended_ram_gb": 303.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 4.9,
    "context_window": 128000,
    "speed_tier": "medium",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Text Summarization",
      "Translation",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers looking for a state-of-the-art model for general knowledge, multilingual translation, and tool use.",
    "model_family": "Llama",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": "Llama 3.1 Community License",
    "strengths": [
      "State-of-the-art general knowledge",
      "Strong multilingual translation capabilities",
      "Advanced tool use",
      "Long context length",
      "Stronger reasoning capabilities"
    ],
    "limitations": [
      "Requires significant computational resources",
      "May not be suitable for low-RAM devices"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 110500000,
    "tags": 93,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "44bb714f-65b9-4c4e-af3e-01e85480934f",
    "model_identifier": "deepseek-r1",
    "model_name": "deepseek-r1",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-r1",
    "description": "DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n\n\n\n\nModels\n\n\nDeepSeek-R1-0528-Qwen3-8B\n\n\nollama run deepseek-r1\n\n\n\nDeepSeek-R1\n\n\nollama run deepseek-r1:671b\n\n\n\n\n\nNote: to update the model from an older version, run \nollama pull deepseek-r1\n\n\n\n\nDistilled models\n\n\nDeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.\n\n\nBelow are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.\n\n\nDeepSeek-R1-0528-Qwen3-8B\n\n\nollama run deepseek-r1:8b\n\n\n\nDeepSeek-R1-Distill-Qwen-1.5B\n\n\nollama run deepseek-r1:1.5b\n\n\n\nDeepSeek-R1-Distill-Qwen-7B\n\n\nollama run deepseek-r1:7b\n\n\n\nDeepSeek-R1-Distill-Qwen-14B\n\n\nollama run deepseek-r1:14b\n\n\n\nDeepSeek-R1-Distill-Qwen-32B\n\n\nollama run deepseek-r1:32b\n\n\n\nDeepSeek-R1-Distill-Llama-70B\n\n\nollama run deepseek-r1:70b\n\n\n\nLicense\n\n\nThe model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n\n\nThe Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\n\n\nThe Llama 8B distilled model is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\n\n\nThe Llama 70B distilled model is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" />\n\nDeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n\n![image.png](/assets/library/deepseek-r1/ba9ac535-ac57-4d42-9f36-20067c8eaa50)\n\n## Models\n\n**DeepSeek-R1-0528-Qwen3-8B**\n\n```\nollama run deepseek-r1\n```\n\n**DeepSeek-R1**\n\n```\nollama run deepseek-r1:671b\n```\n\n> Note: to update the model from an older version, run `ollama pull deepseek-r1`\n\n### Distilled models \n\nDeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. \n\nBelow are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. \n\n**DeepSeek-R1-0528-Qwen3-8B** \n\n```\nollama run deepseek-r1:8b\n```\n\n**DeepSeek-R1-Distill-Qwen-1.5B**   \n\n```\nollama run deepseek-r1:1.5b\n```\n\n**DeepSeek-R1-Distill-Qwen-7B** \n\n```\nollama run deepseek-r1:7b\n```\n\n**DeepSeek-R1-Distill-Qwen-14B** \n\n```\nollama run deepseek-r1:14b\n```\n\n**DeepSeek-R1-Distill-Qwen-32B** \n\n```\nollama run deepseek-r1:32b\n```\n\n**DeepSeek-R1-Distill-Llama-70B**  \n\n```\nollama run deepseek-r1:70b\n```\n\n\n\n\n### License \n\nThe model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n\nThe Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\n\nThe Llama 8B distilled model is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license. \n\nThe Llama 70B distilled model is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "1.5b",
      "7b",
      "8b",
      "14b",
      "32b",
      "70b",
      "671b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-r1:1.5b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:7b",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:70b",
        "size": "43GB",
        "size_gb": 43.0,
        "recommended_ram_gb": 53.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "deepseek-r1:671b",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      }
    ],
    "min_ram_gb": 1.1,
    "context_window": 160000,
    "speed_tier": "fast",
    "use_cases": [
      "Reasoning",
      "Math",
      "Code Generation",
      "Code Review",
      "Code Explanation"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "DeepSeek-R1 is ideal for tasks that require advanced reasoning, mathematics, and programming capabilities, comparable to leading models like O3 and Gemini 2.5 Pro.",
    "model_family": "DeepSeek",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": "DeepSeek License",
    "strengths": [
      "Advanced reasoning capabilities",
      "High performance in mathematics and programming",
      "Approaching performance of leading models like O3 and Gemini 2.5 Pro"
    ],
    "limitations": [
      "Requires significant computational resources for larger models",
      "May have limited performance in areas outside of mathematics and programming"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists",
      "Students"
    ],
    "pulls": 78600000,
    "tags": 35,
    "last_updated": "2025-07-26",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "c3f93e14-fcb7-42f5-ba78-e8008b1142ff",
    "model_identifier": "llama3.2",
    "model_name": "llama3.2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.2",
    "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n\nSizes\n\n\n3B parameters (default)\n\n\nThe 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as:\n\n\n\n\nFollowing instructions\n\n\nSummarization\n\n\nPrompt rewriting\n\n\nTool use\n\n\n\n\nollama run llama3.2\n\n\n\n1B parameters\n\n\nThe 1B model is competitive with other 1-3B parameter models. It’s use cases include:\n\n\n\n\nPersonal information management\n\n\nMultilingual knowledge retrieval\n\n\nRewriting tasks running locally on edge\n\n\n\n\nollama run llama3.2:1b\n\n\n\nBenchmarks\n\n\n\n\nSupported Languages:\n English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/llama3.2/be01fadf-7fbd-404d-929b-50a77249b030\" width=\"280\" />\n\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n## Sizes\n\n### 3B parameters (default)\n\nThe 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as:\n\n* Following instructions\n* Summarization\n* Prompt rewriting\n* Tool use\n\n```\nollama run llama3.2\n```\n\n### 1B parameters\n\nThe 1B model is competitive with other 1-3B parameter models. It's use cases include:\n\n* Personal information management\n* Multilingual knowledge retrieval\n* Rewriting tasks running locally on edge\n\n```\nollama run llama3.2:1b\n```\n\n### Benchmarks\n\n![Llama 3.2 instruction-tuned benchmarks](https://ollama.com/assets/library/llama3.2/c1a51716-d8bb-4642-8044-48f5022b777d)\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "1b",
      "3b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.2:1b",
        "size": "1.3GB",
        "size_gb": 1.3,
        "recommended_ram_gb": 1.6,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "llama3.2:latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "2.0GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 1.3,
    "context_window": 128000,
    "speed_tier": "fast",
    "use_cases": [
      "Chat Assistant",
      "Text Summarization",
      "Role Play",
      "Creative Writing",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Multilingual dialogue and text-based tasks such as summarization and prompt rewriting.",
    "model_family": "Llama",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Multilingual support",
      "Instruction-tuned for dialogue",
      "Competitive performance on industry benchmarks"
    ],
    "limitations": [
      "Limited to text-based tasks",
      "May not perform well on non-dialogue tasks"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Students"
    ],
    "pulls": 58000000,
    "tags": 63,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "61779516-4284-4873-affc-c24855fae7fd",
    "model_identifier": "nomic-embed-text",
    "model_name": "nomic-embed-text",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/nomic-embed-text",
    "description": "A high-performing open embedding model with a large token context window.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.1.26 or later. \nDownload it here\n. It can only be used to generate embeddings.\n\n\n\n\nnomic-embed-text\n is a large context length text encoder that surpasses OpenAI \ntext-embedding-ada-002\n and \ntext-embedding-3-small\n performance on short and long context tasks.\n\n\nUsage\n\n\nThis model is an embedding model, meaning it can only be used to generate embeddings.\n\n\nREST API\n\n\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\n\n\n\nPython library\n\n\nollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering')\n\n\n\nJavascript library\n\n\nollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' })\n\n\n\nReferences\n\n\nHuggingFace\n\n\nBlog Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![nomic_logo](https://github.com/ollama/ollama/assets/251292/bf242e43-3c1a-4590-887d-abcab76cb304)\n\n\n> Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). It can only be used to generate embeddings.\n\n`nomic-embed-text` is a large context length text encoder that surpasses OpenAI `text-embedding-ada-002` and `text-embedding-3-small` performance on short and long context tasks.\n\n## Usage\n\nThis model is an embedding model, meaning it can only be used to generate embeddings.\n\n### REST API\n\n```\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\n```\n\n### Python library\n\n```\nollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering')\n```\n\n### Javascript library\n\n```\nollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' })\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)\n\n[Blog Post](https://blog.nomic.ai/posts/nomic-embed-text-v1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [],
    "memory_requirements": [],
    "min_ram_gb": null,
    "context_window": null,
    "speed_tier": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Generating high-quality text embeddings for various NLP tasks.",
    "model_family": "Nomic",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "High-performance text embedding",
      "Large token context window",
      "Surpasses other models in short and long context tasks"
    ],
    "limitations": [
      "Can only be used to generate embeddings",
      "Requires Ollama 0.1.26 or later"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 54800000,
    "tags": 3,
    "last_updated": "2024-02-26",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "b82c82d1-7473-4c98-9618-2ca6c2fff58b",
    "model_identifier": "gemma3",
    "model_name": "gemma3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma3",
    "description": "The current, most capable model that runs on a single GPU.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nThis model requires Ollama 0.6 or later. \nDownload Ollama\n\n\n\n\nGemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices.\n\n\nModels\n\n\nText\n\n\n270M parameter model\n (32k context window)\n\n\nollama run gemma3:270m\n\n\n\n1B parameter model\n (32k context window)\n\n\nollama run gemma3:1b \n\n\n\nMultimodal (Vision)\n\n\n4B parameter model\n (128k context window)\n\n\nollama run gemma3:4b\n\n\n\n12B parameter model\n (128k context window)\n\n\nollama run gemma3:12b\n\n\n\n27B parameter model\n (128k context window)\n\n\nollama run gemma3:27b\n\n\n\nQuantization aware trained models (QAT)\n\n\nThe quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models).\n\n\n1B parameter model\n\n\nollama run gemma3:1b-it-qat\n\n\n\n4B parameter model\n\n\nollama run gemma3:4b-it-qat\n\n\n\n12B parameter model\n\n\nollama run gemma3:12b-it-qat\n\n\n\n27B parameter model\n\n\nollama run gemma3:27b-it-qat\n\n\n\nEvaluation\n\n\n\n\nBenchmark Results\n\n\nGemma 3 270M\n\n\n\n\n\n\n\n\nBenchmark\n\n\nn-shot\n\n\nGemma 3 270m instruction tuned\n\n\n\n\n\n\n\n\n\n\nHellaSwag\n\n\n0-shot\n\n\n37.7\n\n\n\n\n\n\nPIQA\n\n\n0-shot\n\n\n66.2\n\n\n\n\n\n\nARC-c\n\n\n0-shot\n\n\n28.2\n\n\n\n\n\n\nWinoGrande\n\n\n0-shot\n\n\n52.3\n\n\n\n\n\n\nBIG-Bench Hard\n\n\nfew-shot\n\n\n26.7\n\n\n\n\n\n\nIF Eval\n\n\n0-shot\n\n\n51.2\n\n\n\n\n\n\n\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n\nReasoning, logic and code capabilities\n\n\n\n\n\n\n\n\nBenchmark\n\n\nMetric\n\n\nGemma 3 PT 1B\n\n\nGemma 3 PT 4B\n\n\nGemma 3 PT 12B\n\n\nGemma 3 PT 27B\n\n\n\n\n\n\n\n\n\n\nHellaSwag\n\n\n10-shot\n\n\n62.3\n\n\n77.2\n\n\n84.2\n\n\n85.6\n\n\n\n\n\n\nBoolQ\n\n\n0-shot\n\n\n63.2\n\n\n72.3\n\n\n78.8\n\n\n82.4\n\n\n\n\n\n\nPIQA\n\n\n0-shot\n\n\n73.8\n\n\n79.6\n\n\n81.8\n\n\n83.3\n\n\n\n\n\n\nSocialIQA\n\n\n0-shot\n\n\n48.9\n\n\n51.9\n\n\n53.4\n\n\n54.9\n\n\n\n\n\n\nTriviaQA\n\n\n5-shot\n\n\n39.8\n\n\n65.8\n\n\n78.2\n\n\n85.5\n\n\n\n\n\n\nNatural Questions\n\n\n5-shot\n\n\n9.48\n\n\n20.0\n\n\n31.4\n\n\n36.1\n\n\n\n\n\n\nARC-c\n\n\n25-shot\n\n\n38.4\n\n\n56.2\n\n\n68.9\n\n\n70.6\n\n\n\n\n\n\nARC-e\n\n\n0-shot\n\n\n73.0\n\n\n82.4\n\n\n88.3\n\n\n89.0\n\n\n\n\n\n\nWinoGrande\n\n\n5-shot\n\n\n58.2\n\n\n64.7\n\n\n74.3\n\n\n78.8\n\n\n\n\n\n\nBIG-Bench Hard\n\n\n\n\n28.4\n\n\n50.9\n\n\n72.6\n\n\n77.7\n\n\n\n\n\n\nDROP\n\n\n3-shot, F1\n\n\n42.4\n\n\n60.1\n\n\n72.2\n\n\n77.2\n\n\n\n\n\n\nAGIEval\n\n\n3-5-shot\n\n\n22.2\n\n\n42.1\n\n\n57.4\n\n\n66.2\n\n\n\n\n\n\nMMLU\n\n\n5-shot, top-1\n\n\n26.5\n\n\n59.6\n\n\n74.5\n\n\n78.6\n\n\n\n\n\n\nMATH\n\n\n4-shot\n\n\n–\n\n\n24.2\n\n\n43.3\n\n\n50.0\n\n\n\n\n\n\nGSM8K\n\n\n5-shot, maj@1\n\n\n1.36\n\n\n38.4\n\n\n71.0\n\n\n82.6\n\n\n\n\n\n\nGPQA\n\n\n\n\n9.38\n\n\n15.0\n\n\n25.4\n\n\n24.3\n\n\n\n\n\n\nMMLU\n (Pro)\n\n\n5-shot\n\n\n11.2\n\n\n23.7\n\n\n40.8\n\n\n43.9\n\n\n\n\n\n\nMBPP\n\n\n3-shot\n\n\n9.80\n\n\n46.0\n\n\n60.4\n\n\n65.6\n\n\n\n\n\n\nHumanEval\n\n\npass@1\n\n\n6.10\n\n\n36.0\n\n\n45.7\n\n\n48.8\n\n\n\n\n\n\nMMLU\n (Pro COT)\n\n\n5-shot\n\n\n9.7\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n\n\n\nMultilingual capabilities\n\n\n\n\n\n\n\n\nBenchmark\n\n\nGemma 3 PT 1B\n\n\nGemma 3 PT 4B\n\n\nGemma 3 PT 12B\n\n\nGemma 3 PT 27B\n\n\n\n\n\n\n\n\n\n\nMGSM\n\n\n2.04\n\n\n34.7\n\n\n64.3\n\n\n74.3\n\n\n\n\n\n\nGlobal-MMLU-Lite\n\n\n24.9\n\n\n57.0\n\n\n69.4\n\n\n75.7\n\n\n\n\n\n\nBelebele\n\n\n26.6\n\n\n59.4\n\n\n78.0\n\n\n–\n\n\n\n\n\n\nWMT24++\n (ChrF)\n\n\n36.7\n\n\n48.4\n\n\n53.9\n\n\n55.7\n\n\n\n\n\n\nFloRes\n\n\n29.5\n\n\n39.2\n\n\n46.0\n\n\n48.8\n\n\n\n\n\n\nXL-Sum\n\n\n4.82\n\n\n8.55\n\n\n12.2\n\n\n14.9\n\n\n\n\n\n\nXQuAD\n (all)\n\n\n43.9\n\n\n68.0\n\n\n74.5\n\n\n76.8\n\n\n\n\n\n\n\n\nMultimodal capabilities\n\n\n\n\n\n\n\n\nBenchmark\n\n\nGemma 3 PT 4B\n\n\nGemma 3 PT 12B\n\n\nGemma 3 PT 27B\n\n\n\n\n\n\n\n\n\n\nCOCOcap\n\n\n102\n\n\n111\n\n\n116\n\n\n\n\n\n\nDocVQA\n (val)\n\n\n72.8\n\n\n82.3\n\n\n85.6\n\n\n\n\n\n\nInfoVQA\n (val)\n\n\n44.1\n\n\n54.8\n\n\n59.4\n\n\n\n\n\n\nMMMU\n (pt)\n\n\n39.2\n\n\n50.3\n\n\n56.1\n\n\n\n\n\n\nTextVQA\n (val)\n\n\n58.9\n\n\n66.5\n\n\n68.6\n\n\n\n\n\n\nRealWorldQA\n\n\n45.5\n\n\n52.2\n\n\n53.9\n\n\n\n\n\n\nReMI\n\n\n27.3\n\n\n38.5\n\n\n44.8\n\n\n\n\n\n\nAI2D\n\n\n63.2\n\n\n75.2\n\n\n79.0\n\n\n\n\n\n\nChartQA\n\n\n45.4\n\n\n60.9\n\n\n63.8\n\n\n\n\n\n\nChartQA\n (augmented)\n\n\n81.8\n\n\n88.5\n\n\n88.7\n\n\n\n\n\n\nVQAv2\n\n\n–\n\n\n–\n\n\n–\n\n\n\n\n\n\nBLINK\n\n\n38.0\n\n\n35.9\n\n\n39.6\n\n\n\n\n\n\nOKVQA\n\n\n51.0\n\n\n58.7\n\n\n60.2\n\n\n\n\n\n\nTallyQA\n\n\n42.5\n\n\n51.8\n\n\n54.3\n\n\n\n\n\n\nSpatialSense VQA\n\n\n50.9\n\n\n60.0\n\n\n59.4\n\n\n\n\n\n\nCountBenchQA\n\n\n26.1\n\n\n17.8\n\n\n68.0\n\n\n\n\n\n\n\n\nReference\n\n\nGemma Terms of Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Google Gemma 3 logo](/assets/library/gemma3/b54bf767-f9c5-4284-b551-a49aebe3a3c2)\n\n> This model requires Ollama 0.6 or later. [Download Ollama](https://ollama.com/download)\n\nGemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices.\n\n## Models \n\n### Text\n\n**270M parameter model** (32k context window) \n\n```\nollama run gemma3:270m\n```\n**1B parameter model** (32k context window)\n\n```\nollama run gemma3:1b \n```\n\n### Multimodal (Vision) \n\n**4B parameter model** (128k context window)  \n\n```\nollama run gemma3:4b\n```\n\n**12B parameter model** (128k context window)\n\n```\nollama run gemma3:12b\n```\n\n**27B parameter model** (128k context window)\n\n```\nollama run gemma3:27b\n```\n\n### Quantization aware trained models (QAT) \nThe quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models).   \n\n**1B parameter model**\n```\nollama run gemma3:1b-it-qat\n```\n\n**4B parameter model**\n```\nollama run gemma3:4b-it-qat\n```\n\n**12B parameter model**\n```\nollama run gemma3:12b-it-qat\n```\n\n**27B parameter model**\n```\nollama run gemma3:27b-it-qat\n```\n\n## Evaluation\n\n![Chatbot Arena ELO Score](/assets/library/gemma3/89dc5a19-179e-4dd3-8e5d-12ad54973148)\n\n### Benchmark Results\n\n**Gemma 3 270M**\n\n| **Benchmark**             |  **n-shot**   | **Gemma 3 270m instruction tuned** |\n| :------------------------ | :-----------: | ------------------: |\n| [HellaSwag][hellaswag]    |    0-shot     |                37.7 |\n| [PIQA][piqa]              |    0-shot     |                66.2 |\n| [ARC-c][arc]              |    0-shot     |                28.2 |\n| [WinoGrande][winogrande]  |    0-shot     |                52.3 |\n| [BIG-Bench Hard][bbh]     |   few-shot    |                26.7 |\n| [IF Eval][ifeval]         |    0-shot     |                51.2 |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[piqa]: https://arxiv.org/abs/1911.11641\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[bbh]: https://paperswithcode.com/dataset/bbh\n[ifeval]: https://arxiv.org/abs/2311.07911\n\n\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning, logic and code capabilities\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |     77.2      |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |     72.3      |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |     79.6      |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |     51.9      |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |     65.8      |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |     20.0      |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |     56.2      |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |     82.4      |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |     64.7      |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          |                |      28.4      |     50.9      |      72.6      |      77.7      |\n| [DROP][drop]                   | 3-shot, F1     |      42.4      |     60.1      |      72.2      |      77.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      22.2      |     42.1      |      57.4      |      66.2      |\n| [MMLU][mmlu]                   | 5-shot, top-1  |      26.5      |     59.6      |      74.5      |      78.6      |\n| [MATH][math]                   | 4-shot         |       --       |     24.2      |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 5-shot, maj@1  |      1.36      |     38.4      |      71.0      |      82.6      |\n| [GPQA][gpqa]                   |                |      9.38      |     15.0      |      25.4      |      24.3      |\n| [MMLU][mmlu] (Pro)             | 5-shot         |      11.2      |     23.7      |      40.8      |      43.9      |\n| [MBPP][mbpp]                   | 3-shot         |      9.80      |     46.0      |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | pass@1         |      6.10      |     36.0      |      45.7      |      48.8      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      9.7       |     NaN       |      NaN       |      NaN       |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bb",
    "capabilities": [
      "Vision",
      "Cloud"
    ],
    "capability": "Vision",
    "labels": [
      "270m",
      "1b",
      "4b",
      "12b",
      "27b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma3:latest",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "3.3GB",
        "size_gb": 3.3,
        "recommended_ram_gb": 4.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "gemma3:12b",
        "size": "8.1GB",
        "size_gb": 8.1,
        "recommended_ram_gb": 10.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "gemma3:27b",
        "size": "17GB",
        "size_gb": 17.0,
        "recommended_ram_gb": 21.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 3.3,
    "context_window": 128000,
    "speed_tier": "medium",
    "use_cases": [
      "Question Answering",
      "Text Summarization",
      "Reasoning"
    ],
    "domain": "Multimodal",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "The Gemma 3 model is best for users who need a lightweight, multimodal model that can process text and images and be deployed on resource-limited devices.",
    "model_family": "Gemma",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Multimodal processing",
      "Compact design",
      "Support for over 140 languages"
    ],
    "limitations": [
      "Requires Ollama 0.6 or later"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Content Creators"
    ],
    "pulls": 32100000,
    "tags": 29,
    "last_updated": "2025-12-26",
    "last_updated_str": "2 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "34fe0675-8782-4668-8636-e59b9b0f9fdc",
    "model_identifier": "mistral",
    "model_name": "mistral",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral",
    "description": "The 7B model released by Mistral AI, updated to version 0.3.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion.\n\n\nThe Mistral AI team has noted that Mistral 7B:\n\n\n\n\nOutperforms Llama 2 13B on all benchmarks\n\n\nOutperforms Llama 1 34B on many benchmarks\n\n\nApproaches CodeLlama 7B performance on code, while remaining good at English tasks\n\n\n\n\nVersions\n\n\n\n\n\n\n\n\nTag\n\n\nDate\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nv0.3\n \nlatest\n\n\n05/22/2024\n\n\nA new version of Mistral 7B that supports function calling.\n\n\n\n\n\n\nv0.2\n\n\n03/23/2024\n\n\nA minor release of Mistral 7B\n\n\n\n\n\n\nv0.1\n\n\n09/27/2023\n\n\nInitial release\n\n\n\n\n\n\n\n\nFunction calling\n\n\nMistral 0.3 supports function calling with Ollama’s \nraw mode\n.\n\n\nExample raw prompt\n\n\n[AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST]\n\n\n\nExample response\n\n\n[TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}]\n\n\n\nFor more information on raw mode, see the \nAPI documentation\n.\n\n\nVariations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstruct\n\n\nInstruct models follow instructions\n\n\n\n\n\n\ntext\n\n\nText models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion.\n\n\n\n\n\n\n\n\nUsage\n\n\nCLI\n\n\nInstruct:\n\n\nollama run mistral\n\n\n\nAPI\n\n\nExample:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n\n\n\nReferences\n\n\nHuggingFace\n\n\nMistral AI News Release\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171\" width=\"200\"/>\n\nMistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion.\n\nThe Mistral AI team has noted that Mistral 7B:\n\n- Outperforms Llama 2 13B on all benchmarks\n- Outperforms Llama 1 34B on many benchmarks\n- Approaches CodeLlama 7B performance on code, while remaining good at English tasks\n\n### Versions\n\n| Tag             | Date       | Notes                                                       |\n| --------------- | ---------- | ----------------------------------------------------------- |\n| `v0.3` `latest` | 05/22/2024 | A new version of Mistral 7B that supports function calling. |\n| `v0.2`          | 03/23/2024 | A minor release of Mistral 7B                           |\n| `v0.1`          | 09/27/2023 | Initial release                                             |\n\n### Function calling\n\nMistral 0.3 supports function calling with Ollama's **raw mode**.\n\n<sub>Example raw prompt</sub>\n```\n[AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST]\n```\n\n<sub>Example response</sub>\n\n```\n[TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}]\n```\n\nFor more information on raw mode, see the [API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md#request-raw-mode).\n\n### Variations\n\n|            |                                                                                                                                    |\n| ---------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n| `instruct` | Instruct models follow instructions                                                                                                |\n| `text`     | Text models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion. |\n\n## Usage\n\n### CLI\n\nInstruct:\n\n```\nollama run mistral\n```\n\n### API\n\nExample:\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\n```\n\n## References\n\n[HuggingFace](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n\n[Mistral AI News Release](https://mistral.ai/news/announcing-mistral-7b/)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral:latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.4,
    "context_window": 32000,
    "speed_tier": "medium",
    "use_cases": [
      "Code Generation",
      "Function Calling",
      "Code Explanation"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Mistral 7B is ideal for users who need a versatile model that excels at code generation, function calling, and text-related tasks while being efficient in terms of RAM requirements.",
    "model_family": "Mistral",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": "Apache",
    "strengths": [
      "Outperforms larger models like Llama 2 13B and Llama 1 34B on various benchmarks",
      "Approaches CodeLlama 7B performance on code tasks",
      "Remains proficient in English tasks",
      "Efficient, requiring less RAM"
    ],
    "limitations": [
      "May not perform as well as larger models in certain specialized tasks",
      "Dependence on Ollama’s raw mode for function calling",
      "Limited to the capabilities of a 7B parameter model"
    ],
    "target_audience": [
      "Developers",
      "Data Scientists",
      "Students",
      "Content Creators"
    ],
    "pulls": 25600000,
    "tags": 84,
    "last_updated": "2025-07-26",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "3687d0a5-5e7a-499b-8262-72730cbfbb1b",
    "model_identifier": "qwen2.5",
    "model_name": "qwen2.5",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2.5",
    "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2:\n\n\n\n\nIt possesses \nsignificantly more knowledge\n and has greatly enhanced capabilities in \ncoding\n and \nmathematics\n, due to specialized expert models in these domains.\n\n\nIt demonstrates significant advancements in \ninstruction following\n, \nlong-text generation\n (over 8K tokens), \nunderstanding structured data\n (e.g., tables), and \ngenerating structured outputs\n, especially in JSON format. It is also \nmore resilient to diverse system prompts\n, improving role-play and condition-setting for chatbots.\n\n\nIt supports \nlong contexts\n of up to 128K tokens and can generate up to 8K tokens.\n\n\nIt offers \nmultilingual support\n for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n\n\nPlease note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license.\n\n\nReferences\n\n\nGitHub\n\n\nBlog post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/qwen2.5/4b4f719f-c327-489e-8dc1-89a455c21e89\" width=\"320\" />\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2:\n\n- It possesses **significantly more knowledge** and has greatly enhanced capabilities in **coding** and **mathematics**, due to specialized expert models in these domains.\n- It demonstrates significant advancements in **instruction following**, **long-text generation** (over 8K tokens), **understanding structured data** (e.g., tables), and **generating structured outputs**, especially in JSON format. It is also **more resilient to diverse system prompts**, improving role-play and condition-setting for chatbots.\n- It supports **long contexts** of up to 128K tokens and can generate up to 8K tokens.\n- It offers **multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nPlease note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license.\n\n## References\n\n[GitHub](https://github.com/QwenLM/Qwen2.5)\n\n[Blog post](https://qwenlm.github.io/blog/qwen2.5/)\n\n[HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "0.5b",
      "1.5b",
      "3b",
      "7b",
      "14b",
      "32b",
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2.5:3b",
        "size": "1.9GB",
        "size_gb": 1.9,
        "recommended_ram_gb": 2.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5:72b",
        "size": "47GB",
        "size_gb": 47.0,
        "recommended_ram_gb": 58.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.9,
    "context_window": 32000,
    "speed_tier": "fast",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Code Explanation",
      "Math",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers looking for a powerful multilingual model with advanced coding and mathematical capabilities.",
    "model_family": "Qwen",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Advanced coding capabilities",
      "Enhanced mathematical understanding",
      "Long-text generation",
      "Structured data understanding",
      "Resilient to diverse system prompts"
    ],
    "limitations": [
      "Requires significant computational resources",
      "May struggle with very short input prompts"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 22000000,
    "tags": 133,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "faa054e7-ac4d-4225-ad3e-7fa2acf79839",
    "model_identifier": "qwen3",
    "model_name": "qwen3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen3",
    "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen 3\n is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n\n\nNew 30B model\n\n\nollama run qwen3:30b\n\n\n\n\n\nNew 235B model\n\n\nollama run qwen3:235b\n\n\n\n\n\nOverview\n\n\nThe Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, \nQwen3-235B-A22B\n, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, \nQwen3-30B-A3B\n, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n\n\n\n\nSignificantly enhancement in its reasoning capabilities\n, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n\n\nSuperior human preference alignment\n, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n\n\nExpertise in agent capabilities\n, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n\n\nSupport of 100+ languages and dialects\n with strong capabilities for \nmultilingual instruction following and translation\n.\n\n\n\n\nReference\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522)\n\n**Qwen 3** is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n\n### New 30B model\n\n```\nollama run qwen3:30b\n```\n\n![Qwen3-30B-A3B-Instruct-2507.jpg](/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0)\n\n\n### New 235B model\n\n```\nollama run qwen3:235b\n```\n\n![0d7zztq4GB7G2ZYowO-dQ.jpg](/assets/library/qwen3/8426a459-dd88-49cd-ae89-ece442e58ec5)\n\n### Overview\n\nThe Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, **Qwen3-235B-A22B**, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, **Qwen3-30B-A3B**, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following and translation**.\n\n### Reference\n- [Blog](https://qwenlm.github.io/blog/qwen3/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking"
    ],
    "capability": "Tools",
    "labels": [
      "0.6b",
      "1.7b",
      "4b",
      "8b",
      "14b",
      "30b",
      "32b",
      "235b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen3:1.7b",
        "size": "1.4GB",
        "size_gb": 1.4,
        "recommended_ram_gb": 1.8,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3:4b",
        "size": "2.5GB",
        "size_gb": 2.5,
        "recommended_ram_gb": 3.1,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3:latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "latest",
        "size": "5.2GB",
        "size_gb": 5.2,
        "recommended_ram_gb": 6.5,
        "quantization": "q4_k_m",
        "context": "40K",
        "context_window": 40000
      },
      {
        "tag": "qwen3:14b",
        "size": "9.3GB",
        "size_gb": 9.3,
        "recommended_ram_gb": 11.6,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3:30b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      },
      {
        "tag": "qwen3:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "40K context",
        "context_window": 40000
      },
      {
        "tag": "qwen3:235b",
        "size": "142GB",
        "size_gb": 142.0,
        "recommended_ram_gb": 177.5,
        "quantization": "q4_k_m",
        "context": "256K context",
        "context_window": 256000
      }
    ],
    "min_ram_gb": 1.4,
    "context_window": 256000,
    "speed_tier": "fast",
    "use_cases": [
      "Code Generation",
      "Math",
      "Question Answering",
      "Reasoning",
      "Chat Assistant"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers looking for a comprehensive suite of dense and mixture-of-experts language models for coding, math, and general capabilities.",
    "model_family": "Qwen",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Competitive results in benchmark evaluations",
      "Comprehensive suite of dense and MoE models",
      "Small MoE model outcompetes other models with fewer parameters"
    ],
    "limitations": [
      "Requires significant RAM for larger models",
      "May not perform as well as other top-tier models in certain tasks"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 19800000,
    "tags": 58,
    "last_updated": "2025-10-26",
    "last_updated_str": "4 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "d80a1d41-5d7f-49e6-bb79-6cf25777e2cd",
    "model_identifier": "gemma2",
    "model_name": "gemma2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma2",
    "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nGoogle’s Gemma 2 model is available in three sizes, 2B, 9B and 27B, featuring a brand new architecture designed for class leading performance and efficiency.\n\n\nClass leading performance\n\n\nAt 27 billion parameters, Gemma 2 delivers performance surpassing models more than twice its size in benchmarks. This breakthrough efficiency sets a new standard in the open model landscape.\n\n\nThree sizes: 2B, 9B and 27B parameters\n\n\n\n\n2B Parameters \nollama run gemma2:2b\n\n\n9B Parameters \nollama run gemma2\n\n\n27B Parameters \nollama run gemma2:27b\n\n\n\n\nBenchmark\n\n\n\n\nIntended Usage\n\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n\n\n\nContent Creation and Communication\n\n\n\n\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\n\n\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\n\n\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\n\n\n\n\nResearch and Education\n\n\n\n\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\n\n\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\n\n\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\n\n\n\n\n\n\nUsing Gemma 2 with popular tooling\n\n\nLangChain\n\n\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.invoke(\"Why is the sky blue?\")\n\n\n\nLlamaIndex\n\n\nfrom llama_index.llms.ollama import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.complete(\"Why is the sky blue?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![Ollama in a Noogler hat with the Gemma 2 logo](https://ollama.com/assets/library/gemma2/58a4be20-b402-4dfa-8f1d-05d820f1204f)\n\nGoogle's Gemma 2 model is available in three sizes, 2B, 9B and 27B, featuring a brand new architecture designed for class leading performance and efficiency.\n\n## Class leading performance \n\nAt 27 billion parameters, Gemma 2 delivers performance surpassing models more than twice its size in benchmarks. This breakthrough efficiency sets a new standard in the open model landscape. \n\n## Three sizes: 2B, 9B and 27B parameters\n\n* 2B Parameters `ollama run gemma2:2b`\n* 9B Parameters `ollama run gemma2`\n* 27B Parameters `ollama run gemma2:27b`\n\n## Benchmark\n\n![Benchmark](https://ollama.com/assets/library/gemma2/79663012-1c9c-4451-871b-4621f1a898d6)\n\n## Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n## Using Gemma 2 with popular tooling\n\n### LangChain\n\n```python\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.invoke(\"Why is the sky blue?\")\n```\n\n### LlamaIndex\n\n```python\nfrom llama_index.llms.ollama import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.complete(\"Why is the sky blue?\")\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "9b",
      "27b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma2:2b",
        "size": "1.6GB",
        "size_gb": 1.6,
        "recommended_ram_gb": 2.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "gemma2:latest",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.4GB",
        "size_gb": 5.4,
        "recommended_ram_gb": 6.8,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "gemma2:27b",
        "size": "16GB",
        "size_gb": 16.0,
        "recommended_ram_gb": 20.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.6,
    "context_window": 8000,
    "speed_tier": "fast",
    "use_cases": [
      "Chat Assistant",
      "Creative Writing",
      "Code Generation",
      "Question Answering",
      "Text Summarization"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "The Gemma 2 model is best for developers and researchers looking for a high-performing and efficient large language model for a wide range of applications.",
    "model_family": "Gemma",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "High performance",
      "Efficient architecture",
      "Class leading performance",
      "Breakthrough efficiency"
    ],
    "limitations": [
      "Limited to three sizes: 2B, 9B, and 27B parameters"
    ],
    "target_audience": [
      "Developers",
      "Researchers"
    ],
    "pulls": 16300000,
    "tags": 94,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "63750f14-2168-4da4-a90e-eefdd8218d06",
    "model_identifier": "llama3",
    "model_name": "llama3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3",
    "description": "Meta Llama 3: The most capable openly available LLM to date",
    "readme": "Readme\n\n\n\n\n\n\n\n\nLlama 3\n\n\nThe most capable openly available LLM to date.\n\n\n\n\nMeta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both \n8B\n and \n70B\n parameter sizes (pre-trained or instruction-tuned).\n\n\nLlama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.\n\n\n\n\n\n\nCLI\n\n\nOpen the terminal and run \nollama run llama3\n\n\nAPI\n\n\nExample using curl:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nAPI documentation\n\n\nModel variants\n\n\nInstruct\n is fine-tuned for chat/dialogue use cases.\n\n\nExample:\n\n\nollama run llama3\n\n\nollama run llama3:70b\n\n\nPre-trained\n is the base model.\n\n\nExample:\n\n\nollama run llama3:text\n\n\nollama run llama3:70b-text\n\n\nReferences\n\n\nIntroducing Meta Llama 3: The most capable openly available LLM to date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Llama 3\n\nThe most capable openly available LLM to date.\n\n<img src=\"https://github.com/ollama/ollama/assets/3325447/15750d75-668c-42bd-aaf2-d0d203136d55\" width=\"660\" />\n\nMeta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both **8B** and **70B** parameter sizes (pre-trained or instruction-tuned). \n\nLlama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.  \n\n<img src=\"https://github.com/ollama/ollama/assets/3325447/8910aebc-cd9e-4d2d-b9c2-258b5ac3eeac\" />\n\n\n<img src=\"https://github.com/ollama/ollama/assets/3325447/f6df22a6-fd54-4aa2-876b-2b9354821ec6\" />\n\n\n### CLI\n\nOpen the terminal and run `ollama run llama3`\n\n### API\n\nExample using curl:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n```\n\n[API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)\n\n## Model variants\n\n**Instruct** is fine-tuned for chat/dialogue use cases. \n\n*Example:*\n`ollama run llama3`\n`ollama run llama3:70b`\n\n**Pre-trained** is the base model. \n\n*Example:*\n`ollama run llama3:text`\n`ollama run llama3:70b-text`\n\n## References\n[Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      },
      {
        "tag": "llama3:70b",
        "size": "40GB",
        "size_gb": 40.0,
        "recommended_ram_gb": 50.0,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 4.7,
    "context_window": 8000,
    "speed_tier": "medium",
    "use_cases": [
      "Chat Assistant",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Ideal for users seeking a highly capable and openly available LLM for general-purpose dialogue and question-answering tasks.",
    "model_family": "Llama",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Highly capable and state-of-the-art performance",
      "Available in multiple parameter sizes",
      "Optimized for dialogue and chat use cases"
    ],
    "limitations": [
      "May require significant computational resources for larger parameter sizes",
      "Limited to English language support is not explicitly stated but other languages are not mentioned"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Students"
    ],
    "pulls": 16100000,
    "tags": 68,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "9d796162-ddc2-4637-993e-21a30bb50598",
    "model_identifier": "phi3",
    "model_name": "phi3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi3",
    "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhi-3 is a family of open AI models developed by Microsoft.\n\n\nParameter sizes\n\n\n\n\nPhi-3 Mini\n – 3B parameters – \nollama run phi3:mini\n\n\nPhi-3 Medium\n – 14B parameters – \nollama run phi3:medium\n\n\n\n\nContext window sizes\n\n\n\n\nNote: the 128k version of this model requires \nOllama 0.1.39\n or later.\n\n\n\n\n\n\n4k \nollama run phi3:mini\n \nollama run phi3:medium\n\n\n128k \nollama run phi3:medium-128k\n\n\n\n\n\n\nPhi-3 Mini\n\n\nPhi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\n\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n\nPhi-3 Medium\n\n\nPhi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro.\n\n\n\n\nIntended Uses\n\n\nPrimary use cases\n\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require\n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\n\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n\nUse case considerations\n\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios.\nDevelopers  should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n\nResponsible AI Considerations\n\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n\n\n\nQuality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\n\n\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n\n\nInappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\n\n\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\n\n\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as “typing, math, random, collections, datetime, itertools”. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n\n\n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n\n\n\n\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n\n\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n\n\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n\n\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n\n\n\nTraining\n\n\nModel\n\n\n\n\nArchitecture: Phi-3 Mini has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\n\n\nInputs: Text. It is best suited for prompts using chat format.\n\n\nContext length: 128K tokens\n\n\nGPUS: 512 H100-80G\n\n\nTraining time: 7 days\n\n\nTraining data: 3.3T tokens\n\n\nOutputs: Generated text in response to the input\n\n\nDates: Our models were trained between February and April 2024\n\n\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n\n\n\n\nDatasets\n\n\nOur training data includes a wide variety of sources, totaling 3.3 trillion tokens, and is a combination of\n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\n\nSoftware\n\n\n\n\nPyTorch\n\n\nDeepSpeed\n\n\nTransformers\n\n\nFlash-Attention\n\n\n\n\nLicense\n\n\nThe model is licensed under the \nMIT license\n.\n\n\nTrademarks\n\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow \nMicrosoft’s Trademark & Brand Guidelines\n. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n\n\nResources\n\n\n\n\nHuggingFace\n\n\nPhi-3 Microsoft Blog\n\n\nPhi-3 Technical Report\n\n\nPhi-3 on Azure AI Studio\n\n\nPhi-3 on Hugging Face\n\n\nPhi-3 ONNX: \n4K\n and \n128K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/e06e1a36-97b2-417a-b2b2-028c980359b1\" width=\"240\" />\n\nPhi-3 is a family of open AI models developed by Microsoft.\n\n## Parameter sizes\n\n- [Phi-3 Mini](https://ollama.com/library/phi3:mini) – 3B parameters – `ollama run phi3:mini`\n- [Phi-3 Medium](https://ollama.com/library/phi3:medium) – 14B parameters – `ollama run phi3:medium`\n\n## Context window sizes\n\n> Note: the 128k version of this model requires [Ollama 0.1.39](https://github.com/ollama/ollama/releases/tag/v0.1.39) or later.\n\n- 4k `ollama run phi3:mini` `ollama run phi3:medium`\n- 128k `ollama run phi3:medium-128k`\n\n![image.png](https://ollama.com/assets/library/phi3/83b3de66-82d8-4455-9117-256802c1b82e)\n\n## Phi-3 Mini\n\nPhi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n## Phi-3 Medium \n\nPhi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro. \n\n![image.png](https://ollama.com/assets/library/phi3/2868e29b-3bba-4c4a-a6ed-1a27fb102867)\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require\n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Use case considerations**\n\nOur models ",
    "capabilities": [],
    "capability": null,
    "labels": [
      "3.8b",
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "phi3:latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "2.2GB",
        "size_gb": 2.2,
        "recommended_ram_gb": 2.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "phi3:14b",
        "size": "7.9GB",
        "size_gb": 7.9,
        "recommended_ram_gb": 9.9,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 2.2,
    "context_window": 128000,
    "speed_tier": "fast",
    "use_cases": [
      "Chat Assistant",
      "Question Answering",
      "Reasoning"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Ideal for users who need a lightweight, state-of-the-art open model for general purposes, including chat assistance and question answering.",
    "model_family": "Phi",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Lightweight",
      "State-of-the-art",
      "High-quality training data"
    ],
    "limitations": [
      "Limited context window size",
      "Requires specific Ollama version for 128k context window"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Students"
    ],
    "pulls": 16000000,
    "tags": 72,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "889801db-fb38-4e60-9f96-73eab798ad89",
    "model_identifier": "llava",
    "model_name": "llava",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llava",
    "description": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n🌋 LLaVA: Large Language and Vision Assistant\n\n\nLLaVA is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4.\n\n\nNew in LLaVA 1.6:\n\n\n\n\nIncreasing the input image resolution to up to 4x more pixels, supporting 672x672, 336x1344, 1344x336 resolutions.\n\n\nBetter visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\n\n\nBetter visual conversation for more scenarios, covering different applications.\n\n\nBetter world knowledge and logical reasoning.\n\n\n\n\nCLI Usage\n\n\nRun the model:\n\n\nollama run llava\n\n\n\nThen at the prompt, include the path to your image in the prompt:\n\n\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n\n\n\nAPI Usage\n\n\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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\"]\n}'\n\n\n\nReferences\n\n\nWebsite\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 🌋 LLaVA: Large Language and Vision Assistant\n\nLLaVA is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4.\n\n### New in LLaVA 1.6:\n* Increasing the input image resolution to up to 4x more pixels, supporting 672x672, 336x1344, 1344x336 resolutions.\n* Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\n* Better visual conversation for more scenarios, covering different applications.\n* Better world knowledge and logical reasoning.\n\n### CLI Usage\n\nRun the model:\n\n```\nollama run llava\n```\n\nThen at the prompt, include the path to your image in the prompt:\n\n```\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n```\n\n### API Usage\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "7b",
      "13b",
      "34b"
    ],
    "memory_requirements": [
      {
        "tag": "llava:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "llava:13b",
        "size": "8.0GB",
        "size_gb": 8.0,
        "recommended_ram_gb": 10.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llava:34b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 4.7,
    "context_window": 32000,
    "speed_tier": "medium",
    "use_cases": [
      "Image Understanding",
      "Text Embedding",
      "Chat Assistant",
      "Creative Writing",
      "Reasoning"
    ],
    "domain": "Multimodal",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "LLaVA is best for users needing a multimodal model that combines vision and language understanding for general-purpose tasks.",
    "model_family": "Vicuna",
    "base_model": "Vicuna",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Visual reasoning",
      "OCR capability",
      "Visual conversation",
      "World knowledge",
      "Logical reasoning"
    ],
    "limitations": [
      "High computational requirements",
      "Input image resolution limitations"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Content Creators"
    ],
    "pulls": 12900000,
    "tags": 98,
    "last_updated": "2026-02-26",
    "last_updated_str": "to version 1.6.",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "3bc52307-5ca1-406b-86e4-6d95e5d6fdb7",
    "model_identifier": "qwen2.5-coder",
    "model_name": "qwen2.5-coder",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2.5-coder",
    "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen 2.5 Coder series of models are now updated in 6 sizes: \n0.5B, 1.5B, 3B, 7B, 14B and 32B\n.\n\n\nThere are significant improvements in \ncode generation\n, \ncode reasoning\n and \ncode fixing\n. The 32B model has competitive performance with OpenAI’s GPT-4o.\n\n\n32B:\n\n\nollama run qwen2.5-coder:32b\n\n\n14B:\n\n\nollama run qwen2.5-coder:14b\n\n\n7B:\n\n\nollama run qwen2.5-coder:7b\n\n\n3B:\n\n\nollama run qwen2.5-coder:3b\n\n\n1.5B:\n\n\nollama run qwen2.5-coder:1.5b\n\n\n0.5B:\n\n\nollama run qwen2.5-coder:0.5b\n\n\nCode capabilities reaching state of the art for open-source models\n\n\n\n\nCode Generation:\n Qwen2.5 Coder 32B Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.\n\n\nCode Repair:\n Code repair is an important programming skill. Qwen2.5 Coder 32B Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5 Coder 32B Instruct scored 73.7, performing comparably to GPT-4o on Aider.\n\n\nCode Reasoning:\n Code reasoning refers to the model’s ability to learn the process of code execution and accurately predict the model’s inputs and outputs. The recently released Qwen2.5 Coder 7B Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.\n\n\n\n\nMultiple programming languages\n\n\nAn intelligent programming assistant should be familiar with all programming languages. Qwen 2.5 Coder 32B performs excellent across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket. The Qwen team used their own unique data cleaning and balancing during the pre-training phase.\n\n\n\n\nAdditionally, the multi-language code repair capabilities of Qwen 2.5 Coder 32B Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen 2.5 Coder 32B Instruct scored 75.2, ranking first among all open-source models.\n\n\n\n\nHuman Preference\n\n\nTo evaluate the alignment performance of Qwen 2.5 Coder 32B Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an ‘A vs. B win’ evaluation method, which measures the percentage of instances in the test set where model A’s score exceeds model B’s. The results below demonstrate the advantages of Qwen 2.5 Coder 32B Instruct in preference alignment.\n\n\n\n\nComprehensive model sizes to fit your device\n\n\n\n\nReferences\n\n\nBlog Post\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/qwen2.5-coder/59b3c116-2653-4d50-9b5f-1fefd24a61bf\" width=\"450\" />\n\nQwen 2.5 Coder series of models are now updated in 6 sizes: **0.5B, 1.5B, 3B, 7B, 14B and 32B**. \n\nThere are significant improvements in **code generation**, **code reasoning** and **code fixing**. The 32B model has competitive performance with OpenAI's GPT-4o. \n\n**32B:** \n`ollama run qwen2.5-coder:32b`\n\n**14B:** \n`ollama run qwen2.5-coder:14b`\n\n**7B:** \n`ollama run qwen2.5-coder:7b`\n\n**3B:**\n`ollama run qwen2.5-coder:3b` \n\n**1.5B:**\n`ollama run qwen2.5-coder:1.5b`\n\n**0.5B:**\n`ollama run qwen2.5-coder:0.5b`\n\n### Code capabilities reaching state of the art for open-source models \n\n![Comparison benchmarks](/assets/library/qwen2.5-coder/05059413-3cc4-4b07-b546-001594d0ae26)\n\n**Code Generation:** Qwen2.5 Coder 32B Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.\n\n**Code Repair:** Code repair is an important programming skill. Qwen2.5 Coder 32B Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5 Coder 32B Instruct scored 73.7, performing comparably to GPT-4o on Aider.\n\n**Code Reasoning:** Code reasoning refers to the model’s ability to learn the process of code execution and accurately predict the model’s inputs and outputs. The recently released Qwen2.5 Coder 7B Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.\n\n![Benchmarks](/assets/library/qwen2.5-coder/0bd9e1aa-a87b-474b-84ba-264a85041605)\n\n### Multiple programming languages\nAn intelligent programming assistant should be familiar with all programming languages. Qwen 2.5 Coder 32B performs excellent across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket. The Qwen team used their own unique data cleaning and balancing during the pre-training phase. \n\n![McEval Performance](/assets/library/qwen2.5-coder/6436978b-1371-48a4-a21a-b6da729b74e1)\n\nAdditionally, the multi-language code repair capabilities of Qwen 2.5 Coder 32B Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen 2.5 Coder 32B Instruct scored 75.2, ranking first among all open-source models.\n\n![MdEval Performance](/assets/library/qwen2.5-coder/f2401bd6-f6d7-41ca-981d-98abc62f1493)\n\n### Human Preference \n\nTo evaluate the alignment performance of Qwen 2.5 Coder 32B Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an ‘A vs. B win’ evaluation method, which measures the percentage of instances in the test set where model A’s score exceeds model B’s. The results below demonstrate the advantages of Qwen 2.5 Coder 32B Instruct in preference alignment.\n\n![human preference](/assets/library/qwen2.5-coder/bbf378d8-c80e-4ae3-98ab-90111dfbf3e7)\n\n### Comprehensive model sizes to fit your device\n\n![Model sizes](/assets/library/qwen2.5-coder/752764ea-d510-4bc5-8658-dc5d8ba51019)\n\n## References\n\n[Blog Post](https://qwenlm.github.io/blog/qwen2.5-coder-family/)\n\n[HuggingFace](https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "0.5b",
      "1.5b",
      "3b",
      "7b",
      "14b",
      "32b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2.5-coder:3b",
        "size": "1.9GB",
        "size_gb": 1.9,
        "recommended_ram_gb": 2.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5-coder:latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.7GB",
        "size_gb": 4.7,
        "recommended_ram_gb": 5.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5-coder:14b",
        "size": "9.0GB",
        "size_gb": 9.0,
        "recommended_ram_gb": 11.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen2.5-coder:32b",
        "size": "20GB",
        "size_gb": 20.0,
        "recommended_ram_gb": 25.0,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.9,
    "context_window": 32000,
    "speed_tier": "fast",
    "use_cases": [
      "Code Generation",
      "Code Review",
      "Code Explanation"
    ],
    "domain": "Code",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers looking for state-of-the-art code generation and reasoning capabilities in an open-source model.",
    "model_family": "Qwen",
    "base_model": "qwen2",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "State-of-the-art code generation",
      "Improved code reasoning",
      "Competitive performance with commercial models"
    ],
    "limitations": [
      "Requires significant computational resources for larger models",
      "May not perform as well as commercial models in certain tasks"
    ],
    "target_audience": [
      "Developers",
      "Data Scientists"
    ],
    "pulls": 11300000,
    "tags": 199,
    "last_updated": "2025-05-26",
    "last_updated_str": "9 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "6149d2b9-2272-4a1b-8f86-8070f5eec08d",
    "model_identifier": "mxbai-embed-large",
    "model_name": "mxbai-embed-large",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mxbai-embed-large",
    "description": "State-of-the-art large embedding model from mixedbread.ai",
    "readme": "Readme\n\n\n\n\n\n\n\n\nmxbai-embed-large\n\n\n\n\nAs of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs \ntext-embedding-3-large\n model and matches the performance of model 20x its size.\n\n\nmxbai-embed-large\n was trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length.\n\n\nUsage\n\n\nREST API\n\n\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\"\n}'\n\n\n\nPython library\n\n\nollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering')\n\n\n\nJavascript library\n\n\nollama.embeddings({ model: 'mxbai-embed-large', prompt: 'Represent this sentence for searching relevant passages:  The sky is blue because of Rayleigh scattering' })\n\n\n\nReferences\n\n\nBlog post\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## mxbai-embed-large\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/215cfb6a-8efa-4e9b-824d-e5f466b58c49\" widht=\"400\">\n\nAs of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs `text-embedding-3-large` model and matches the performance of model 20x its size.\n\n`mxbai-embed-large` was trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length.\n\n## Usage\n\n\n### REST API\n\n```\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\"\n}'\n```\n\n### Python library\n\n```\nollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering')\n```\n\n### Javascript library\n\n```\nollama.embeddings({ model: 'mxbai-embed-large', prompt: 'Represent this sentence for searching relevant passages:  The sky is blue because of Rayleigh scattering' })\n```\n\n\n## References\n\n[Blog post](https://www.mixedbread.ai/blog/mxbai-embed-large-v1)\n\n[Hugging Face](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "335m"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "context_window": null,
    "speed_tier": null,
    "use_cases": [
      "Text Embedding"
    ],
    "domain": "Embedding",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "This model is best for generating high-quality text embeddings, especially for tasks like searching relevant passages.",
    "model_family": "mxbai",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "State-of-the-art performance",
      "Generalizes well across several domains and tasks",
      "Outperforms commercial models like OpenAI's text-embedding-3-large model"
    ],
    "limitations": [
      "Requires significant computational resources",
      "May not perform well on very short or very long texts"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 7600000,
    "tags": 4,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "2dac92d1-b70b-473f-b62e-a4a55e2ab992",
    "model_identifier": "phi4",
    "model_name": "phi4",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/phi4",
    "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nPhi-4\n is a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets.\n\n\n\n\nThe model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n\nContext length:\n 16k tokens\n\n\n\n\nPrimary use cases\n\n\nThe model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n\n\n\nMemory/compute constrained environments.\n\n\nLatency bound scenarios.\n\n\nReasoning and logic.\n\n\n\n\nOut-of-scope use cases\n\n\nThe models are not specifically designed or evaluated for all downstream purposes, thus:\n\n\n\n\nDevelopers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n\n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\n\n\nNothing contained in this readme should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" />\n\n**Phi-4** is a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. \n\n![Phi-4 benchmark](/assets/library/phi4/67391ae2-e565-4173-ac3c-5e49bc977ac4)\n\nThe model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. \n\n**Context length:** 16k tokens\n\n![Phi-4 performance eval by Microsoft](/assets/library/phi4/5da7d0b6-53df-42f9-9c1b-651bfc380cdd)\n\n\n### Primary use cases\n\nThe model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n1. Memory/compute constrained environments.\n2. Latency bound scenarios.\n3. Reasoning and logic.\n\n### Out-of-scope use cases \n\nThe models are not specifically designed or evaluated for all downstream purposes, thus:\n\n1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\n3. Nothing contained in this readme should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "14b"
    ],
    "memory_requirements": [
      {
        "tag": "phi4:latest",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "9.1GB",
        "size_gb": 9.1,
        "recommended_ram_gb": 11.4,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      }
    ],
    "min_ram_gb": 9.1,
    "context_window": 16000,
    "speed_tier": "slow",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "English"
    ],
    "complexity": "intermediate",
    "best_for": "The phi4 model is best for researchers and developers working on language models, seeking a building block for generative AI-powered features and general-purpose AI systems.",
    "model_family": "Phi",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "State-of-the-art performance",
      "Robust safety measures",
      "Precise instruction adherence"
    ],
    "limitations": [
      "Limited to 16k token context length",
      "Primarily designed for English language tasks"
    ],
    "target_audience": [
      "Researchers",
      "Developers",
      "Data Scientists"
    ],
    "pulls": 7200000,
    "tags": 5,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "a5ada780-7a7d-49c8-a00e-c224a16d82fb",
    "model_identifier": "gpt-oss",
    "model_name": "gpt-oss",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gpt-oss",
    "description": "OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nWelcome OpenAI’s gpt-oss!\n\n\nOllama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\n\nGet started\n\n\nYou can get started by \ndownloading the latest Ollama version\n.\n\n\nThe model can be downloaded directly in Ollama’s new app or via the terminal:\n\n\nollama run gpt-oss:20b\n\n\nollama run gpt-oss:120b\n\n\nFeature highlights\n\n\n\n\nAgentic capabilities:\n Use the models’ native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs.\n\n\nFull chain-of-thought:\n Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs.\n\n\nConfigurable reasoning effort:\n Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n\n\nFine-tunable:\n Fully customize models to your specific use case through parameter fine-tuning.\n\n\nPermissive Apache 2.0 license:\n Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n\n\n\n\n\n\nQuantization - MXFP4 format\n\n\nOpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU.\n\n\nOllama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama’s new engine to support the MXFP4 format.\n\n\nOllama collaborated with OpenAI to benchmark against their reference implementations to ensure Ollama’s implementations have the same quality.\n\n\n20B parameter model\n\n\n\n\ngpt-oss-20b model is designed for lower latency, local, or specialized use-cases.\n\n\n120B parameter model\n\n\n\n\nReference\n\n\n\n\nOpenAI launch blog\n\n\nOpenAI model card\n\n\nNVIDIA RTX blog\n\n\nNVIDIA cloud to edge\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![OpenAI gpt-oss banner](/assets/library/gpt-oss/e9da5025-e172-441d-9f06-8dfa797da9b0)\n\n### Welcome OpenAI's gpt-oss! \n\nOllama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases. \n\n### Get started \n\nYou can get started by [downloading the latest Ollama version](https://ollama.com/download). \n\nThe model can be downloaded directly in Ollama’s new app or via the terminal: \n\n**ollama run gpt-oss:20b** \n\n**ollama run gpt-oss:120b**\n\n### Feature highlights\n\n- **Agentic capabilities:** Use the models’ native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs.\n- **Full chain-of-thought:** Gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs.\n- **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n- **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n- **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n\n![benchmark](/assets/library/gpt-oss/d29a500d-303f-4d24-b557-a8247d17ed4c)\n\n### Quantization - MXFP4 format\n\nOpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU. \n\nOllama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama’s new engine to support the MXFP4 format. \n\nOllama collaborated with OpenAI to benchmark against their reference implementations to ensure Ollama’s implementations have the same quality. \n\n### 20B parameter model \n\n![gpt-oss 20B](/assets/library/gpt-oss/343d558d-a3bc-472e-9956-37fdc3cc4f6c)\n\ngpt-oss-20b model is designed for lower latency, local, or specialized use-cases. \n\n\n### 120B parameter model \n\n![gpt-oss 120B](/assets/library/gpt-oss/51b69d33-c747-4117-ba76-a6efa1b0a986)\n\n### Reference \n\n- [OpenAI launch blog](https://openai.com/index/introducing-gpt-oss)\n- [OpenAI model card](https://openai.com/index/gpt-oss-model-card/)\n- [NVIDIA RTX blog](https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss)\n- [NVIDIA cloud to edge](https://developer.nvidia.com/blog/delivering-1-5-m-tps-inference-on-nvidia-gb200-nvl72-nvidia-accelerates-openai-gpt-oss-models-from-cloud-to-edge/)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools",
      "Thinking",
      "Cloud"
    ],
    "capability": "Tools",
    "labels": [
      "20b",
      "120b"
    ],
    "memory_requirements": [
      {
        "tag": "gpt-oss:latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "14GB",
        "size_gb": 14.0,
        "recommended_ram_gb": 17.5,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "gpt-oss:120b",
        "size": "65GB",
        "size_gb": 65.0,
        "recommended_ram_gb": 81.2,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 14.0,
    "context_window": 128000,
    "speed_tier": "slow",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Function Calling",
      "Agent / Automation",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and users who need powerful reasoning and agentic tasks in their applications",
    "model_family": "Other",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Powerful reasoning",
      "Agentic tasks",
      "Versatile developer use cases",
      "Function calling",
      "Web browsing"
    ],
    "limitations": [
      "Requires significant computational resources",
      "May have limited support for certain languages or tasks",
      "Dependent on Ollama's app or terminal for usage"
    ],
    "target_audience": [
      "Developers",
      "Data Scientists",
      "Researchers"
    ],
    "pulls": 7100000,
    "tags": 5,
    "last_updated": "2025-10-26",
    "last_updated_str": "4 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "c05103bf-4a13-4f42-ac3f-8de640f11f87",
    "model_identifier": "gemma",
    "model_name": "gemma",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/gemma",
    "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires Ollama 0.1.26 or later. \nDownload it here\n.\n\n\n\n\nGemma is a new open model developed by Google and its DeepMind team. It’s inspired by Gemini models at Google.\n\n\nGemma is available in both \n2b\n and \n7b\n parameter sizes:\n\n\n\n\nollama run gemma:2b\n\n\nollama run gemma:7b\n (default)\n\n\n\n\nThe models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning.\n\n\nTo ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies.\n\n\nReference\n\n\nGoogle Gemma\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/ollama/ollama/assets/251292/01333db3-c27b-4044-88b3-9b2ffbe06415\"/>\n\n> Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download).\n\nGemma is a new open model developed by Google and its DeepMind team. It’s inspired by Gemini models at Google.\n\nGemma is available in both `2b` and `7b` parameter sizes:\n\n* `ollama run gemma:2b`\n* `ollama run gemma:7b` (default)\n\nThe models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning.\n\nTo ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies.\n\n# Reference\n\n[Google Gemma](https://ai.google.dev/gemma/docs/model_card)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "2b",
      "7b"
    ],
    "memory_requirements": [
      {
        "tag": "gemma:2b",
        "size": "1.7GB",
        "size_gb": 1.7,
        "recommended_ram_gb": 2.1,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "gemma:latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "5.0GB",
        "size_gb": 5.0,
        "recommended_ram_gb": 6.2,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.7,
    "context_window": 8000,
    "speed_tier": "fast",
    "use_cases": [
      "Chat Assistant",
      "Creative Writing",
      "Code Generation",
      "Text Summarization"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Best for developers and researchers looking for a lightweight, state-of-the-art open model for general-purpose tasks.",
    "model_family": "Gemma",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Lightweight",
      "State-of-the-art",
      "Versatile parameter sizes",
      "Trained on diverse dataset"
    ],
    "limitations": [
      "Requires Ollama 0.1.26 or later",
      "Limited to 2b and 7b parameter sizes"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Students"
    ],
    "pulls": 5900000,
    "tags": 102,
    "last_updated": "2026-02-26",
    "last_updated_str": "to version 1.1",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "728b0397-9506-4bfa-9a1b-ccf5d2b87589",
    "model_identifier": "qwen",
    "model_name": "qwen",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen",
    "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
    "readme": "Readme\n\n\n\n\n\n\n\n\nQwen 2 is now available \nhere\n.\n\n\n\n\nQwen is a series of transformer-based large language models by Alibaba Cloud, pre-trained on a large volume of data, including web texts, books, code, etc.\n\n\nNew in Qwen 1.5\n\n\n\n\n6 model sizes, including 0.5B, 1.8B, 4B (default), 7B, 14B, 32B (new) and 72B\n\n\n\n\nollama run qwen:0.5b\n\n\nollama run qwen:1.8b\n\n\nollama run qwen:4b\n\n\nollama run qwen:7b\n\n\nollama run qwen:14b\n\n\nollama run qwen:32b\n\n\nollama run qwen:72b\n\n\nollama run qwen:110b\n\n\n\n\nSignificant performance improvement in human preference for chat models\n\n\nMultilingual support of both base and chat models\n\n\nStable support of 32K context length for models of all sizes\n\n\n\n\nThe original Qwen model is offered in four different parameter sizes: 1.8B, 7B, 14B, and 72B.\n\n\nFeatures\n\n\n\n\nLow-cost deployment\n: the minimum memory requirement for inference is less than 2GB.\n\n\nLarge-scale high-quality training corpora\n: Models are pre-trained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.\n\n\nGood performance\n: Qwen supports long context lengths (8K on the \n1.8b\n, \n7b\n and \n14b\n parameter models, and 32K on the \n72b\n parameter model), and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including common-sense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks.\n\n\nMore comprehensive vocabulary coverage\n: Compared with other open-source models based on Chinese and English vocabularies, Qwen uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.\n\n\nSystem prompt\n: Qwen can realize role playing, language style transfer, task setting, and behavior-setting by using a system prompt.\n\n\n\n\nReference\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQwen 2 is now available [here](https://ollama.com/library/qwen2). \n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/160726a3-a986-427d-b1df-3b75357903a4\" width=\"280\" />\n\nQwen is a series of transformer-based large language models by Alibaba Cloud, pre-trained on a large volume of data, including web texts, books, code, etc. \n\n### New in Qwen 1.5 \n\n- 6 model sizes, including 0.5B, 1.8B, 4B (default), 7B, 14B, 32B (new) and 72B\n  * `ollama run qwen:0.5b`\n  * `ollama run qwen:1.8b`\n  * `ollama run qwen:4b` \n  * `ollama run qwen:7b` \n  * `ollama run qwen:14b`\n  * `ollama run qwen:32b`\n  * `ollama run qwen:72b`\n  * `ollama run qwen:110b`\n- Significant performance improvement in human preference for chat models\n- Multilingual support of both base and chat models\n- Stable support of 32K context length for models of all sizes\n\nThe original Qwen model is offered in four different parameter sizes: 1.8B, 7B, 14B, and 72B. \n\n## Features\n\n* **Low-cost deployment**: the minimum memory requirement for inference is less than 2GB.\n\n* **Large-scale high-quality training corpora**: Models are pre-trained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.\n\n* **Good performance**: Qwen supports long context lengths (8K on the `1.8b`, `7b` and `14b` parameter models, and 32K on the `72b` parameter model), and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including common-sense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks.\n\n* **More comprehensive vocabulary coverage**: Compared with other open-source models based on Chinese and English vocabularies, Qwen uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.\n\n* **System prompt**: Qwen can realize role playing, language style transfer, task setting, and behavior-setting by using a system prompt.\n\n\n## Reference\n\n[GitHub](https://github.com/QwenLM/Qwen)\n\n[Hugging Face](https://huggingface.co/Qwen)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "0.5b",
      "1.8b",
      "4b",
      "7b",
      "14b",
      "32b",
      "72b",
      "110b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen:1.8b",
        "size": "1.1GB",
        "size_gb": 1.1,
        "recommended_ram_gb": 1.4,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "than",
        "size": "2GB",
        "size_gb": 2.0,
        "recommended_ram_gb": 2.5,
        "quantization": "q4_k_m",
        "context": ".",
        "context_window": null
      },
      {
        "tag": "qwen:latest",
        "size": "2.3GB",
        "size_gb": 2.3,
        "recommended_ram_gb": 2.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "2.3GB",
        "size_gb": 2.3,
        "recommended_ram_gb": 2.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen:7b",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:14b",
        "size": "8.2GB",
        "size_gb": 8.2,
        "recommended_ram_gb": 10.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:32b",
        "size": "18GB",
        "size_gb": 18.0,
        "recommended_ram_gb": 22.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:72b",
        "size": "41GB",
        "size_gb": 41.0,
        "recommended_ram_gb": 51.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "qwen:110b",
        "size": "63GB",
        "size_gb": 63.0,
        "recommended_ram_gb": 78.8,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 1.1,
    "context_window": 32000,
    "speed_tier": "fast",
    "use_cases": [
      "Chat Assistant",
      "Creative Writing",
      "Code Generation",
      "Question Answering",
      "Translation"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Ideal for chat models, multilingual support, and users requiring a range of model sizes.",
    "model_family": "Qwen",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Multilingual support",
      "Range of model sizes",
      "Stable support of 32K context length",
      "Significant performance improvement in human preference for chat models"
    ],
    "limitations": [
      "Limited information on specific use cases",
      "May require adjustments for very large models"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Students",
      "Content Creators"
    ],
    "pulls": 5600000,
    "tags": 379,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "8b10434d-6cff-49a5-b07f-a1bd5241c766",
    "model_identifier": "llama2",
    "model_name": "llama2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama2",
    "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nLlama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat.\n\n\nCLI\n\n\nOpen the terminal and run \nollama run llama2\n\n\nAPI\n\n\nExample using curl:\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n\n\n\nAPI documentation\n\n\nMemory requirements\n\n\n\n\n7b models generally require at least 8GB of RAM\n\n\n13b models generally require at least 16GB of RAM\n\n\n70b models generally require at least 64GB of RAM\n\n\n\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n\nModel variants\n\n\nChat\n is fine-tuned for chat/dialogue use cases. These are the default in Ollama, and for models tagged with -chat in the tags tab.\n\n\nExample: \nollama run llama2\n\n\nPre-trained\n is without the chat fine-tuning. This is tagged as -text in the tags tab.\n\n\nExample: \nollama run llama2:text\n\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\nReferences\n\n\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n\n\nMeta’s Hugging Face repo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/286e4e13-0b2b-43a4-9b07-23a50d3a3d33\" width=\"360\" />\n\nLlama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat.\n\n### CLI\n\nOpen the terminal and run `ollama run llama2`\n\n### API\n\nExample using curl:\n\n```bash\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\n```\n\n[API documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md)\n\n## Memory requirements\n\n- 7b models generally require at least 8GB of RAM\n- 13b models generally require at least 16GB of RAM\n- 70b models generally require at least 64GB of RAM\n\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\n\n## Model variants\n\n**Chat** is fine-tuned for chat/dialogue use cases. These are the default in Ollama, and for models tagged with -chat in the tags tab.\n\n*Example: `ollama run llama2`*\n\n**Pre-trained** is without the chat fine-tuning. This is tagged as -text in the tags tab.\n\n*Example: `ollama run llama2:text`*\n\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\n\n\n## References\n[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n\n[Meta's Hugging Face repo](https://huggingface.co/meta-llama)\n\n\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "llama2:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "llama2:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "llama2:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 3.8,
    "context_window": 4000,
    "speed_tier": "medium",
    "use_cases": [
      "Chat Assistant",
      "Question Answering"
    ],
    "domain": "Language",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Users looking for a conversational AI model with a wide range of parameter options.",
    "model_family": "Llama",
    "base_model": null,
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": "Llama 2 Community License",
    "strengths": [
      "Supports a wide range of parameter options",
      "Fine-tuned on over 1 million human annotations",
      "Supports a context length of 4096"
    ],
    "limitations": [
      "Requires significant RAM for larger models",
      "Higher quantization levels may cause issues"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Content Creators"
    ],
    "pulls": 5500000,
    "tags": 102,
    "last_updated": "2024-02-26",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "7e343097-dd0f-4919-84d7-653d11ae74be",
    "model_identifier": "qwen2",
    "model_name": "qwen2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/qwen2",
    "description": "Qwen2 is a new series of large language models from Alibaba group",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nQwen2\n is trained on data in \n29 languages\n, including \nEnglish and Chinese\n.\n\n\nIt is available in 4 parameter sizes: \n0.5B\n, \n1.5B\n, \n7B\n, \n72B\n.\n\n\nIn the 7B and 72B models, context length has been extended to \n128k tokens\n.\n\n\n\n\n\n\n\n\nModels\n\n\nQwen2-0.5B\n\n\nQwen2-1.5B\n\n\nQwen2-7B\n\n\nQwen2-72B\n\n\n\n\n\n\n\n\n\n\nParams\n\n\n0.49B\n\n\n1.54B\n\n\n7.07B\n\n\n72.71B\n\n\n\n\n\n\nNon-Emb Params\n\n\n0.35B\n\n\n1.31B\n\n\n5.98B\n\n\n70.21B\n\n\n\n\n\n\nGQA\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\n\n\nTie Embedding\n\n\nTrue\n\n\nTrue\n\n\nFalse\n\n\nFalse\n\n\n\n\n\n\nContext Length\n\n\n32K\n\n\n32K\n\n\n128K\n\n\n128K\n\n\n\n\n\n\n\n\nSupported languages\n\n\nThis is in addition to English and Chinese\n\n\n\n\n\n\n\n\nRegions\n\n\nLanguages\n\n\n\n\n\n\n\n\n\n\nWestern Europe\n\n\nGerman, French, Spanish, Portuguese, Italian, Dutch\n\n\n\n\n\n\nEastern & Central Europe\n\n\nRussian, Czech, Polish\n\n\n\n\n\n\nMiddle East\n\n\nArabic, Persian, Hebrew, Turkish\n\n\n\n\n\n\nEastern Asia\n\n\nJapanese, Korean\n\n\n\n\n\n\nSouth-Eastern Asia\n\n\nVietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog\n\n\n\n\n\n\nSouthern Asia\n\n\nHindi, Bengali, Urdu\n\n\n\n\n\n\n\n\nPerformance\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\nAll models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed.\n\n\nQwen2 72B model still uses the original Qianwen License.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/qwen2/c011288c-fb07-42eb-b802-7287a5f12ea6\" width=\"320\" />\n\n**Qwen2** is trained on data in **29 languages**, including **English and Chinese**. \n\nIt is available in 4 parameter sizes: **0.5B**, **1.5B**, **7B**, **72B**. \n\nIn the 7B and 72B models, context length has been extended to **128k tokens**. \n\nModels | Qwen2-0.5B | Qwen2-1.5B | Qwen2-7B | Qwen2-72B\n-- | -- | -- | -- | -- \nParams | 0.49B | 1.54B | 7.07B | 72.71B\nNon-Emb Params | 0.35B | 1.31B | 5.98B | 70.21B\nGQA | True | True | True | True\nTie Embedding | True | True | False | False\nContext Length | 32K | 32K | 128K | 128K\n\n### Supported languages\n\nThis is in addition to English and Chinese \n\nRegions | Languages\n-- | --\nWestern Europe | German, French, Spanish, Portuguese, Italian, Dutch\nEastern & Central Europe | Russian, Czech, Polish\nMiddle East | Arabic, Persian, Hebrew, Turkish\nEastern Asia | Japanese, Korean\nSouth-Eastern Asia | Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog\nSouthern Asia | Hindi, Bengali, Urdu\n\n### Performance \n\n![image.png](https://ollama.com/assets/library/qwen2/68b445e3-bf1b-4fff-9621-4e5bbf4a72a2)\n\n![image.png](https://ollama.com/assets/library/qwen2/72e9bf41-f8d6-4b7a-a7ef-9599ef533af6)\n\n![image.png](https://ollama.com/assets/library/qwen2/6c978d72-c37c-45a2-b7f4-c06178c0182c)\n\n![image.png](https://ollama.com/assets/library/qwen2/5247046f-3c32-4edc-a8e1-a10f831ef916)\n\n### License \n\nAll models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed. \n\nQwen2 72B model still uses the original Qianwen License.\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "0.5b",
      "1.5b",
      "7b",
      "72b"
    ],
    "memory_requirements": [
      {
        "tag": "qwen2:latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "4.4GB",
        "size_gb": 4.4,
        "recommended_ram_gb": 5.5,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      },
      {
        "tag": "qwen2:72b",
        "size": "41GB",
        "size_gb": 41.0,
        "recommended_ram_gb": 51.2,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 4.4,
    "context_window": 32000,
    "speed_tier": "medium",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Text Summarization",
      "Translation"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Qwen2 is suitable for users who need a large language model with extended context length and support for multiple languages.",
    "model_family": "Qwen",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Supports 29 languages",
      "Extended context length up to 128k tokens",
      "Available in 4 parameter sizes"
    ],
    "limitations": [
      "Requires a minimum of 4.4 GB RAM",
      "Limited information available on specific use cases"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Content Creators"
    ],
    "pulls": 4900000,
    "tags": 97,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "bface244-95a8-4ffa-b0fa-8ff70ba39d5e",
    "model_identifier": "minicpm-v",
    "model_name": "minicpm-v",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/minicpm-v",
    "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.3.10\n or later.\n\n\n\n\nMiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\n\n\n\n\n🔥 Leading Performance\n: MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding.\n\n\n🖼️ Multi Image Understanding and In-context Learning\n. MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\n\n\n💪 Strong OCR Capability\n: MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro. Based on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc.\n\n\n🚀 Superior Efficiency\n: In addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption.\n\n\n\n\nRefrences\n\n\nGitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/minicpm-v/9252c73d-2c9c-434c-8a34-21f4d5cdd25e\" width=\"320\" />\n\n> Note: this model requires [Ollama 0.3.10](https://github.com/ollama/ollama/releases/tag/v0.3.10) or later.\n\nMiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\n\n* **🔥 Leading Performance**: MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding.\n\n* **🖼️ Multi Image Understanding and In-context Learning**. MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\n\n* **💪 Strong OCR Capability**: MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro. Based on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc.\n\n* **🚀 Superior Efficiency**: In addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption.\n\n## Refrences\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-V)\n\n[Hugging Face](https://huggingface.co/openbmb/MiniCPM-V-2_6)\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "minicpm-v:latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "32K context",
        "context_window": 32000
      },
      {
        "tag": "latest",
        "size": "5.5GB",
        "size_gb": 5.5,
        "recommended_ram_gb": 6.9,
        "quantization": "q4_k_m",
        "context": "32K",
        "context_window": 32000
      }
    ],
    "min_ram_gb": 5.5,
    "context_window": 32000,
    "speed_tier": "medium",
    "use_cases": [
      "Image Understanding",
      "Video Understanding"
    ],
    "domain": "Vision",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "This model is ideal for users requiring leading performance in vision-language understanding tasks, such as multi-image and video understanding.",
    "model_family": "Qwen",
    "base_model": "SigLip-400M, Qwen2-7B",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Leading performance in vision-language understanding tasks",
      "Significant performance improvement over MiniCPM-Llama3-V 2.5",
      "New features for multi-image and video understanding"
    ],
    "limitations": [
      "Requires Ollama 0.3.10 or later",
      "May not perform well with limited RAM (requires at least 5.5 GB)"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Content Creators"
    ],
    "pulls": 4600000,
    "tags": 17,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "d862ad67-6aa2-4165-bf65-1c9681e60c89",
    "model_identifier": "codellama",
    "model_name": "codellama",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/codellama",
    "description": "A large language model that can use text prompts to generate and discuss code.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nCode Llama is a model for generating and discussing code, built on top of \nLlama 2\n. It’s designed to make workflows faster and efficient for developers and make it easier for people to learn how to code. It can generate both code and natural language about code. Code Llama supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash and more.\n\n\nParameter counts\n\n\n\n\n\n\n\n\nParameter Count\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 billion\n\n\nView\n\n\nollama run codellama:7b\n\n\n\n\n\n\n13 billion\n\n\nView\n\n\nollama run codellama:13b\n\n\n\n\n\n\n34 billion\n\n\nView\n\n\nollama run codellama:34b\n\n\n\n\n\n\n70 billion\n\n\nView\n\n\nollama run codellama:70b\n\n\n\n\n\n\n\n\nUsage\n\n\nCLI\n\n\nollama run codellama \"Write me a function that outputs the fibonacci sequence\"\n\n\n\nAPI\n\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama\",\n  \"prompt\": \"Write me a function that outputs the fibonacci sequence\"\n}'\n\n\n\nVariations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstruct\n\n\nFine-tuned to generate helpful and safe answers in natural language\n\n\n\n\n\n\npython\n\n\nA specialized variation of Code Llama further fine-tuned on 100B tokens of Python code\n\n\n\n\n\n\ncode\n\n\nBase model for code completion\n\n\n\n\n\n\n\n\nExample prompts\n\n\nAsk questions\n\n\nollama run codellama:7b-instruct 'You are an expert programmer that writes simple, concise code and explanations. Write a python function to generate the nth fibonacci number.'\n\n\n\nFill-in-the-middle (FIM) or infill\n\n\nollama run codellama:7b-code '<PRE> def compute_gcd(x, y): <SUF>return result <MID>'\n\n\n\nFill-in-the-middle (FIM) is a special prompt format supported by the code completion model can complete code between two already written code blocks. Code Llama expects a specific format for infilling code:\n\n\n<PRE> {prefix} <SUF>{suffix} <MID>\n\n\n\n<PRE>\n, \n<SUF>\n and \n<MID>\n are special tokens that guide the model.\n\n\nCode review\n\n\nollama run codellama '\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n'\n\n\n\nWriting tests\n\n\nollama run codellama \"write a unit test for this function: $(cat example.py)\"\n\n\n\nCode completion\n\n\nollama run codellama:7b-code '# A simple python function to remove whitespace from a string:'\n\n\n\nMore information\n\n\n\n\nHow to prompt Code Llama\n\n\nWhitepaper\n\n\nCodeLlama GitHub\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/3325447/286e4e13-0b2b-43a4-9b07-23a50d3a3d33\" width=\"360\" />\n\nCode Llama is a model for generating and discussing code, built on top of [Llama 2](https://ollama.ai/library/llama2). It's designed to make workflows faster and efficient for developers and make it easier for people to learn how to code. It can generate both code and natural language about code. Code Llama supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash and more.\n\n## Parameter counts\n\n| Parameter Count |                                |                            |\n| --------------- | ------------------------------ | -------------------------- |\n| 7 billion       | [View](/library/codellama:7b)  | `ollama run codellama:7b`  |\n| 13 billion      | [View](/library/codellama:13b) | `ollama run codellama:13b` |\n| 34 billion      | [View](/library/codellama:34b) | `ollama run codellama:34b` |\n| 70 billion      | [View](/library/codellama:70b) | `ollama run codellama:70b` |\n\n## Usage\n\n### CLI\n\n```\nollama run codellama \"Write me a function that outputs the fibonacci sequence\"\n```\n\n### API\n\n```\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama\",\n  \"prompt\": \"Write me a function that outputs the fibonacci sequence\"\n}'\n```\n\n## Variations\n\n|            |                                                                                        |\n| ---------- | -------------------------------------------------------------------------------------- |\n| `instruct` | Fine-tuned to generate helpful and safe answers in natural language                    |\n| `python`   | A specialized variation of Code Llama further fine-tuned on 100B tokens of Python code |\n| `code`     | Base model for code completion                                                         |\n\n## Example prompts\n\n### Ask questions\n\n```\nollama run codellama:7b-instruct 'You are an expert programmer that writes simple, concise code and explanations. Write a python function to generate the nth fibonacci number.'\n```\n\n### Fill-in-the-middle (FIM) or infill\n\n```\nollama run codellama:7b-code '<PRE> def compute_gcd(x, y): <SUF>return result <MID>'\n```\n\nFill-in-the-middle (FIM) is a special prompt format supported by the code completion model can complete code between two already written code blocks. Code Llama expects a specific format for infilling code:\n\n```\n<PRE> {prefix} <SUF>{suffix} <MID>\n```\n\n`<PRE>`, `<SUF>` and `<MID>` are special tokens that guide the model.\n\n### Code review\n\n```\nollama run codellama '\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n'\n```\n\n### Writing tests\n\n```\nollama run codellama \"write a unit test for this function: $(cat example.py)\"\n```\n\n### Code completion\n\n```\nollama run codellama:7b-code '# A simple python function to remove whitespace from a string:'\n```\n\n## More information\n\n- [How to prompt Code Llama](https://ollama.ai/blog/how-to-prompt-code-llama)\n- [Whitepaper](https://arxiv.org/abs/2308.12950)\n- [CodeLlama GitHub](https://github.com/facebookresearch/codellama)\n- [Hugging Face](https://huggingface.co/codellama)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b",
      "34b",
      "70b"
    ],
    "memory_requirements": [
      {
        "tag": "codellama:latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "latest",
        "size": "3.8GB",
        "size_gb": 3.8,
        "recommended_ram_gb": 4.8,
        "quantization": "q4_k_m",
        "context": "16K",
        "context_window": 16000
      },
      {
        "tag": "codellama:13b",
        "size": "7.4GB",
        "size_gb": 7.4,
        "recommended_ram_gb": 9.2,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "codellama:34b",
        "size": "19GB",
        "size_gb": 19.0,
        "recommended_ram_gb": 23.8,
        "quantization": "q4_k_m",
        "context": "16K context",
        "context_window": 16000
      },
      {
        "tag": "codellama:70b",
        "size": "39GB",
        "size_gb": 39.0,
        "recommended_ram_gb": 48.8,
        "quantization": "q4_k_m",
        "context": "2K context",
        "context_window": 2000
      }
    ],
    "min_ram_gb": 3.8,
    "context_window": 16000,
    "speed_tier": "medium",
    "use_cases": [
      "Code Generation",
      "Code Explanation",
      "Code Review"
    ],
    "domain": "Code",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Developers who need help with code generation and discussion, and beginners who want to learn how to code more efficiently.",
    "model_family": "Llama",
    "base_model": "Llama 2",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Code generation",
      "Code discussion",
      "Support for multiple programming languages"
    ],
    "limitations": [
      "Limited to code-related tasks",
      "May require significant computational resources for larger models"
    ],
    "target_audience": [
      "Developers",
      "Students",
      "Data Scientists"
    ],
    "pulls": 4300000,
    "tags": 199,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "53eec1d6-fad7-4d99-aaf2-be34f2f5a073",
    "model_identifier": "llama3.2-vision",
    "model_name": "llama3.2-vision",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/llama3.2-vision",
    "description": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\n\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\n\n\nUsage\n\n\nFirst, pull the model:\n\n\nollama pull llama3.2-vision\n\n\n\nPython Library\n\n\nTo use Llama 3.2 Vision with the Ollama \nPython library\n:\n\n\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2-vision',\n    messages=[{\n        'role': 'user',\n        'content': 'What is in this image?',\n        'images': ['image.jpg']\n    }]\n)\n\nprint(response)\n\n\n\nJavaScript Library\n\n\nTo use Llama 3.2 Vision with the Ollama \nJavaScript library\n:\n\n\nimport ollama from 'ollama'\n\nconst response = await ollama.chat({\n  model: 'llama3.2-vision',\n  messages: [{\n    role: 'user',\n    content: 'What is in this image?',\n    images: ['image.jpg']\n  }]\n})\n\nconsole.log(response)\n\n\n\ncURL\n\n\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2-vision\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"<base64-encoded image data>\"]\n    }\n  ]\n}'\n\n\n\nReferences\n\n\nGitHub\n\n\nHuggingFace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/llama3.2-vision/ea1fa75c-0d15-453d-a291-ce2d97d8646a\" width=\"280\" />\n\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\n\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\n\n## Usage\n\nFirst, pull the model:\n\n```bash\nollama pull llama3.2-vision\n```\n\n### Python Library\n\nTo use Llama 3.2 Vision with the Ollama [Python library](https://github.com/ollama/ollama-python):\n\n```python\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2-vision',\n    messages=[{\n        'role': 'user',\n        'content': 'What is in this image?',\n        'images': ['image.jpg']\n    }]\n)\n\nprint(response)\n```\n\n### JavaScript Library\n\nTo use Llama 3.2 Vision with the Ollama [JavaScript library](https://github.com/ollama/ollama-js):\n\n```javascript\nimport ollama from 'ollama'\n\nconst response = await ollama.chat({\n  model: 'llama3.2-vision',\n  messages: [{\n    role: 'user',\n    content: 'What is in this image?',\n    images: ['image.jpg']\n  }]\n})\n\nconsole.log(response)\n```\n\n### cURL\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2-vision\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"<base64-encoded image data>\"]\n    }\n  ]\n}'\n```\n\n## References\n\n[GitHub](https://github.com/meta-llama/llama-models)\n\n[HuggingFace](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Vision"
    ],
    "capability": "Vision",
    "labels": [
      "11b",
      "90b"
    ],
    "memory_requirements": [
      {
        "tag": "llama3.2-vision:latest",
        "size": "7.8GB",
        "size_gb": 7.8,
        "recommended_ram_gb": 9.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "7.8GB",
        "size_gb": 7.8,
        "recommended_ram_gb": 9.8,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      },
      {
        "tag": "llama3.2-vision:90b",
        "size": "55GB",
        "size_gb": 55.0,
        "recommended_ram_gb": 68.8,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 7.8,
    "context_window": 128000,
    "speed_tier": "slow",
    "use_cases": [
      "Image Understanding",
      "Question Answering"
    ],
    "domain": "Vision",
    "ai_languages": [
      "English",
      "German",
      "French",
      "Italian",
      "Portuguese",
      "Hindi",
      "Spanish",
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "The llama3.2-vision model is ideal for tasks involving visual recognition, image reasoning, captioning, and answering general questions about an image.",
    "model_family": "Llama",
    "base_model": "llama3.2",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Visual recognition",
      "Image reasoning",
      "Captioning",
      "Multilingual support"
    ],
    "limitations": [
      "Requires significant computational resources",
      "Limited to specific languages for certain tasks"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 3800000,
    "tags": 9,
    "last_updated": "2025-05-26",
    "last_updated_str": "9 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "101c70d3-3573-43e6-8415-94bee6286fa9",
    "model_identifier": "tinyllama",
    "model_name": "tinyllama",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/tinyllama",
    "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nTinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n\nReferences\n\n\nHugging Face\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://github.com/jmorganca/ollama/assets/251292/6f17d445-d019-4e54-bac8-9cb6b3b01a26\" width=\"320\" />\n\nTinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n## References\n\n[Hugging Face](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6)\n\n[GitHub](https://github.com/jzhang38/TinyLlama)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "1.1b"
    ],
    "memory_requirements": [],
    "min_ram_gb": null,
    "context_window": null,
    "speed_tier": null,
    "use_cases": [
      "Chat Assistant",
      "Creative Writing",
      "Question Answering"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "TinyLlama is best for applications with restricted computation and memory footprint.",
    "model_family": "Llama",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Compact model size",
      "Low memory requirements",
      "Versatile applications"
    ],
    "limitations": [
      "Limited parameters",
      "May not perform as well as larger models"
    ],
    "target_audience": [
      "Developers",
      "Students",
      "Content Creators"
    ],
    "pulls": 3700000,
    "tags": 36,
    "last_updated": "2024-02-26",
    "last_updated_str": "2 years ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "7f86b3b4-52f4-4fc2-9500-0186708444fe",
    "model_identifier": "dolphin3",
    "model_name": "dolphin3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/dolphin3",
    "description": "Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nDolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\n\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products.\n\n\n\n\nThey maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\n\n\nThey maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\n\n\nThey maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\n\n\nThey can see all your queries and they can potentially use that data in ways you wouldn’t want. Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt. You decide the alignment. You have control of your data. Dolphin does not impose its ethics or guidelines on you. You are the one who decides the guidelines.\n\n\n\n\nReferences\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"/assets/library/dolphin3/ce75cebe-b012-4195-9a6b-aa6c4b68f93f\" width=\"360\" />\n\nDolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products.\n\n1. They maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\n2. They maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\n3. They maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\n4. They can see all your queries and they can potentially use that data in ways you wouldn't want. Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt. You decide the alignment. You have control of your data. Dolphin does not impose its ethics or guidelines on you. You are the one who decides the guidelines.\n\n## References\n\n[Hugging Face](https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.1-8B)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "8b"
    ],
    "memory_requirements": [
      {
        "tag": "dolphin3:latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K context",
        "context_window": 128000
      },
      {
        "tag": "latest",
        "size": "4.9GB",
        "size_gb": 4.9,
        "recommended_ram_gb": 6.1,
        "quantization": "q4_k_m",
        "context": "128K",
        "context_window": 128000
      }
    ],
    "min_ram_gb": 4.9,
    "context_window": 128000,
    "speed_tier": "medium",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Math",
      "Agent / Automation",
      "Function Calling"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "The dolphin3 model is best for businesses seeking a general-purpose local model for various use cases, including coding, math, and agentic tasks, without relying on third-party models.",
    "model_family": "Dolphin",
    "base_model": "llama3.1",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "General-purpose capabilities",
      "Coding and math support",
      "Agentic and function calling abilities",
      "Local model for business use"
    ],
    "limitations": [
      "Dependent on the base model (Llama 3.1)",
      "May require adjustments for specific business use cases"
    ],
    "target_audience": [
      "Developers",
      "Data Scientists",
      "Business Analysts"
    ],
    "pulls": 3600000,
    "tags": 5,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "3a50bfe0-8541-4af8-adda-61eaf5e0ef19",
    "model_identifier": "deepseek-v3",
    "model_name": "deepseek-v3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/deepseek-v3",
    "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.5.5\n or later.\n\n\n\n\n\n\nDeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally.\n\n\nReferences\n\n\nGitHub\n\n\nPaper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires [Ollama 0.5.5](https://github.com/ollama/ollama/releases/tag/v0.5.5) or later.\n\n<img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" />\n\nDeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally.\n\n## References\n\n[GitHub](https://github.com/deepseek-ai/DeepSeek-V3)\n\n[Paper](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "671b"
    ],
    "memory_requirements": [
      {
        "tag": "deepseek-v3:latest",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K context",
        "context_window": 160000
      },
      {
        "tag": "latest",
        "size": "404GB",
        "size_gb": 404.0,
        "recommended_ram_gb": 505.0,
        "quantization": "q4_k_m",
        "context": "160K",
        "context_window": 160000
      }
    ],
    "min_ram_gb": 404.0,
    "context_window": 160000,
    "speed_tier": "slow",
    "use_cases": [
      "Chat Assistant",
      "Creative Writing",
      "Question Answering",
      "Reasoning",
      "Text Summarization"
    ],
    "domain": "Language",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "advanced",
    "best_for": "Developers and researchers looking for a highly performant and efficient language model for a variety of natural language processing tasks.",
    "model_family": "DeepSeek",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "High-performance language modeling",
      "Fast inference speed",
      "State-of-the-art results among open-source models"
    ],
    "limitations": [
      "Requires significant computational resources (at least 404 GB RAM)",
      "May require specific versions of Ollama (0.5.5 or later)"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 3500000,
    "tags": 5,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "f3d38af0-ef1c-4d51-be3f-e92c630c4bf2",
    "model_identifier": "olmo2",
    "model_name": "olmo2",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/olmo2",
    "description": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nNote: this model requires \nOllama 0.5.5\n\n\n\n\n\n\nOLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.\n\n\nReferences\n\n\nBlog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> Note: this model requires [Ollama 0.5.5](https://github.com/ollama/ollama/releases/tag/v0.5.5)\n\n![1732650119-wide-4x.webp](/assets/library/olmo2/71e694b3-a4fe-4bd1-8338-684506f85e8d)\n\nOLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.\n\n## References\n\n[Blog post](https://allenai.org/blog/olmo2)\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [],
    "capability": null,
    "labels": [
      "7b",
      "13b"
    ],
    "memory_requirements": [
      {
        "tag": "olmo2:latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      },
      {
        "tag": "latest",
        "size": "4.5GB",
        "size_gb": 4.5,
        "recommended_ram_gb": 5.6,
        "quantization": "q4_k_m",
        "context": "4K",
        "context_window": 4000
      },
      {
        "tag": "olmo2:13b",
        "size": "8.4GB",
        "size_gb": 8.4,
        "recommended_ram_gb": 10.5,
        "quantization": "q4_k_m",
        "context": "4K context",
        "context_window": 4000
      }
    ],
    "min_ram_gb": 4.5,
    "context_window": 4000,
    "speed_tier": "medium",
    "use_cases": [
      "Chat Assistant",
      "Question Answering",
      "Text Summarization"
    ],
    "domain": "General",
    "ai_languages": [
      "English",
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "Olmo2 is suitable for users who require a competitive model for English academic benchmarks and general text-based tasks.",
    "model_family": "Other",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Competitive with open-weight models",
      "Trained on up to 5T tokens",
      "On par with or better than equivalently sized fully open models"
    ],
    "limitations": [
      "Requires Ollama 0.5.5",
      "May have limited support for non-English languages"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Students"
    ],
    "pulls": 3500000,
    "tags": 9,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "9a0f2afe-aa28-4a2c-bace-bbdf615b97a8",
    "model_identifier": "mistral-nemo",
    "model_name": "mistral-nemo",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/mistral-nemo",
    "description": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
    "readme": "Readme\n\n\n\n\n\n\n\n\n\n\nMistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.\n\n\n\n\nReference\n\n\nBlog\n\n\nHugging Face\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<img src=\"https://ollama.com/assets/library/mistral-nemo/72045292-694a-4867-88c8-8635c9d97030\" width=\"280\" />\n\nMistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.\n\n![nemo-base-performance.png](https://ollama.com/assets/library/mistral-nemo/1adf7b56-30e3-49a0-8a52-bb74c19d8a78)\n\n## Reference\n\n[Blog](https://mistral.ai/news/mistral-nemo/)\n\n[Hugging Face](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)\n\n\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Tools"
    ],
    "capability": "Tools",
    "labels": [
      "12b"
    ],
    "memory_requirements": [
      {
        "tag": "mistral-nemo:latest",
        "size": "7.1GB",
        "size_gb": 7.1,
        "recommended_ram_gb": 8.9,
        "quantization": "q4_k_m",
        "context": "1000K context",
        "context_window": 1000000
      },
      {
        "tag": "latest",
        "size": "7.1GB",
        "size_gb": 7.1,
        "recommended_ram_gb": 8.9,
        "quantization": "q4_k_m",
        "context": "1000K",
        "context_window": 1000000
      }
    ],
    "min_ram_gb": 7.1,
    "context_window": 1000000,
    "speed_tier": "slow",
    "use_cases": [
      "Chat Assistant",
      "Code Generation",
      "Code Review",
      "Question Answering",
      "Reasoning"
    ],
    "domain": "General",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "intermediate",
    "best_for": "Developers and researchers looking for a state-of-the-art model with high reasoning and coding accuracy",
    "model_family": "Mistral",
    "base_model": null,
    "is_fine_tuned": false,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "State-of-the-art reasoning",
      "High coding accuracy",
      "Large context window of up to 128k tokens",
      "Easy to use and integrate"
    ],
    "limitations": [
      "Requires at least 7.1 GB of RAM",
      "May not perform well on very specialized or niche tasks"
    ],
    "target_audience": [
      "Developers",
      "Researchers"
    ],
    "pulls": 3400000,
    "tags": 17,
    "last_updated": "2025-07-26",
    "last_updated_str": "7 months ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  },
  {
    "id": "722f3db1-0b02-4e12-a379-fd50cf71a989",
    "model_identifier": "bge-m3",
    "model_name": "bge-m3",
    "model_type": "official",
    "namespace": null,
    "url": "https://ollama.com/library/bge-m3",
    "description": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.",
    "readme": "Readme\n\n\n\n\n\n\n\n\nBGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity:\n\n\n\n\nMulti-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n\n\nMulti-Linguality: It can support more than 100 working languages.\n\n\nMulti-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\n\n\n\n\nBenchmarks from the open-source community\n\n\n\n\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite\n\n\nPreview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity:\n\n- Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n- Multi-Linguality: It can support more than 100 working languages.\n- Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\n\n**Benchmarks from the open-source community**\n![image.png](https://ollama.com/assets/library/bge-m3/17a9804b-f3da-4d09-bd18-90d4fe8900d3)\n\n```\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n\n\n\n\n                Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
    "capabilities": [
      "Embedding"
    ],
    "capability": "Embedding",
    "labels": [
      "567m"
    ],
    "memory_requirements": [
      {
        "tag": "bge-m3:latest",
        "size": "1.2GB",
        "size_gb": 1.2,
        "recommended_ram_gb": 1.5,
        "quantization": "q4_k_m",
        "context": "8K context",
        "context_window": 8000
      },
      {
        "tag": "latest",
        "size": "1.2GB",
        "size_gb": 1.2,
        "recommended_ram_gb": 1.5,
        "quantization": "q4_k_m",
        "context": "8K",
        "context_window": 8000
      }
    ],
    "min_ram_gb": 1.2,
    "context_window": 8000,
    "speed_tier": "fast",
    "use_cases": [
      "Text Embedding",
      "Question Answering",
      "RAG / Retrieval"
    ],
    "domain": "Multimodal",
    "ai_languages": [
      "Multilingual"
    ],
    "complexity": "beginner",
    "best_for": "BGE-M3 is ideal for tasks requiring versatile text embedding capabilities, supporting multiple languages and granularities.",
    "model_family": "Other",
    "base_model": "XLM-RoBERTa",
    "is_fine_tuned": true,
    "is_uncensored": false,
    "license": null,
    "strengths": [
      "Multi-functionality",
      "Multi-linguality",
      "Multi-granularity"
    ],
    "limitations": [
      "Limited to retrieval functionalities"
    ],
    "target_audience": [
      "Developers",
      "Researchers",
      "Data Scientists"
    ],
    "pulls": 3300000,
    "tags": 3,
    "last_updated": "2025-02-26",
    "last_updated_str": "1 year ago",
    "timestamp": "2026-02-26T22:00:24.669617",
    "enrich_version": 1,
    "validated": true,
    "validation_failed": null
  }
]